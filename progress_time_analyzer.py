#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ—¶é—´å’Œè¿›åº¦åˆ†æè„šæœ¬
åˆ†æå¹¶è¡Œå¤šæ¨¡å‹å¤„ç†ç³»ç»Ÿçš„æ•ˆç‡å’Œé¢„æœŸå®Œæˆæ—¶é—´
"""

import os
import json
import glob
from datetime import datetime, timedelta
from pathlib import Path
import statistics

def analyze_progress():
    """åˆ†æå¤„ç†è¿›åº¦å’Œæ—¶é—´æ•ˆç‡"""
    print("ğŸ“Š å¹¶è¡Œå¤šæ¨¡å‹å¤„ç†ç³»ç»Ÿ - æ—¶é—´ä¸è¿›åº¦åˆ†ææŠ¥å‘Š")
    print("=" * 60)

    # åŸºç¡€é…ç½®ä¿¡æ¯
    total_files = 550
    completed_files = 3
    remaining_files = total_files - completed_files
    num_processes = 4
    models_per_file = 3

    print(f"ğŸ“‹ ä»»åŠ¡é…ç½®:")
    print(f"   æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"   å¹¶è¡Œè¿›ç¨‹æ•°: {num_processes}")
    print(f"   æ¯æ–‡ä»¶æ¨¡å‹æ•°: {models_per_file}")
    print(f"   å½“å‰å·²å®Œæˆ: {completed_files}")
    print(f"   å‰©ä½™æ–‡ä»¶: {remaining_files}")
    print()

    # åˆ†æå·²å®Œæˆæ–‡ä»¶çš„æ—¶é—´æˆ³
    results_dir = "multi_model_5segment_results"
    analysis_files = glob.glob(os.path.join(results_dir, "*_multi_model_5segment_analysis.json"))

    if not analysis_files:
        print("âŒ æœªæ‰¾åˆ°å·²å®Œæˆçš„åˆ†ææ–‡ä»¶")
        return

    print(f"ğŸ“ å·²å®Œæˆåˆ†ææ–‡ä»¶: {len(analysis_files)}")

    # æå–æ—¶é—´ä¿¡æ¯
    completion_times = []
    for file_path in analysis_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            analysis_date = data.get('file_info', {}).get('analysis_date')
            if analysis_date:
                dt = datetime.fromisoformat(analysis_date.replace('Z', '+00:00'))
                completion_times.append(dt)
                print(f"   {Path(file_path).name[:50]}... -> {dt.strftime('%H:%M:%S')}")
        except Exception as e:
            print(f"   âŒ è§£ææ–‡ä»¶å¤±è´¥: {Path(file_path).name} - {e}")

    if len(completion_times) < 2:
        print("âš ï¸ éœ€è¦è‡³å°‘2ä¸ªå®Œæˆæ–‡ä»¶æ¥è®¡ç®—å¤„ç†é€Ÿåº¦")
        return

    # è®¡ç®—å¤„ç†é€Ÿåº¦
    completion_times.sort()
    time_span = completion_times[-1] - completion_times[0]
    files_processed = len(completion_times)

    print(f"\nâ±ï¸ å¤„ç†æ—¶é—´åˆ†æ:")
    print(f"   é¦–ä¸ªå®Œæˆ: {completion_times[0].strftime('%H:%M:%S')}")
    print(f"   æœ€æ–°å®Œæˆ: {completion_times[-1].strftime('%H:%M:%S')}")
    print(f"   æ€»è€—æ—¶: {time_span}")
    print(f"   å¤„ç†æ–‡ä»¶æ•°: {files_processed}")

    if time_span.total_seconds() > 0:
        # è®¡ç®—å¹³å‡å¤„ç†é€Ÿåº¦
        avg_time_per_file = time_span.total_seconds() / files_processed
        files_per_hour = 3600 / avg_time_per_file

        print(f"   å¹³å‡æ¯æ–‡ä»¶: {avg_time_per_file:.1f}ç§’")
        print(f"   å¤„ç†é€Ÿåº¦: {files_per_hour:.1f}æ–‡ä»¶/å°æ—¶")

        # é¢„ä¼°å‰©ä½™æ—¶é—´
        estimated_remaining_time = remaining_files * avg_time_per_file / num_processes
        estimated_completion = datetime.now() + timedelta(seconds=estimated_remaining_time)

        print(f"\nğŸ¯ å®Œæˆæ—¶é—´é¢„ä¼°:")
        print(f"   å‰©ä½™æ–‡ä»¶: {remaining_files}")
        print(f"   é¢„ä¼°å‰©ä½™æ—¶é—´: {timedelta(seconds=int(estimated_remaining_time))}")
        print(f"   é¢„è®¡å®Œæˆæ—¶é—´: {estimated_completion.strftime('%Y-%m-%d %H:%M:%S')}")

        # æ•ˆç‡åˆ†æ
        single_process_time = remaining_files * avg_time_per_file
        parallel_time_saved = single_process_time - estimated_remaining_time
        efficiency_gain = (parallel_time_saved / single_process_time) * 100

        print(f"\nğŸš€ å¹¶è¡Œå¤„ç†æ•ˆç‡:")
        print(f"   å•è¿›ç¨‹é¢„ä¼°æ—¶é—´: {timedelta(seconds=int(single_process_time))}")
        print(f"   å¹¶è¡Œé¢„ä¼°æ—¶é—´: {timedelta(seconds=int(estimated_remaining_time))}")
        print(f"   èŠ‚çœæ—¶é—´: {timedelta(seconds=int(parallel_time_saved))}")
        print(f"   æ•ˆç‡æå‡: {efficiency_gain:.1f}%")

        # APIè°ƒç”¨ç»Ÿè®¡
        total_segments_per_file = 10  # 50é¢˜åˆ†10æ®µï¼Œæ¯æ®µ5é¢˜
        total_api_calls = completed_files * models_per_file * total_segments_per_file
        remaining_api_calls = remaining_files * models_per_file * total_segments_per_file

        print(f"\nğŸ“¡ APIè°ƒç”¨ç»Ÿè®¡:")
        print(f"   å·²å®Œæˆè°ƒç”¨: {total_api_calls}")
        print(f"   å‰©ä½™è°ƒç”¨: {remaining_api_calls}")
        print(f"   æ€»è°ƒç”¨æ•°: {total_api_calls + remaining_api_calls}")

        # æˆåŠŸç‡å’Œè´¨é‡æŒ‡æ ‡
        success_rate = (completed_files / total_files) * 100
        print(f"\nâœ… è´¨é‡æŒ‡æ ‡:")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"   æ¨¡å‹ä¸€è‡´æ€§: æ­£åœ¨è®¡ç®—ä¸­...")

        # æ—¶é—´åˆ†å¸ƒåˆ†æ
        if len(completion_times) >= 3:
            intervals = []
            for i in range(1, len(completion_times)):
                interval = (completion_times[i] - completion_times[i-1]).total_seconds()
                intervals.append(interval)

            avg_interval = statistics.mean(intervals)
            min_interval = min(intervals)
            max_interval = max(intervals)

            print(f"\nğŸ“ˆ å¤„ç†é—´éš”åˆ†æ:")
            print(f"   å¹³å‡é—´éš”: {avg_interval:.1f}ç§’")
            print(f"   æœ€å¿«é—´éš”: {min_interval:.1f}ç§’")
            print(f"   æœ€æ…¢é—´éš”: {max_interval:.1f}ç§’")

            # ç¨³å®šæ€§è¯„ä¼°
            if len(intervals) > 1:
                interval_std = statistics.stdev(intervals)
                cv = (interval_std / avg_interval) * 100  # å˜å¼‚ç³»æ•°
                print(f"   ç¨³å®šæ€§(CV): {cv:.1f}% ({'ç¨³å®š' if cv < 20 else 'ä¸­ç­‰' if cv < 50 else 'ä¸ç¨³å®š'})")

def main():
    """ä¸»å‡½æ•°"""
    analyze_progress()

if __name__ == "__main__":
    main()