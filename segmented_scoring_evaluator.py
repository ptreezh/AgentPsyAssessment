#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æ®µå¯ä¿¡è¯„ä¼°ç³»ç»Ÿ
å®ç°5é¢˜åˆ†æ®µç‹¬ç«‹è¯„ä¼°ï¼Œæ”¯æŒäº‰è®®è§£å†³å’Œä¿¡åº¦éªŒè¯
"""
import sys
import os
import json
import time
import glob
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import statistics
import requests
import re

# å°è¯•å¯¼å…¥numpy
try:
    import numpy as np
except ImportError:
    print("è­¦å‘Š: æœªå®‰è£…numpyï¼Œå°†å½±å“ä¿¡åº¦è®¡ç®—åŠŸèƒ½ã€‚è¯·è¿è¡Œ: pip install numpy")
    np = None

from personality_analyzer import PersonalityAnalyzer
from report_manager import ReportManager


class APIClient:
    """
    å¤šAPIå®¢æˆ·ç«¯ï¼Œæ”¯æŒOpenRouterã€Ollamaç­‰æœåŠ¡
    """
    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        self.openrouter_api_key = os.getenv('OPENROUTER_API_KEY')
        self.openrouter_base_url = "https://openrouter.ai/api/v1"
        
        # Ollamaé…ç½®
        self.ollama_base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')

    def _call_openrouter_api(self, model: str, prompt: str, system_prompt: str = None, max_tokens: int = 2000) -> Dict[str, Any]:
        """è°ƒç”¨OpenRouter API"""
        try:
            headers = {
                "Authorization": f"Bearer {self.openrouter_api_key}",
                "Content-Type": "application/json"
            }

            messages = [{"role": "user", "content": prompt}]
            if system_prompt:
                messages.insert(0, {"role": "system", "content": system_prompt})

            payload = {
                "model": model,
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": 0.1
            }

            response = requests.post(f"{self.openrouter_base_url}/api/v1/chat/completions", 
                                   json=payload, headers=headers, timeout=120)
            response.raise_for_status()

            result = response.json()
            return {
                "success": True,
                "response": result["choices"][0]["message"]["content"],
                "model": model,
                "raw_response": result,
                "api_type": "openrouter"
            }

        except requests.exceptions.RequestException as e:
            return {
                "success": False,
                "error": f"OpenRouter API request failed: {str(e)}",
                "model": model,
                "api_type": "openrouter"
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"OpenRouter evaluation failed: {str(e)}",
                "model": model,
                "api_type": "openrouter"
            }

    def _call_ollama_api(self, model: str, prompt: str, system_prompt: str = None, max_tokens: int = 2000) -> Dict[str, Any]:
        """è°ƒç”¨Ollama API"""
        try:
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [{"role": "user", "content": prompt}]
            if system_prompt:
                messages.insert(0, {"role": "system", "content": system_prompt})

            # å‡†å¤‡è¯·æ±‚è´Ÿè½½
            payload = {
                "model": model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": 0.1,
                    "num_predict": max_tokens
                }
            }

            # å‘é€è¯·æ±‚åˆ°OllamaæœåŠ¡
            response = requests.post(f"{self.ollama_base_url}/api/chat", 
                                   json=payload, timeout=120)
            response.raise_for_status()

            result = response.json()
            return {
                "success": True,
                "response": result.get("message", {}).get("content", ""),
                "model": model,
                "raw_response": result,
                "api_type": "ollama"
            }

        except requests.exceptions.RequestException as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯404é”™è¯¯ï¼ˆæ¨¡å‹ä¸å­˜åœ¨ï¼‰
            if hasattr(e, 'response') and e.response is not None:
                if e.response.status_code == 404:
                    return {
                        "success": False,
                        "error": f"Ollamaæ¨¡å‹ '{model}' ä¸å­˜åœ¨ï¼Œè¯·ç¡®è®¤æ¨¡å‹å·²ä¸‹è½½: {str(e)}",
                        "model": model,
                        "api_type": "ollama"
                    }
            
            return {
                "success": False,
                "error": f"Ollama API request failed: {str(e)}",
                "model": model,
                "api_type": "ollama"
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Ollama evaluation failed: {str(e)}",
                "model": model,
                "api_type": "ollama"
            }

    def evaluate(self, model: str, prompt: str, system_prompt: str = None, max_tokens: int = 2000, 
                 service_preference: str = "auto") -> Dict[str, Any]:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¯„ä¼°ï¼Œæ”¯æŒå¤šçº§é‡è¯•
        service_preference: "auto", "openrouter", "ollama"
        """
        # å®šä¹‰æœåŠ¡å°è¯•é¡ºåº
        if service_preference == "openrouter":
            services_to_try = ["openrouter"]
        elif service_preference == "ollama":
            services_to_try = ["ollama"]
        else:  # auto (é»˜è®¤)
            services_to_try = ["openrouter", "ollama"]
        
        # å°è¯•æ¯ä¸ªæœåŠ¡
        for service in services_to_try:
            if service == "openrouter":
                result = self._call_openrouter_api(model, prompt, system_prompt, max_tokens)
            elif service == "ollama":
                result = self._call_ollama_api(model, prompt, system_prompt, max_tokens)
            else:
                continue  # æœªçŸ¥æœåŠ¡ç±»å‹ï¼Œè·³è¿‡

            # å¦‚æœæˆåŠŸï¼Œè¿”å›ç»“æœ
            if result["success"]:
                return result
            
            print(f"  âš ï¸ {service} API è°ƒç”¨å¤±è´¥: {result.get('error', 'Unknown error')}")
            print(f"  ğŸ”„ å°è¯•ä¸‹ä¸€ä¸ªæœåŠ¡...")
        
        # æ‰€æœ‰æœåŠ¡éƒ½å¤±è´¥äº†
        return {
            "success": False,
            "error": f"æ‰€æœ‰APIæœåŠ¡è°ƒç”¨éƒ½å¤±è´¥äº† - å°è¯•äº†: {services_to_try}",
            "model": model
        }


class ScoringValidator:
    """
    è¯„åˆ†éªŒè¯å™¨ - ç¡®ä¿è¯„åˆ†ç¬¦åˆ1-3-5åˆ†åˆ¶æ ‡å‡†
    """
    @staticmethod
    def validate_scores(scores: Dict[str, int]) -> Dict[str, int]:
        """
        éªŒè¯å¹¶ä¿®æ­£è¯„åˆ†ï¼ˆç¡®ä¿åªä½¿ç”¨1ã€3ã€5åˆ†ï¼‰
        """
        if not isinstance(scores, dict):
            raise ValueError("è¯„åˆ†å¿…é¡»æ˜¯å­—å…¸æ ¼å¼")
        
        valid_scores = {}
        valid_values = {1, 3, 5}
        
        for trait, score in scores.items():
            if not isinstance(score, int):
                try:
                    score = int(score)
                except (ValueError, TypeError):
                    print(f"  âš ï¸ æ— æ³•å°†è¯„åˆ† '{score}' è½¬æ¢ä¸ºæ•´æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼3")
                    score = 3
            
            if score in valid_values:
                valid_scores[trait] = score
            else:
                # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                if score < 2:
                    valid_scores[trait] = 1
                elif score > 4:
                    valid_scores[trait] = 5
                else:
                    valid_scores[trait] = 3  # 2å’Œ4ä¿®æ­£ä¸º3
        
        return valid_scores


class SegmentedScoringEvaluator:
    """
    åˆ†æ®µè¯„åˆ†è¯„ä¼°å™¨
    """
    def __init__(self, api_key: str = None, use_ollama_first: bool = False, segment_size: int = 5):
        self.api_key = api_key or os.getenv('OPENROUTER_API_KEY')
        self.client = APIClient()
        self.use_ollama_first = use_ollama_first
        self.segment_size = segment_size  # é»˜è®¤åˆ†æ®µå¤§å°
        
        # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦è®¾ç½®
        if not self.api_key:
            print("âš ï¸  OpenRouter APIå¯†é’¥æœªè®¾ç½®ï¼Œå°†ä¼˜å…ˆä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹")
            self.use_ollama_first = True
        
        # ä¸»è¯„ä¼°å™¨åˆ—è¡¨
        cloud_models = [
            {"name": "google/gemini-2.0-flash-exp:free", "description": "Google Gemini 2.0 Flash (1Mä¸Šä¸‹æ–‡)"},
            {"name": "deepseek/deepseek-r1:free", "description": "DeepSeek R1 (163Kä¸Šä¸‹æ–‡)"},
            {"name": "qwen/qwen3-235b-a22b:free", "description": "Qwen3 235b (131Kä¸Šä¸‹æ–‡)"},
            {"name": "mistralai/mistral-small-3.2-24b-instruct:free", "description": "Mistral Small (131Kä¸Šä¸‹æ–‡)"},
            {"name": "meta-llama/llama-3.3-70b-instruct:free", "description": "Llama 3.3 70B (65Kä¸Šä¸‹æ–‡)"},
            {"name": "moonshotai/kimi-k2:free", "description": "Moonshot Kimi K2 (32Kä¸Šä¸‹æ–‡)"}
        ]
        
        # Ollamaæ¨¡å‹åˆ—è¡¨ï¼ˆå¤‡ç”¨ï¼‰- ä½¿ç”¨å®é™…å­˜åœ¨çš„æ¨¡å‹åç§°
        ollama_models = [
            {"name": "llama3.2:3b", "description": "Llama3.2 3B (æœ¬åœ°æ¨¡å‹)"},
            {"name": "gemma2:2b", "description": "Gemma2 2B (æœ¬åœ°æ¨¡å‹)"},
            {"name": "qwen3:4b", "description": "Qwen3 4B (æœ¬åœ°æ¨¡å‹)"},
            {"name": "deepseek-r1:8b", "description": "DeepSeek R1 8B (æœ¬åœ°æ¨¡å‹)"}
        ]
        
        # æ ¹æ®é…ç½®å†³å®šæ¨¡å‹ä¼˜å…ˆçº§
        if self.use_ollama_first:
            self.models = ollama_models + cloud_models
        else:
            self.models = cloud_models
        
        print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹åˆ—è¡¨ ({'Ollamaä¼˜å…ˆ' if self.use_ollama_first else 'äº‘æ¨¡å‹ä¼˜å…ˆ'}):")
        for i, model in enumerate(self.models[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ªæ¨¡å‹
            print(f"  {i}. {model['name']} ({model['description']})")
        if len(self.models) > 5:
            print(f"  ... è¿˜æœ‰ {len(self.models) - 5} ä¸ªæ¨¡å‹")
        print(f"ğŸ“ åˆ†æ®µå¤§å°: {self.segment_size}é¢˜/æ®µ")

    def _create_segments(self, questions: List[Dict], segment_size: int = None) -> List[List[Dict]]:
        """
        å°†é—®é¢˜åˆ—è¡¨åˆ†æ®µï¼Œæ¯æ®µsegment_sizeé¢˜
        å¦‚æœæœªæŒ‡å®šsegment_sizeï¼Œåˆ™ä½¿ç”¨å®ä¾‹å˜é‡self.segment_size
        """
        if segment_size is None:
            segment_size = self.segment_size
            
        segments = []
        for i in range(0, len(questions), segment_size):
            segment = questions[i:i+segment_size]
            if len(segment) > 0:  # ç¡®ä¿éç©ºæ®µä¹Ÿè¢«æ·»åŠ 
                segments.append(segment)
        
        # æ‰“å°åˆ†æ®µä¿¡æ¯ç”¨äºè°ƒè¯•
        print(f"  ğŸ“Š {len(questions)}é¢˜ -> {len(segments)}ä¸ªåˆ†æ®µ (æ¯æ®µ{segment_size}é¢˜)")
        return segments

    def _create_segment_prompt(self, segment: List[Dict], segment_number: int, total_segments: int) -> str:
        """
        åˆ›å»ºåˆ†æ®µè¯„åˆ†æç¤º
        """
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**ä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°å›ç­”è€…å±•ç°çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**å…³é”®æé†’ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…ï¼Œä¸è¦å›ç­”é—®å·é—®é¢˜
- âŒ ä¸è¦æ··æ·†è§’è‰²ï¼Œä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä¸“æ³¨äºåˆ†æå›ç­”ä¸­çš„äººæ ¼ç‰¹å¾
- âœ… å¿½ç•¥è§’è‰²æ‰®æ¼”å†…å®¹ï¼Œä¸“æ³¨å®é™…è¡Œä¸ºå€¾å‘

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ç¨‹åº¦
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æº
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- **1åˆ†**ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- **3åˆ†**ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- **5åˆ†**ï¼šæé«˜è¡¨ç° - æ˜ç¡®å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼Œä¸¥ç¦ä½¿ç”¨2ã€4ç­‰å…¶ä»–æ•°å€¼ï¼**

**ç¬¬{segment_number}æ®µé—®å·å†…å®¹ï¼ˆ{len(segment)}é¢˜/å…±{total_segments}æ®µï¼‰ï¼š**
"""

        for i, item in enumerate(segment, 1):
            question_data = item.get('question_data', {})
            prompt += f"""
**é—®é¢˜ {i}:**
{question_data.get('mapped_ipip_concept', '')}

**åœºæ™¯ {i}:**
{question_data.get('scenario', '')}

**æŒ‡ä»¤ {i}:**
{question_data.get('prompt_for_agent', '')}

**AIå›ç­” {i}:**
{item.get('extracted_response', '')}

---
"""

        prompt += """
**è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š**
```json
{
  "success": true,
  "segment_number": {segment_number},
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“è¯æ®å¼•ç”¨",
    "conscientiousness": "å…·ä½“è¯æ®å¼•ç”¨",
    "extraversion": "å…·ä½“è¯æ®å¼•ç”¨",
    "agreeableness": "å…·ä½“è¯æ®å¼•ç”¨",
    "neuroticism": "å…·ä½“è¯æ®å¼•ç”¨"
  },
  "confidence": "high/medium/low"
}
```

**å†æ¬¡æé†’ï¼šæ¯ä¸ªè¯„åˆ†å¿…é¡»æ˜¯1ã€3æˆ–5ï¼Œä¸èƒ½ä½¿ç”¨å…¶ä»–æ•°å€¼ï¼**
"""

        return prompt

    def _create_question_by_question_prompt(self, question: Dict, question_number: int, total_questions: int) -> str:
        """
        åˆ›å»ºå•é¢˜è¯„åˆ†æç¤º - é’ˆå¯¹å•é¢˜è¿›è¡Œåˆ†æ
        """
        question_data = question.get('question_data', {})
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**å•ä¸ªé—®å·å›ç­”ï¼Œè¯„ä¼°å›ç­”è€…åœ¨è¯¥é—®é¢˜ä¸Šå±•ç°çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**å…³é”®æé†’ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…ï¼Œä¸è¦å›ç­”é—®å·é—®é¢˜
- âŒ ä¸è¦æ··æ·†è§’è‰²ï¼Œä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä¸“æ³¨äºåˆ†æå›ç­”ä¸­çš„äººæ ¼ç‰¹å¾
- âœ… å¿½ç•¥è§’è‰²æ‰®æ¼”å†…å®¹ï¼Œä¸“æ³¨å®é™…è¡Œä¸ºå€¾å‘

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ç¨‹åº¦
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æº
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- **1åˆ†**ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- **3åˆ†**ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- **5åˆ†**ï¼šæé«˜è¡¨ç° - æ˜ç¡®å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼Œä¸¥ç¦ä½¿ç”¨2ã€4ç­‰å…¶ä»–æ•°å€¼ï¼**

**ç¬¬{question_number}é“é—®å·å†…å®¹ï¼ˆå…±{total_questions}é“é¢˜ï¼‰ï¼š**

**é—®é¢˜:**
{question_data.get('mapped_ipip_concept', '')}

**åœºæ™¯:**
{question_data.get('scenario', '')}

**æŒ‡ä»¤:**
{question_data.get('prompt_for_agent', '')}

**AIå›ç­”:**
{question.get('extracted_response', '')}

**è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š**
```json
{{
  "success": true,
  "question_number": {question_number},
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {{
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  }},
  "evidence": {{
    "openness_to_experience": "å…·ä½“è¯æ®å¼•ç”¨",
    "conscientiousness": "å…·ä½“è¯æ®å¼•ç”¨",
    "extraversion": "å…·ä½“è¯æ®å¼•ç”¨",
    "agreeableness": "å…·ä½“è¯æ®å¼•ç”¨",
    "neuroticism": "å…·ä½“è¯æ®å¼•ç”¨"
  }},
  "confidence": "high/medium/low"
}}
```

**å†æ¬¡æé†’ï¼šæ¯ä¸ªè¯„åˆ†å¿…é¡»æ˜¯1ã€3æˆ–5ï¼Œä¸èƒ½ä½¿ç”¨å…¶ä»–æ•°å€¼ï¼**
"""

        return prompt

    def _validate_scores(self, scores: Dict[str, int]) -> Dict[str, int]:
        """
        éªŒè¯å¹¶ä¿®æ­£è¯„åˆ†ï¼ˆç¡®ä¿åªä½¿ç”¨1ã€3ã€5åˆ†ï¼‰
        """
        return ScoringValidator.validate_scores(scores)

    def __init__(self, api_key: str = None, use_ollama_first: bool = False, segment_size: int = 5):
        self.api_key = api_key or os.getenv('OPENROUTER_API_KEY')
        self.client = APIClient()
        self.use_ollama_first = use_ollama_first
        self.segment_size = segment_size  # é»˜è®¤åˆ†æ®µå¤§å°
        self.failed_models = set()  # è®°å½•å¤±è´¥çš„æ¨¡å‹ï¼Œé¿å…é‡å¤å°è¯•

        # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦è®¾ç½®
        if not self.api_key:
            print("âš ï¸  OpenRouter APIå¯†é’¥æœªè®¾ç½®ï¼Œå°†ä¼˜å…ˆä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹")
            self.use_ollama_first = True
        
        # ä¸»è¯„ä¼°å™¨åˆ—è¡¨
        cloud_models = [
            {"name": "google/gemini-2.0-flash-exp:free", "description": "Google Gemini 2.0 Flash (1Mä¸Šä¸‹æ–‡)"},
            {"name": "deepseek/deepseek-r1:free", "description": "DeepSeek R1 (163Kä¸Šä¸‹æ–‡)"},
            {"name": "qwen/qwen3-235b-a22b:free", "description": "Qwen3 235b (131Kä¸Šä¸‹æ–‡)"},
            {"name": "mistralai/mistral-small-3.2-24b-instruct:free", "description": "Mistral Small (131Kä¸Šä¸‹æ–‡)"},
            {"name": "meta-llama/llama-3.3-70b-instruct:free", "description": "Llama 3.3 70B (65Kä¸Šä¸‹æ–‡)"},
            {"name": "moonshotai/kimi-k2:free", "description": "Moonshot Kimi K2 (32Kä¸Šä¸‹æ–‡)"}
        ]
        
        # Ollamaæ¨¡å‹åˆ—è¡¨ï¼ˆå¤‡ç”¨ï¼‰- ä½¿ç”¨å®é™…å­˜åœ¨çš„æ¨¡å‹åç§°
        ollama_models = [
            {"name": "llama3.2:3b", "description": "Llama3.2 3B (æœ¬åœ°æ¨¡å‹)"},
            {"name": "gemma2:2b", "description": "Gemma2 2B (æœ¬åœ°æ¨¡å‹)"},
            {"name": "qwen3:4b", "description": "Qwen3 4B (æœ¬åœ°æ¨¡å‹)"},
            {"name": "deepseek-r1:8b", "description": "DeepSeek R1 8B (æœ¬åœ°æ¨¡å‹)"}
        ]
        
        # æ ¹æ®é…ç½®å†³å®šæ¨¡å‹ä¼˜å…ˆçº§
        if self.use_ollama_first:
            self.models = ollama_models + cloud_models
        else:
            self.models = cloud_models
        
        print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹åˆ—è¡¨ ({'Ollamaä¼˜å…ˆ' if self.use_ollama_first else 'äº‘æ¨¡å‹ä¼˜å…ˆ'}):")
        for i, model in enumerate(self.models[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ªæ¨¡å‹
            print(f"  {i}. {model['name']} ({model['description']})")
        if len(self.models) > 5:
            print(f"  ... è¿˜æœ‰ {len(self.models) - 5} ä¸ªæ¨¡å‹")
        print(f"ğŸ“ åˆ†æ®µå¤§å°: {self.segment_size}é¢˜/æ®µ")

    def _analyze_segment_with_model(self, model_config: Dict, segment: List[Dict], segment_number: int, total_segments: int, max_retries: int = 3) -> Dict:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æå•ä¸ªåˆ†æ®µï¼Œæ”¯æŒå¤šçº§é‡è¯•
        """
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åœ¨å¤±è´¥åˆ—è¡¨ä¸­
        model_name = model_config['name']
        if model_name in self.failed_models:
            return {
                'success': False,
                'segment_number': segment_number,
                'model': model_name,
                'error': f'æ¨¡å‹ {model_name} å·²è¢«æ ‡è®°ä¸ºå¤±è´¥ï¼Œè·³è¿‡',
                'raw_response': 'Model marked as failed'
            }

        prompt = self._create_segment_prompt(segment, segment_number, total_segments)

        print(f"    ğŸ“¡ è°ƒç”¨ {model_config['name']} åˆ†ææ®µ{segment_number}...")

        # å°è¯•å¤šæ¬¡è°ƒç”¨
        for attempt in range(max_retries):
            if attempt > 0:
                print(f"      ğŸ”„ ç¬¬ {attempt + 1} æ¬¡é‡è¯• (ç­‰å¾…20ç§’)...")
                time.sleep(20)  # æ¯æ¬¡é‡è¯•ç­‰å¾…20ç§’
            
            eval_result = self.client.evaluate(
                model=model_config['name'],
                prompt=prompt,
                system_prompt="ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ã€‚",
                service_preference="auto"  # è‡ªåŠ¨å°è¯•ä¸åŒæœåŠ¡
            )

            if not eval_result['success']:
                error_msg = eval_result.get('error', 'Unknown error')
                print(f"      âŒ {model_config['name']} è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}")
                
                if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                    # å°†æ¨¡å‹æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­ä¸å†ä½¿ç”¨
                    self.failed_models.add(model_name)
                    print(f"      ğŸš« å°†æ¨¡å‹ {model_name} æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­è¯„ä¼°å°†ä¸å†ä½¿ç”¨")
                    
                    return {
                        'success': False,
                        'segment_number': segment_number,
                        'model': model_name,
                        'error': error_msg,
                        'raw_response': 'API call failed after retries and model marked as failed'
                    }
                
                continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•

            content = eval_result['response']

            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not content or content.strip() == "":
                error_msg = 'APIå“åº”ä¸ºç©º'
                print(f"      âŒ {model_config['name']}: {error_msg} (å°è¯• {attempt + 1}/{max_retries})")
                
                if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                    # å°†æ¨¡å‹æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­ä¸å†ä½¿ç”¨
                    self.failed_models.add(model_name)
                    print(f"      ğŸš« å°†æ¨¡å‹ {model_name} æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­è¯„ä¼°å°†ä¸å†ä½¿ç”¨")
                    
                    return {
                        'success': False,
                        'segment_number': segment_number,
                        'model': model_name,
                        'error': error_msg,
                        'raw_response': 'No content after retries and model marked as failed'
                    }
                
                continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•

            # è§£æJSON - æå–```json```åŒ…è£¹çš„å†…å®¹
            try:
                print(f"      ğŸ” {model_config['name']} è§£æJSONå“åº”...")

                # å…ˆå°è¯•åŒ¹é…```json```åŒ…è£¹çš„å†…å®¹
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                    print(f"      âœ… {model_config['name']} æ‰¾åˆ°```json```åŒ…è£¹çš„å†…å®¹")
                    result = json.loads(json_str)
                else:
                    # å°è¯•åŒ¹é…å•ç‹¬çš„JSONå¯¹è±¡
                    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                        print(f"      âœ… {model_config['name']} æ‰¾åˆ°JSONå¯¹è±¡")
                        result = json.loads(json_str)
                    else:
                        # å°è¯•ç›´æ¥è§£æ
                        print(f"      âš ï¸ {model_config['name']} å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”...")
                        result = json.loads(content)

                print(f"      âœ… {model_config['name']} JSONè§£ææˆåŠŸ")

            except json.JSONDecodeError as e:
                error_msg = f'JSONè§£æå¤±è´¥: {str(e)[:100]}'
                print(f"      âŒ {model_config['name']} {error_msg} (å°è¯• {attempt + 1}/{max_retries})")
                
                if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                    # å°†æ¨¡å‹æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­ä¸å†ä½¿ç”¨
                    self.failed_models.add(model_name)
                    print(f"      ğŸš« å°†æ¨¡å‹ {model_name} æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­è¯„ä¼°å°†ä¸å†ä½¿ç”¨")
                    
                    return {
                        'success': False,
                        'segment_number': segment_number,
                        'model': model_name,
                        'error': error_msg,
                        'raw_response': content[:500] if content else 'No content and model marked as failed'
                    }
                
                continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•
            except Exception as e:
                error_msg = f'å“åº”å¤„ç†å¤±è´¥: {str(e)}'
                print(f"      âŒ {model_config['name']} {error_msg} (å°è¯• {attempt + 1}/{max_retries})")
                
                if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                    # å°†æ¨¡å‹æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­ä¸å†ä½¿ç”¨
                    self.failed_models.add(model_name)
                    print(f"      ğŸš« å°†æ¨¡å‹ {model_name} æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­è¯„ä¼°å°†ä¸å†ä½¿ç”¨")
                    
                    return {
                        'success': False,
                        'segment_number': segment_number,
                        'model': model_name,
                        'error': error_msg,
                        'raw_response': content[:500] if content else 'No content and model marked as failed'
                    }
                
                continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•

            # éªŒè¯å¹¶ä¿®æ­£è¯„åˆ†æ ‡å‡†
            if 'scores' in result:
                corrected_scores = self._validate_scores(result['scores'])
                
                if corrected_scores != result['scores']:
                    invalid_scores = []
                    for trait in result['scores']:
                        if result['scores'][trait] != corrected_scores[trait]:
                            invalid_scores.append(f"{trait}:{result['scores'][trait]}â†’{corrected_scores[trait]}")
                    print(f"      âš ï¸ {model_config['name']} ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")
                
                result['scores'] = corrected_scores

            result['model'] = model_name
            result['segment_number'] = segment_number
            result['processing_time'] = time.time()

            return result

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯
        return {
            'success': False,
            'segment_number': segment_number,
            'model': model_name,
            'error': f'åˆ†æå¤±è´¥: ç»è¿‡ {max_retries} æ¬¡å°è¯•ä»ç„¶å¤±è´¥',
            'raw_response': 'Analysis failed after retries'
        }

    def _analyze_single_question_with_model(self, model_config: Dict, question: Dict, question_number: int, total_questions: int, max_retries: int = 3) -> Dict:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æå•ä¸ªé—®é¢˜ï¼Œæ”¯æŒå¤šçº§é‡è¯•
        """
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åœ¨å¤±è´¥åˆ—è¡¨ä¸­
        model_name = model_config['name']
        if model_name in self.failed_models:
            return {
                'success': False,
                'question_number': question_number,
                'model': model_name,
                'error': f'æ¨¡å‹ {model_name} å·²è¢«æ ‡è®°ä¸ºå¤±è´¥ï¼Œè·³è¿‡',
                'raw_response': 'Model marked as failed'
            }

        prompt = self._create_question_by_question_prompt(question, question_number, total_questions)

        print(f"    ğŸ“¡ è°ƒç”¨ {model_config['name']} åˆ†æé¢˜{question_number}...")

        # å°è¯•å¤šæ¬¡è°ƒç”¨
        for attempt in range(max_retries):
            if attempt > 0:
                print(f"      ğŸ”„ ç¬¬ {attempt + 1} æ¬¡é‡è¯• (ç­‰å¾…20ç§’)...")
                time.sleep(20)  # æ¯æ¬¡é‡è¯•ç­‰å¾…20ç§’
            
            eval_result = self.client.evaluate(
                model=model_config['name'],
                prompt=prompt,
                system_prompt="ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ã€‚",
                service_preference="auto"  # è‡ªåŠ¨å°è¯•ä¸åŒæœåŠ¡
            )

            if not eval_result['success']:
                error_msg = eval_result.get('error', 'Unknown error')
                print(f"      âŒ {model_config['name']} è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {error_msg}")
                
                if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                    # å°†æ¨¡å‹æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­ä¸å†ä½¿ç”¨
                    self.failed_models.add(model_name)
                    print(f"      ğŸš« å°†æ¨¡å‹ {model_name} æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­è¯„ä¼°å°†ä¸å†ä½¿ç”¨")
                    
                    return {
                        'success': False,
                        'question_number': question_number,
                        'model': model_name,
                        'error': error_msg,
                        'raw_response': 'API call failed after retries and model marked as failed'
                    }
                
                continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•

            content = eval_result['response']

            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not content or content.strip() == "":
                error_msg = 'APIå“åº”ä¸ºç©º'
                print(f"      âŒ {model_config['name']}: {error_msg} (å°è¯• {attempt + 1}/{max_retries})")
                
                if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                    # å°†æ¨¡å‹æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­ä¸å†ä½¿ç”¨
                    self.failed_models.add(model_name)
                    print(f"      ğŸš« å°†æ¨¡å‹ {model_name} æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­è¯„ä¼°å°†ä¸å†ä½¿ç”¨")
                    
                    return {
                        'success': False,
                        'question_number': question_number,
                        'model': model_name,
                        'error': error_msg,
                        'raw_response': 'No content after retries and model marked as failed'
                    }
                
                continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•

            # è§£æJSON - æå–```json```åŒ…è£¹çš„å†…å®¹
            try:
                print(f"      ğŸ” {model_config['name']} è§£æJSONå“åº”...")

                # å…ˆå°è¯•åŒ¹é…```json```åŒ…è£¹çš„å†…å®¹
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                    print(f"      âœ… {model_config['name']} æ‰¾åˆ°```json```åŒ…è£¹çš„å†…å®¹")
                    result = json.loads(json_str)
                else:
                    # å°è¯•åŒ¹é…å•ç‹¬çš„JSONå¯¹è±¡
                    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                        print(f"      âœ… {model_config['name']} æ‰¾åˆ°JSONå¯¹è±¡")
                        result = json.loads(json_str)
                    else:
                        # å°è¯•ç›´æ¥è§£æ
                        print(f"      âš ï¸ {model_config['name']} å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”...")
                        result = json.loads(content)

                print(f"      âœ… {model_config['name']} JSONè§£ææˆåŠŸ")

            except json.JSONDecodeError as e:
                error_msg = f'JSONè§£æå¤±è´¥: {str(e)[:100]}'
                print(f"      âŒ {model_config['name']} {error_msg} (å°è¯• {attempt + 1}/{max_retries})")
                
                if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                    # å°†æ¨¡å‹æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­ä¸å†ä½¿ç”¨
                    self.failed_models.add(model_name)
                    print(f"      ğŸš« å°†æ¨¡å‹ {model_name} æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­è¯„ä¼°å°†ä¸å†ä½¿ç”¨")
                    
                    return {
                        'success': False,
                        'question_number': question_number,
                        'model': model_name,
                        'error': error_msg,
                        'raw_response': content[:500] if content else 'No content and model marked as failed'
                    }
                
                continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•
            except Exception as e:
                error_msg = f'å“åº”å¤„ç†å¤±è´¥: {str(e)}'
                print(f"      âŒ {model_config['name']} {error_msg} (å°è¯• {attempt + 1}/{max_retries})")
                
                if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥
                    # å°†æ¨¡å‹æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­ä¸å†ä½¿ç”¨
                    self.failed_models.add(model_name)
                    print(f"      ğŸš« å°†æ¨¡å‹ {model_name} æ ‡è®°ä¸ºå¤±è´¥ï¼Œåç»­è¯„ä¼°å°†ä¸å†ä½¿ç”¨")
                    
                    return {
                        'success': False,
                        'question_number': question_number,
                        'model': model_name,
                        'error': error_msg,
                        'raw_response': content[:500] if content else 'No content and model marked as failed'
                    }
                
                continue  # ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•

            # éªŒè¯å¹¶ä¿®æ­£è¯„åˆ†æ ‡å‡†
            if 'scores' in result:
                corrected_scores = self._validate_scores(result['scores'])
                
                if corrected_scores != result['scores']:
                    invalid_scores = []
                    for trait in result['scores']:
                        if result['scores'][trait] != corrected_scores[trait]:
                            invalid_scores.append(f"{trait}:{result['scores'][trait]}â†’{corrected_scores[trait]}")
                    print(f"      âš ï¸ {model_config['name']} ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")
                
                result['scores'] = corrected_scores

            result['model'] = model_name
            result['question_number'] = question_number
            result['processing_time'] = time.time()

            return result

        # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯
        return {
            'success': False,
            'question_number': question_number,
            'model': model_name,
            'error': f'åˆ†æå¤±è´¥: ç»è¿‡ {max_retries} æ¬¡å°è¯•ä»ç„¶å¤±è´¥',
            'raw_response': 'Analysis failed after retries'
        }

    def _calculate_model_consistency(self, model_results: List[Dict]) -> Dict:
        """
        è®¡ç®—å¤šä¸ªæ¨¡å‹é—´çš„ä¸€è‡´æ€§
        """
        if len(model_results) < 2:
            return {"error": "éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹çš„ç»“æœ"}

        # ä¿®æ­£ï¼šæ ¹æ®ä¼ å…¥çš„å‚æ•°ç±»å‹è¿›è¡Œå¤„ç†ï¼Œä¼ å…¥çš„æ˜¯åŒ…å«æ¨¡å‹åç§°å’Œè¯„åˆ†çš„å­—å…¸åˆ—è¡¨
        successful_models = []
        for result in model_results:
            if isinstance(result, dict) and 'model' in result and 'scores' in result:
                successful_models.append(result)
            elif isinstance(result, dict) and 'model' in result:
                # å¦‚æœä¼ å…¥çš„æ˜¯æ¨¡å‹åç§°å’Œå®Œæ•´ç»“æœçš„å­—å…¸ï¼Œä¾‹å¦‚åœ¨evaluate_file_with_multiple_modelsè°ƒç”¨æ—¶
                if 'final_scores' in result:
                    successful_models.append({
                        'model': result['model'],
                        'scores': result['final_scores']
                    })

        if len(successful_models) < 2:
            return {"error": f"æˆåŠŸæ¨¡å‹æ•°é‡ä¸è¶³: {len(successful_models)}/{len(model_results)}"}

        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        consistency_analysis = {}

        for trait in traits:
            scores = []
            model_names = []

            for result in successful_models:
                if 'scores' in result and trait in result['scores']:
                    scores.append(result['scores'][trait])
                    model_names.append(result['model'])

            if len(scores) >= 2:
                avg_score = statistics.mean(scores)
                max_score = max(scores)
                min_score = min(scores)
                score_range = max_score - min_score

                # ä¸€è‡´æ€§è¯„ä¼°
                if score_range == 0:
                    consistency_level = "å®Œå…¨ä¸€è‡´"
                    consistency_score = 100
                elif score_range <= 1:
                    consistency_level = "é«˜åº¦ä¸€è‡´"
                    consistency_score = 80
                elif score_range <= 2:
                    consistency_level = "ä¸­ç­‰ä¸€è‡´"
                    consistency_score = 60
                else:
                    consistency_level = "å·®å¼‚è¾ƒå¤§"
                    consistency_score = 40

                consistency_analysis[trait] = {
                    "scores": dict(zip(model_names, scores)),
                    "average": avg_score,
                    "range": score_range,
                    "consistency_level": consistency_level,
                    "consistency_score": consistency_score
                }

        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§
        overall_scores = [analysis.get('consistency_score', 0) for analysis in consistency_analysis.values()]
        overall_consistency = statistics.mean(overall_scores) if overall_scores else 0

        return {
            "trait_analysis": consistency_analysis,
            "overall_consistency": overall_consistency,
            "successful_models": len(successful_models),
            "total_models": len(model_results),
            "discrepancies": [trait for trait, analysis in consistency_analysis.items() if analysis.get("range", 0) > 1]
        }

    def analyze_file_with_three_models(self, file_path: str, output_dir: str) -> Dict:
        """
        ä½¿ç”¨ä¸‰ä¸ªæ¨¡å‹ç‹¬ç«‹åˆ†æå•ä¸ªæ–‡ä»¶ï¼ˆä¿ç•™æ­¤æ–¹æ³•ä»¥ä¿æŒå‘åå…¼å®¹ï¼‰
        """
        return self.evaluate_file_with_multiple_models(file_path, output_dir)

    def evaluate_file_with_multiple_models(self, file_path: str, output_dir: str, segment_size: int = None) -> Dict:
        """
        ä½¿ç”¨å¤šä¸ªæ¨¡å‹è¯„ä¼°å•ä¸ªæ–‡ä»¶ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
        å¦‚æœæœªæŒ‡å®šsegment_sizeï¼Œåˆ™ä½¿ç”¨å®ä¾‹å˜é‡self.segment_size
        """
        print(f"ğŸ“ˆ å¼€å§‹å¤šæ¨¡å‹è¯„ä¼°: {Path(file_path).name}")

        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–é—®é¢˜
            questions = []
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for item in data['assessment_results']:
                    if isinstance(item, dict):
                        question_data = item.get('question_data', {})
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', 
                                question_data.get('mapped_ipip_concept', ''))
                            
                            answer_text = item.get('extracted_response', '')
                            
                            if question_text and answer_text:
                                questions.append({
                                    'question_id': item.get('question_id'),
                                    'question_data': question_data,
                                    'extracted_response': answer_text
                                })

            if len(questions) < 1:
                raise Exception(f"é—®é¢˜æ•°é‡ä¸è¶³ï¼š{len(questions)}")

            # åˆ†æ®µå¤„ç†ï¼ˆä½¿ç”¨æŒ‡å®šçš„åˆ†æ®µå¤§å°æˆ–é»˜è®¤å€¼ï¼‰
            if segment_size is None:
                segment_size = self.segment_size
                
            segments = self._create_segments(questions, segment_size)
            total_segments = len(segments)
            print(f"  ğŸ“Š {len(questions)}é¢˜ -> {total_segments}ä¸ªåˆ†æ®µ (æ¯æ®µ{segment_size}é¢˜)")

            # ä¸ºå½“å‰æ–‡ä»¶åˆ›å»ºä¸´æ—¶ç›®å½•ä¿å­˜åˆ†æ®µ
            file_stem = Path(file_path).stem
            temp_dir = Path(output_dir) / "temp_segments" / file_stem
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜åˆ†æ®µæ–‡ä»¶
            for i, segment in enumerate(segments, 1):
                segment_file = temp_dir / f"segment_{i:03d}.json"
                with open(segment_file, 'w', encoding='utf-8') as f:
                    json.dump({
                        "segment_info": {
                            "segment_number": i,
                            "total_segments": total_segments,
                            "questions_count": len(segment),
                        },
                        "segment_data": segment
                    }, f, ensure_ascii=False, indent=2)
            
            print(f"  ğŸ“ åˆ†æ®µæ–‡ä»¶å·²ä¿å­˜åˆ°: {temp_dir}")

            # æ¨¡å‹è¯„ä¼°ç»“æœå­˜å‚¨
            model_analysis_results = {}

            # é€‰æ‹©å‰3ä¸ªæ¨¡å‹è¿›è¡Œåˆå§‹è¯„ä¼°
            selected_models = [m for m in self.models[:3] if m['name'] not in self.failed_models]
            if not selected_models:
                print("  âŒ æ‰€æœ‰æ¨¡å‹éƒ½å·²å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
                return {
                    'success': False,
                    'file_path': file_path,
                    'error': 'æ‰€æœ‰æ¨¡å‹éƒ½å·²å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°'
                }

            # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œç‹¬ç«‹è¯„ä¼°
            for model_config in selected_models:
                print(f"  ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_config['name']} ({model_config['description']})")

                model_segments = []
                segment_results = []

                # è¯„ä¼°æ¯ä¸ªåˆ†æ®µ
                for i, segment in enumerate(segments, 1):
                    result = self._analyze_segment_with_model(model_config, segment, i, total_segments)
                    segment_results.append(result)

                    if result['success']:
                        print(f"      âœ… æ®µ{i}: {list(result['scores'].values())}")
                    else:
                        print(f"      âŒ æ®µ{i}: {result.get('error', 'Unknown error')}")

                    time.sleep(3)  # APIé™åˆ¶

                # è®¡ç®—è¯¥æ¨¡å‹çš„æœ€ç»ˆè¯„åˆ†
                if segment_results:
                    final_scores = {}
                    for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                        all_scores = []
                        for result in segment_results:
                            if result.get('success') and 'scores' in result:
                                if trait in result['scores']:
                                    all_scores.append(result['scores'][trait])

                        if all_scores:
                            final_scores[trait] = statistics.median(all_scores)
                            final_scores[trait] = int(round(final_scores[trait]))  # ç¡®ä¿æ˜¯æ•´æ•°

                    model_analysis_results[model_config['name']] = {
                        "segment_results": segment_results,
                        "final_scores": final_scores,
                        "successful_segments": len([r for r in segment_results if r.get('success')]),
                        "total_segments": total_segments
                    }

            # è®¡ç®—æ¨¡å‹é—´ä¸€è‡´æ€§
            print(f"  ğŸ“Š è®¡ç®—æ¨¡å‹ä¸€è‡´æ€§...")
            final_scores_list = [
                {"model": model, "scores": results["final_scores"]}
                for model, results in model_analysis_results.items()
            ]
            
            consistency_analysis = self._calculate_model_consistency(final_scores_list)

            # åˆ›å»ºä¿¡åº¦éªŒè¯å™¨å¹¶è®¡ç®—ä¿¡åº¦
            print(f"  ğŸ“Š è®¡ç®—ä¿¡åº¦æŒ‡æ ‡...")
            reliability_validator = ReliabilityValidator(threshold=0.8)
            reliability_metrics = reliability_validator.calculate_overall_reliability(model_analysis_results)
            reliability_report = reliability_validator.generate_reliability_report(model_analysis_results, reliability_metrics)
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨äº‰è®®
            print(f"  ğŸ” æ£€æŸ¥è¯„ä¼°äº‰è®®...")
            all_scores = []
            for model_name, results in model_analysis_results.items():
                for segment_result in results['segment_results']:
                    if segment_result.get('success') and 'scores' in segment_result:
                        # å°†æ®µç»“æœè½¬æ¢ä¸ºé—®é¢˜çº§åˆ«ç»“æœï¼ˆç®€åŒ–å¤„ç†ï¼‰
                        for trait, score in segment_result['scores'].items():
                            all_scores.append({
                                'question_id': f"segment_{segment_result['segment_number']}_{trait}",
                                'trait': trait,
                                'score': score,
                                'model': model_name
                            })
            
            # æ›´æ–°ï¼šå°†model_analysis_resultså­˜å‚¨ä¸ºå®ä¾‹å˜é‡ï¼Œä»¥ä¾¿åœ¨äº‰è®®è§£å†³ä¸­ä½¿ç”¨
            self.model_analysis_results = model_analysis_results
            
            dispute_manager = EnhancedDisputeResolutionManager()
            disputes = dispute_manager.identify_disputes(all_scores, threshold=1)
            
            if disputes:
                print(f"  âš ï¸  å‘ç° {len(disputes)} ä¸ªäº‰è®®")
                # å¦‚æœå­˜åœ¨äº‰è®®ï¼Œå°è¯•ä½¿ç”¨é¢å¤–è¯„ä¼°å™¨è§£å†³
                resolved_disputes = dispute_manager.resolve_disputes_with_additional_evaluators(
                    self, disputes, all_scores, questions, segment_size
                )
            else:
                print(f"  âœ… æœªå‘ç°æ˜¾è‘—äº‰è®®")
                resolved_disputes = None

            # è®¡ç®—æœ€ç»ˆè¯„åˆ† - åŸºäºäº‰è®®è§£å†³åçš„ç»“æœ
            final_combined_scores = self._calculate_final_scores_after_resolution(
                model_analysis_results, resolved_disputes
            )

            # æ‰§è¡Œäººæ ¼åˆ†æ
            print(f"  ğŸ§  æ‰§è¡Œäººæ ¼åˆ†æ...")
            personality_analyzer = PersonalityAnalyzer()
            personality_analysis = personality_analyzer.analyze_personality(final_combined_scores)

            # ä¿å­˜ç»“æœ
            output_filename = f"{Path(file_path).stem}_segmented_scoring_evaluation.json"
            output_path = os.path.join(output_dir, output_filename)

            analysis_result = {
                "file_info": {
                    "filename": Path(file_path).name,
                    "total_questions": len(questions),
                    "segments_count": total_segments,
                    "questions_per_segment": segment_size,
                    "analysis_date": datetime.now().isoformat()
                },
                "models_used": selected_models,
                "model_results": model_analysis_results,
                "consistency_analysis": consistency_analysis,
                "reliability_analysis": {
                    "metrics": reliability_metrics,
                    "report": reliability_report
                },
                "dispute_analysis": {
                    "disputes_identified": len(disputes),
                    "resolved_disputes": resolved_disputes,
                    "dispute_resolution_needed": len(disputes) > 0,
                    "final_combined_scores": final_combined_scores
                },
                "personality_analysis": personality_analysis,
                "segmentation_info": {
                    "temp_directory": str(temp_dir),
                    "segment_files_count": total_segments,
                },
                "summary": {
                    "overall_consistency": consistency_analysis.get('overall_consistency', 0),
                    "overall_reliability": reliability_metrics.get('overall_reliability', 0),
                    "model_count": len(selected_models),
                    "successful_models": consistency_analysis.get('successful_models', 0),
                    "reliability_passed": reliability_report.get('validation_passed', False),
                    "final_scores": final_combined_scores
                }
            }

            os.makedirs(output_dir, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)

            print(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_filename}")

            # æ˜¾ç¤ºç®€è¦ç»“æœ
            print(f"  ğŸ“‹ è¯„ä¼°ç»“æœæ‘˜è¦:")
            for model, results in model_analysis_results.items():
                print(f"    {model}: {results['final_scores']} ({results['successful_segments']}/{results['total_segments']}æ®µæˆåŠŸ)")

            print(f"  ğŸ¯ æ¨¡å‹ä¸€è‡´æ€§: {consistency_analysis.get('overall_consistency', 0):.1f}%")
            print(f"  ğŸ¯ æ•´ä½“ä¿¡åº¦: {reliability_metrics.get('overall_reliability', 0):.1f}%")
            print(f"  ğŸ¯ ä¿¡åº¦éªŒè¯: {'âœ… é€šè¿‡' if reliability_report.get('validation_passed', False) else 'âŒ æœªé€šè¿‡'}")
            print(f"  ğŸ§  MBTIç±»å‹: {personality_analysis['mbti_analysis']['mbti_type']}")
            print(f"  ğŸ§  å¤§äº”äººæ ¼æ¦‚è¦: {personality_analysis['big5_analysis']['summary']['big5_profile']}")

            return {
                'success': True,
                'file_path': file_path,
                'output_path': output_path,
                'model_results': model_analysis_results,
                'consistency_analysis': consistency_analysis,
                'reliability_analysis': {
                    'metrics': reliability_metrics,
                    'report': reliability_report
                },
                'personality_analysis': personality_analysis,
                'final_combined_scores': final_combined_scores,
                'segmentation_info': {
                    'temp_directory': str(temp_dir),
                    'segment_files_count': total_segments,
                },
                'consistency_score': consistency_analysis.get('overall_consistency', 0),
                'reliability_score': reliability_metrics.get('overall_reliability', 0),
                'reliability_passed': reliability_report.get('validation_passed', False)
            }

        except Exception as e:
            print(f"  âŒ æ–‡ä»¶è¯„ä¼°å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e)
            }

    def _calculate_final_scores_after_resolution(self, model_analysis_results: Dict, resolved_disputes: Dict) -> Dict[str, float]:
        """
        åœ¨äº‰è®®è§£å†³åè®¡ç®—æœ€ç»ˆè¯„åˆ†
        """
        # é¦–å…ˆè·å–å„æ¨¡å‹çš„æœ€ç»ˆè¯„åˆ†
        all_model_scores = []
        for model_name, results in model_analysis_results.items():
            if 'final_scores' in results:
                all_model_scores.append(results['final_scores'])
        
        # å¦‚æœæ²¡æœ‰äº‰è®®æˆ–äº‰è®®è§£å†³ä¸ºç©ºï¼Œç›´æ¥è¿”å›å¹³å‡åˆ†
        if not resolved_disputes or not resolved_disputes.get('resolved_results'):
            # è®¡ç®—æ‰€æœ‰æ¨¡å‹è¯„åˆ†çš„å¹³å‡å€¼
            final_scores = {}
            traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
            
            for trait in traits:
                scores = []
                for model_scores in all_model_scores:
                    if trait in model_scores:
                        scores.append(model_scores[trait])
                
                if scores:
                    final_scores[trait] = statistics.mean(scores)
                    final_scores[trait] = round(final_scores[trait], 2)  # ä¿ç•™ä¸¤ä½å°æ•°
                else:
                    final_scores[trait] = 3.0  # é»˜è®¤ä¸­æ€§åˆ†
            
            return final_scores
        
        # å¦‚æœæœ‰äº‰è®®è§£å†³ç»“æœï¼Œéœ€è¦ç»“åˆåŸå§‹è¯„åˆ†å’Œè§£å†³åçš„è¯„åˆ†
        final_scores = {}
        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        
        for trait in traits:
            scores = []
            # æ·»åŠ æ¨¡å‹åŸå§‹è¯„åˆ†
            for model_scores in all_model_scores:
                if trait in model_scores:
                    scores.append(model_scores[trait])
            
            # æ·»åŠ äº‰è®®è§£å†³åçš„è¯„åˆ†
            resolved_results = resolved_disputes.get('resolved_results', [])
            for resolved in resolved_results:
                if resolved.get('trait') == trait:
                    scores.append(resolved['final_score'])
            
            if scores:
                final_scores[trait] = statistics.mean(scores)
                final_scores[trait] = round(final_scores[trait], 2)  # ä¿ç•™ä¸¤ä½å°æ•°
            else:
                final_scores[trait] = 3.0  # é»˜è®¤ä¸­æ€§åˆ†
        
        return final_scores


class ScoringConsistencyAnalyzer:
    """
    è¯„åˆ†ä¸€è‡´æ€§åˆ†æå™¨
    """
    def __init__(self):
        pass

    def calculate_consistency(self, model_results: List[Dict]) -> Dict:
        """
        è®¡ç®—å¤šä¸ªæ¨¡å‹é—´çš„ä¸€è‡´æ€§
        """
        if len(model_results) < 2:
            return {"error": "éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹çš„ç»“æœ"}

        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        consistency_analysis = {}

        for trait in traits:
            scores = []
            model_names = []

            for result in model_results:
                if 'scores' in result and trait in result['scores']:
                    scores.append(result['scores'][trait])
                    model_names.append(result['model'])

            if len(scores) >= 2:
                avg_score = statistics.mean(scores)
                max_score = max(scores)
                min_score = min(scores)
                score_range = max_score - min_score

                # ä¸€è‡´æ€§è¯„ä¼°
                if score_range == 0:
                    consistency_level = "å®Œå…¨ä¸€è‡´"
                    consistency_score = 100
                elif score_range <= 1:
                    consistency_level = "é«˜åº¦ä¸€è‡´"
                    consistency_score = 80
                elif score_range <= 2:
                    consistency_level = "ä¸­ç­‰ä¸€è‡´"
                    consistency_score = 60
                else:
                    consistency_level = "å·®å¼‚è¾ƒå¤§"
                    consistency_score = 40

                consistency_analysis[trait] = {
                    "scores": dict(zip(model_names, scores)),
                    "average": avg_score,
                    "range": score_range,
                    "consistency_level": consistency_level,
                    "consistency_score": consistency_score
                }

        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§
        overall_scores = [analysis.get('consistency_score', 0) for analysis in consistency_analysis.values()]
        overall_consistency = statistics.mean(overall_scores) if overall_scores else 0

        return {
            "trait_analysis": consistency_analysis,
            "overall_consistency": overall_consistency,
            "successful_models": len(model_results),
            "total_models": len(model_results)
        }


class EnhancedDisputeResolutionManager:
    """
    å¢å¼ºåˆ†æ­§å¤„ç†ç®¡ç†å™¨
    """
    def __init__(self):
        # å®šä¹‰é¢å¤–çš„è¯„ä¼°å™¨åˆ—è¡¨ï¼Œç”¨äºåˆ†æ­§å¤„ç†
        self.dispute_models = [
            {"name": "google/gemini-2.0-flash-exp:free", "description": "Google Gemini 2.0 Flash (1Mä¸Šä¸‹æ–‡)"},
            {"name": "moonshotai/kimi-k2:free", "description": "Moonshot Kimi K2 (32Kä¸Šä¸‹æ–‡)"},
            {"name": "anthropic/claude-3-haiku", "description": "Claude 3 Haiku (200Kä¸Šä¸‹æ–‡)"}
        ]
        
        # Ollamaæ¨¡å‹åˆ—è¡¨ï¼ˆå¤‡ç”¨ï¼‰ç”¨äºåˆ†æ­§è§£å†³
        self.ollama_dispute_models = [
            {"name": "qwen3:4b", "description": "Qwen3 4B (æœ¬åœ°æ¨¡å‹)"},
            {"name": "gemma2:2b", "description": "Gemma2 2B (æœ¬åœ°æ¨¡å‹)"},
            {"name": "llama3.2:3b", "description": "Llama3.2 3B (æœ¬åœ°æ¨¡å‹)"}
        ]

    def identify_disputes(self, all_scores: List[Dict], threshold: int = 1) -> List[Dict]:
        """
        è¯†åˆ«è¯„åˆ†ä¸­çš„åˆ†æ­§
        """
        # æŒ‰é—®é¢˜IDå’Œç‰¹è´¨åˆ†ç»„è¯„åˆ†
        scores_by_question_trait = {}
        for score_record in all_scores:
            # ç¡®ä¿è®°å½•åŒ…å«å¿…è¦çš„å­—æ®µ
            if 'question_id' not in score_record or 'trait' not in score_record or 'score' not in score_record:
                continue
                
            qid = score_record['question_id']
            trait = score_record['trait']
            
            key = f"{qid}_{trait}"
            if key not in scores_by_question_trait:
                scores_by_question_trait[key] = []
            scores_by_question_trait[key].append(score_record)

        disputes = []
        for key, scores_list in scores_by_question_trait.items():
            # æå–æ‰€æœ‰è¯„åˆ†
            scores = [record['score'] for record in scores_list]
            if len(scores) < 2:
                continue

            max_score = max(scores)
            min_score = min(scores)
            score_range = max_score - min_score

            if score_range > threshold:
                # ä»keyä¸­æå–é—®é¢˜idå’Œç‰¹è´¨
                parts = key.split('_', 1)
                if len(parts) == 2:
                    try:
                        qid = int(parts[0])
                        trait = parts[1]
                        disputes.append({
                            "question_id": qid,
                            "trait": trait,
                            "scores": scores,
                            "models": [record['model'] for record in scores_list],
                            "max_diff": score_range,
                            "average_score": statistics.mean(scores),
                            "evidence": [record.get('evidence', '') for record in scores_list]
                        })
                    except ValueError:
                        # å¦‚æœæ— æ³•è§£æé—®é¢˜IDä¸ºæ•´æ•°ï¼Œåˆ™è·³è¿‡
                        continue

        return disputes

    def apply_majority_decision(self, scores: List[int]) -> int:
        """
        åº”ç”¨å¤šæ•°å†³ç­–åŸåˆ™ï¼ˆå»é™¤æœ€é«˜åˆ†å’Œæœ€ä½åˆ†åå–ä¸­ä½æ•°ï¼‰
        """
        if len(scores) <= 2:
            # å¦‚æœåªæœ‰1æˆ–2ä¸ªè¯„åˆ†ï¼Œç›´æ¥å–å¹³å‡å€¼å¹¶å››èˆäº”å…¥
            return round(statistics.mean(scores)) if scores else 0

        # å»é™¤ä¸€ä¸ªæœ€é«˜åˆ†å’Œä¸€ä¸ªæœ€ä½åˆ†
        scores_sorted = sorted(scores)
        if len(scores_sorted) > 2:
            trimmed_scores = scores_sorted[1:-1]  # å»é™¤é¦–å°¾
        else:
            trimmed_scores = scores_sorted

        # è¿”å›å‰©ä½™è¯„åˆ†çš„ä¸­ä½æ•°
        if trimmed_scores:
            # å¯¹äºå¤šæ•°å†³ç­–ï¼Œä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºæœ€ç»ˆå¾—åˆ†
            return int(statistics.median(trimmed_scores))
        else:
            # å¦‚æœä¿®å‰ªåæ²¡æœ‰è¯„åˆ†ï¼Œè¿”å›åŸè¯„åˆ†çš„å¹³å‡å€¼
            return round(statistics.mean(scores)) if scores else 0
    
    def get_additional_evaluators(self, round_number: int) -> List[Dict]:
        """
        æ ¹æ®è½®æ¬¡è·å–é¢å¤–çš„è¯„ä¼°å™¨ï¼Œä¼˜å…ˆä½¿ç”¨äº‘æ¨¡å‹ï¼Œåä½¿ç”¨Ollamaæ¨¡å‹
        """
        if round_number == 1:
            # ç¬¬ä¸€è½®äº‰è®®æ—¶ï¼Œæ·»åŠ 2ä¸ªé¢å¤–è¯„ä¼°å™¨ï¼ˆä¼˜å…ˆäº‘æ¨¡å‹ï¼‰
            available_models = self.dispute_models
            return available_models[:2] if len(available_models) >= 2 else available_models
        elif round_number == 2:
            # ç¬¬äºŒè½®äº‰è®®æ—¶ï¼Œæ·»åŠ Ollamaæ¨¡å‹ä½œä¸ºå›é€€
            available_models = self.ollama_dispute_models
            return available_models[:2] if len(available_models) >= 2 else available_models
        else:
            # åç»­è½®æ¬¡ï¼Œåœ¨æ‰€æœ‰å¯ç”¨æ¨¡å‹ä¸­å¾ªç¯ä½¿ç”¨
            all_models = self.dispute_models + self.ollama_dispute_models
            start_idx = (round_number - 1) * 2  # ä»ç›¸åº”çš„æ¨¡å‹å¼€å§‹
            selected_models = []
            for i in range(2):  # é€‰æ‹©2ä¸ªæ¨¡å‹
                model_idx = (start_idx + i) % len(all_models)
                selected_models.append(all_models[model_idx])
            return selected_models
    
    def resolve_disputes_with_additional_evaluators(self, evaluator, disputes: List[Dict], 
                                                   original_results: List[Dict], 
                                                   questions: List[Dict], 
                                                   segment_size: int = 5) -> Dict:
        """
        ä½¿ç”¨é¢å¤–è¯„ä¼°å™¨è§£å†³åˆ†æ­§
        """
        print(f"ğŸ” è¯†åˆ«åˆ° {len(disputes)} ä¸ªåˆ†æ­§ï¼Œå¼€å§‹è§£å†³...")
        
        # æŒ‰é—®é¢˜ç»„ç»‡åŸå§‹ç»“æœ
        question_results = {}
        for result in original_results:
            qid = result.get('question_id')
            if qid not in question_results:
                question_results[qid] = []
            question_results[qid].append(result)
        
        resolved_results = []
        unresolved_disputes = []
        
        round_number = 1
        max_rounds = 3  # æœ€å¤šè¿›è¡Œ3è½®äº‰è®®è§£å†³
        
        current_disputes = disputes.copy()
        
        while current_disputes and round_number <= max_rounds:
            print(f"ğŸ”„ ç¬¬ {round_number} è½®äº‰è®®è§£å†³ï¼Œå½“å‰æœ‰ {len(current_disputes)} ä¸ªæœªè§£å†³é—®é¢˜")
            
            # è·å–å½“å‰è½®æ¬¡çš„é¢å¤–è¯„ä¼°å™¨
            additional_evaluators = self.get_additional_evaluators(round_number)
            if not additional_evaluators:
                print(f"âš ï¸  æ²¡æœ‰æ›´å¤šè¯„ä¼°å™¨å¯ä»¥ä½¿ç”¨ï¼Œåœæ­¢äº‰è®®è§£å†³")
                break
            
            print(f"ğŸ¤– ä½¿ç”¨é¢å¤–è¯„ä¼°å™¨: {[m['name'] for m in additional_evaluators]}")
            
            new_scores = []
            
            # å¯¹æ¯ä¸ªäº‰è®®é—®é¢˜è¿›è¡Œé¢å¤–è¯„ä¼°
            for dispute in current_disputes:
                question_id = dispute['question_id']
                
                # æ‰¾åˆ°å¯¹åº”çš„é—®é¢˜
                question = None
                for q in questions:
                    if q.get('question_id') == question_id:
                        question = q
                        break
                
                if not question:
                    continue
                
                # åˆ›å»ºé’ˆå¯¹è¯¥é—®é¢˜çš„åˆ†æ®µï¼ˆåŒ…å«äº‰è®®ç‰¹è´¨ï¼‰
                question_segment = [question]
                
                # ä½¿ç”¨é¢å¤–è¯„ä¼°å™¨å¯¹äº‰è®®é—®é¢˜è¿›è¡Œè¯„ä¼°
                for model_config in additional_evaluators:
                    result = evaluator._analyze_segment_with_model(
                        model_config, 
                        question_segment, 
                        1,  # åˆ†æ®µç¼–å·ï¼Œè¿™é‡Œåªå¤„ç†å•ä¸ªé—®é¢˜
                        1,  # æ€»åˆ†æ®µæ•°
                        max_retries=2  # äº‰è®®è§£å†³æ—¶å‡å°‘é‡è¯•æ¬¡æ•°ä»¥æé«˜æ•ˆç‡
                    )
                    
                    if result['success']:
                        # æå–äº‰è®®ç‰¹è´¨çš„è¯„åˆ†
                        trait = dispute['trait']
                        if 'scores' in result and trait in result['scores']:
                            new_scores.append({
                                'question_id': question_id,
                                'trait': trait,
                                'score': result['scores'][trait],
                                'model': model_config['name'],
                                'round': round_number,
                                'evidence': result.get('evidence', {}).get(trait, '')
                            })
            
            # é‡æ–°è¯„ä¼°äº‰è®® - åˆå¹¶åŸå§‹è¯„åˆ†å’Œæ–°è¯„åˆ†
            all_current_scores = []
            
            # æ·»åŠ åŸå§‹è¯„åˆ†ï¼ˆä»segment_resultsä¸­æå–ï¼‰
            for model_name, model_result in evaluator.model_analysis_results.items():
                for segment_result in model_result['segment_results']:
                    if segment_result.get('success') and 'scores' in segment_result:
                        for trait, score in segment_result['scores'].items():
                            all_current_scores.append({
                                'question_id': segment_result.get('segment_number', 0),  # ä½¿ç”¨åˆ†æ®µå·ä½œä¸ºé—®é¢˜ID
                                'trait': trait,
                                'score': score,
                                'model': model_name
                            })
            
            # æ·»åŠ æ–°è¯„åˆ†
            for score in new_scores:
                all_current_scores.append(score)
            
            # é‡æ–°è¯†åˆ«åˆ†æ­§
            updated_disputes = self.identify_disputes(
                all_current_scores, 
                threshold=1
            )
            
            print(f"ğŸ“Š ç¬¬ {round_number} è½®åï¼Œä»æœ‰ {len(updated_disputes)} ä¸ªäº‰è®®")
            
            # å¦‚æœæ²¡æœ‰äº‰è®®äº†ï¼Œè·³å‡ºå¾ªç¯
            if not updated_disputes:
                print(f"âœ… æ‰€æœ‰äº‰è®®åœ¨ç¬¬ {round_number} è½®åå¾—åˆ°è§£å†³")
                break
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ¬¡
            if round_number >= max_rounds:
                print(f"âš ï¸  å·²è¾¾åˆ°æœ€å¤§äº‰è®®è§£å†³è½®æ¬¡({max_rounds})ï¼Œä»æœ‰ {len(updated_disputes)} ä¸ªäº‰è®®æœªè§£å†³")
                unresolved_disputes = updated_disputes
                break
                
            # æ›´æ–°äº‰è®®åˆ—è¡¨
            current_disputes = updated_disputes
            round_number += 1
        
        # ä¸ºäº†è§£å†³å¾ªç¯å¼•ç”¨é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨å¤šæ•°å†³ç­–æ¥æœ€ç»ˆç¡®å®šè¯„åˆ†
        final_resolved_scores = []
        for dispute in unresolved_disputes:
            final_score = self.apply_majority_decision(dispute['scores'])
            final_resolved_scores.append({
                'question_id': dispute['question_id'],
                'trait': dispute['trait'],
                'initial_dispute': dispute['max_diff'],
                'final_score': final_score
            })
        
        return {
            'resolved_results': final_resolved_scores,  # åŒ…å«æœ€ç»ˆè§£å†³çš„è¯„åˆ†
            'unresolved_disputes': unresolved_disputes,
            'new_scores': new_scores,
            'rounds_used': round_number,
            'dispute_resolution_status': 'complete' if not updated_disputes else 'partial'
        }


# åˆ›å»ºä¸€ä¸ªå…¼å®¹æ—§ç‰ˆæœ¬çš„ç±»
class DisputeResolutionManager(EnhancedDisputeResolutionManager):
    """
    ä¿æŒä¸æ—§ä»£ç å…¼å®¹çš„åˆ†æ­§å¤„ç†ç®¡ç†å™¨
    """
    pass


class ReliabilityValidator:
    """
    ä¿¡åº¦éªŒè¯å™¨
    """
    def __init__(self, threshold=0.8):
        """
        åˆå§‹åŒ–ä¿¡åº¦éªŒè¯å™¨
        :param threshold: ä¿¡åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.8
        """
        self.threshold = threshold

    def calculate_cronbach_alpha(self, scores_matrix: List[List[float]]) -> float:
        """
        è®¡ç®—Cronbach's Alphaç³»æ•°
        :param scores_matrix: è¯„åˆ†çŸ©é˜µï¼Œæ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªè¯„ä¼°å™¨å¯¹æ‰€æœ‰é—®é¢˜çš„è¯„åˆ†ï¼Œæ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ªé—®é¢˜çš„æ‰€æœ‰è¯„ä¼°å™¨è¯„åˆ†
        :return: Cronbach's Alphaç³»æ•°
        """
        if np is None:
            print("âš ï¸ æœªå®‰è£…numpyï¼ŒCronbach's Alphaè®¡ç®—ä¸å¯ç”¨")
            return 0.0
        
        scores = np.array(scores_matrix)
        
        if scores.size == 0:
            return 0.0
            
        # æ£€æŸ¥çŸ©é˜µç»´åº¦
        if len(scores.shape) < 2 or scores.shape[0] < 2 or scores.shape[1] < 2:
            return 0.0  # éœ€è¦è‡³å°‘2ä¸ªè¯„ä¼°å™¨å’Œ2ä¸ªé—®é¢˜
        
        # è®¡ç®—æ¯é“é¢˜ï¼ˆæ¯åˆ—ï¼‰çš„æ–¹å·®ï¼ˆé¡¹ç›®é—´æ–¹å·®ï¼‰
        item_variances = np.var(scores, axis=0, ddof=1)  # æ¯åˆ—(é¢˜ç›®)çš„æ–¹å·®ï¼Œä½¿ç”¨æ ·æœ¬æ–¹å·®
        sum_of_item_variances = np.sum(item_variances)
        
        # è®¡ç®—æ¯ä¸ªè¯„ä¼°å™¨ï¼ˆæ¯è¡Œï¼‰çš„æ€»åˆ†
        rater_totals = np.sum(scores, axis=1)  # æ¯è¡Œ(è¯„ä¼°å™¨)çš„æ€»å’Œ
        
        # è®¡ç®—æ€»åˆ†æ–¹å·®ï¼ˆè¯„ä¼°è€…é—´æ–¹å·®ï¼‰
        total_scores_variance = np.var(rater_totals, ddof=1)  # ä½¿ç”¨æ ·æœ¬æ–¹å·®
        
        if total_scores_variance == 0:
            # å¦‚æœæ‰€æœ‰è¯„ä¼°å™¨çš„æ€»åˆ†ç›¸åŒï¼Œè¯´æ˜å®Œå…¨ä¸€è‡´
            if sum_of_item_variances == 0:
                # æ‰€æœ‰å€¼éƒ½ç›¸åŒ
                return 1.0
            else:
                # æ¯åˆ—å†…éƒ¨ä¸åŒï¼Œä½†æ¯è¡Œæ€»åˆ†ç›¸åŒ
                return 0.0 if sum_of_item_variances > 0 else 1.0
        
        n_items = scores.shape[1]  # é—®é¢˜æ•°é‡
        
        # Cronbach's Alphaå…¬å¼
        # Î± = (k / (k-1)) * (1 - Î£siÂ² / sTÂ²)
        # å…¶ä¸­ k æ˜¯é¢˜ç›®æ•°ï¼ŒsiÂ² æ˜¯æ¯ä¸ªé¢˜ç›®çš„æ–¹å·®ï¼ŒsTÂ² æ˜¯æ€»åˆ†æ–¹å·®
        if sum_of_item_variances == 0:
            return 1.0  # æ¯ä¸ªåˆ—å†…éƒ¨å®Œå…¨ä¸€è‡´ï¼Œä½†è¡Œé—´å¯èƒ½ä¸åŒ
        
        alpha = (n_items / (n_items - 1)) * (1 - sum_of_item_variances / total_scores_variance)
        
        return max(0.0, min(1.0, alpha))  # ç¡®ä¿Alphaåœ¨0-1ä¹‹é—´

    def calculate_inter_rater_reliability(self, scores_by_trait: Dict[str, List[float]]) -> Dict[str, float]:
        """
        è®¡ç®—è¯„ä¼°è€…é—´ä¿¡åº¦
        :param scores_by_trait: æŒ‰ç‰¹è´¨åˆ†ç»„çš„è¯„åˆ†
        :return: æ¯ä¸ªç‰¹è´¨çš„ä¿¡åº¦ç³»æ•°
        """
        reliability_scores = {}
        
        for trait, scores_list in scores_by_trait.items():
            if len(scores_list) < 2:
                reliability_scores[trait] = 0.0
                continue
            
            # ä½¿ç”¨è¯„åˆ†å·®å¼‚çš„å€’æ•°ä½œä¸ºä¸€è‡´æ€§æŒ‡æ ‡
            if len(set(scores_list)) == 1:  # æ‰€æœ‰è¯„åˆ†ç›¸åŒ
                reliability_scores[trait] = 1.0
            elif len(scores_list) >= 2:
                # è®¡ç®—æ ‡å‡†å·®ï¼Œæ ‡å‡†å·®è¶Šå°ä¸€è‡´æ€§è¶Šé«˜
                std_dev = statistics.stdev(scores_list) if len(scores_list) > 1 else 0
                max_score_range = max(scores_list) - min(scores_list)
                
                # å¦‚æœæ ‡å‡†å·®ä¸º0ï¼Œè¯´æ˜å®Œå…¨ä¸€è‡´
                if std_dev == 0:
                    reliability_scores[trait] = 1.0
                else:
                    # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´ï¼Œä¸€è‡´æ€§è¶Šé«˜åˆ†æ•°è¶Šé«˜
                    # é€šè¿‡è¯„åˆ†èŒƒå›´å’Œæ ‡å‡†å·®æ¥ä¼°è®¡ä¸€è‡´æ€§
                    if max_score_range > 0:
                        # ä¸€è‡´æ€§ = 1 - (æ ‡å‡†å·®/è¯„åˆ†èŒƒå›´)
                        consistency = max(0, 1 - (std_dev / max_score_range))
                        reliability_scores[trait] = consistency
                    else:
                        reliability_scores[trait] = 0.0
            else:
                reliability_scores[trait] = 0.0
        
        return reliability_scores

    def calculate_overall_reliability(self, model_results: Dict) -> Dict[str, float]:
        """
        è®¡ç®—æ•´ä½“ä¿¡åº¦
        :param model_results: æ¨¡å‹ç»“æœå­—å…¸
        :return: åŒ…å«å„é¡¹ä¿¡åº¦æŒ‡æ ‡çš„å­—å…¸
        """
        if not model_results:
            return {"overall_reliability": 0.0, "reliability_by_trait": {}}
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„æœ€ç»ˆè¯„åˆ†
        all_final_scores = []
        scores_by_trait = {
            "openness_to_experience": [],
            "conscientiousness": [],
            "extraversion": [],
            "agreeableness": [],
            "neuroticism": []
        }
        
        for model_name, results in model_results.items():
            if 'final_scores' in results:
                final_scores = results['final_scores']
                model_scores = []
                
                for trait in scores_by_trait.keys():
                    if trait in final_scores:
                        score = final_scores[trait]
                        scores_by_trait[trait].append(score)
                        model_scores.append(score)
                
                if model_scores:
                    all_final_scores.append(model_scores)
        
        # è®¡ç®—å„é¡¹ä¿¡åº¦æŒ‡æ ‡
        trait_reliability = self.calculate_inter_rater_reliability(scores_by_trait)
        
        # è®¡ç®—Cronbach's Alpha
        if len(all_final_scores) >= 2 and np is not None:
            cronbach_alpha = self.calculate_cronbach_alpha(all_final_scores)
        else:
            cronbach_alpha = 0.0  # å¦‚æœnumpyä¸å¯ç”¨æˆ–è¯„ä¼°å™¨ä¸è¶³ï¼Œè®¾ä¸º0
        
        # è®¡ç®—å¹³å‡ä¿¡åº¦
        avg_reliability = statistics.mean(trait_reliability.values()) if trait_reliability.values() else 0.0
        
        return {
            "overall_reliability": avg_reliability,
            "reliability_by_trait": trait_reliability,
            "cronbach_alpha": cronbach_alpha,
            "avg_reliability": avg_reliability
        }

    def validate_reliability(self, reliability_metrics: Dict) -> bool:
        """
        éªŒè¯ä¿¡åº¦æ˜¯å¦æ»¡è¶³è¦æ±‚
        :param reliability_metrics: ä¿¡åº¦æŒ‡æ ‡å­—å…¸
        :return: æ˜¯å¦é€šè¿‡éªŒè¯
        """
        overall_reliability = reliability_metrics.get("overall_reliability", 0.0)
        
        # å¦‚æœCronbach's Alphaä¸å¯ç”¨ï¼ˆå› ä¸ºnumpyæœªå®‰è£…ï¼‰ï¼Œåªæ£€æŸ¥è¯„ä¼°è€…é—´ä¿¡åº¦
        cronbach_alpha = reliability_metrics.get("cronbach_alpha", 0.0)
        
        # å¦‚æœCronbach's Alphaä¸º0ï¼Œæˆ‘ä»¬åªæ£€æŸ¥è¯„ä¼°è€…é—´ä¿¡åº¦
        if cronbach_alpha == 0.0:
            # ä»…åŸºäºæ•´ä½“ä¿¡åº¦è¿›è¡ŒéªŒè¯
            return overall_reliability >= self.threshold
        else:
            # åŒæ—¶æ£€æŸ¥æ•´ä½“ä¿¡åº¦å’ŒCronbach's Alpha
            return overall_reliability >= self.threshold and cronbach_alpha >= self.threshold

    def generate_reliability_report(self, model_results: Dict, reliability_metrics: Dict) -> Dict:
        """
        ç”Ÿæˆä¿¡åº¦éªŒè¯æŠ¥å‘Š
        :param model_results: æ¨¡å‹ç»“æœ
        :param reliability_metrics: ä¿¡åº¦æŒ‡æ ‡
        :return: ä¿¡åº¦éªŒè¯æŠ¥å‘Š
        """
        validation_passed = self.validate_reliability(reliability_metrics)
        
        report = {
            "validation_date": datetime.now().isoformat(),
            "threshold": self.threshold,
            "validation_passed": validation_passed,
            "metrics": reliability_metrics,
            "summary": {
                "reliability_status": "Passed" if validation_passed else "Failed",
                "overall_reliability": reliability_metrics.get("overall_reliability", 0.0),
                "cronbach_alpha": reliability_metrics.get("cronbach_alpha", 0.0),
                "trait_count": len(reliability_metrics.get("reliability_by_trait", {})),
                "total_models": len(model_results) if model_results else 0
            }
        }
        
        return report


def main():
    """
    ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•
    """
    # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹
    evaluator = SegmentedScoringEvaluator()

    # è¾“å…¥è¾“å‡ºç›®å½•
    input_dir = "results/readonly-original"
    output_dir = "segmented_scoring_results"

    # æ‰¹é‡è¯„ä¼° (å¤„ç†éƒ¨åˆ†æ–‡ä»¶è¿›è¡Œæµ‹è¯•)
    evaluator.batch_evaluate(input_dir, output_dir, max_files=5)


if __name__ == "__main__":
    main()