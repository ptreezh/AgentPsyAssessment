#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‹¬ç«‹è¯„ä¼°å™¨åˆ†æ®µè¯„åˆ†ç³»ç»Ÿ
å®ç°5é¢˜åˆ†æ®µç‹¬ç«‹è¯„ä¼°ï¼Œåªè¿›è¡Œåˆ†æ®µè¯„åˆ†ï¼Œä¸è¿›è¡Œåç»­äººæ ¼åˆ†æ
"""
import sys
import os
import json
import time
import glob
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
import statistics
import requests
import re


class OpenRouterClient:
    """
    OpenRouter APIå®¢æˆ·ç«¯
    """
    def __init__(self, api_key: str, base_url: str = "https://openrouter.ai/api/v1"):
        self.api_key = api_key
        self.base_url = base_url

    def evaluate(self, model: str, prompt: str, system_prompt: str = None, max_tokens: int = 2000) -> Dict[str, Any]:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¯„ä¼°
        """
        try:
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            payload = {
                "model": model,
                "messages": [
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": max_tokens,
                "temperature": 0.1
            }

            if system_prompt:
                payload["messages"].insert(0, {"role": "system", "content": system_prompt})

            response = requests.post(f"{self.base_url}/chat/completions", json=payload, headers=headers, timeout=120)
            response.raise_for_status()

            result = response.json()
            return {
                "success": True,
                "response": result["choices"][0]["message"]["content"],
                "model": model,
                "raw_response": result
            }

        except requests.exceptions.RequestException as e:
            return {
                "success": False,
                "error": f"API request failed: {str(e)}",
                "model": model
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"Evaluation failed: {str(e)}",
                "model": model
            }


class SegmentedScoringEvaluator:
    """
    åˆ†æ®µè¯„åˆ†è¯„ä¼°å™¨
    """
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv('OPENROUTER_API_KEY', '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')
        self.client = OpenRouterClient(self.api_key)
        
        # ä¸»è¯„ä¼°å™¨åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åºï¼Œä¼˜å…ˆé€‰æ‹©å¤§ä¸Šä¸‹æ–‡æ¨¡å‹
        self.models = [
            {"name": "google/gemini-2.0-flash-exp:free", "description": "Google Gemini 2.0 Flash (1Mä¸Šä¸‹æ–‡)"},
            {"name": "deepseek/deepseek-r1:free", "description": "DeepSeek R1 (163Kä¸Šä¸‹æ–‡)"},
            {"name": "qwen/qwen3-235b-a22b:free", "description": "Qwen3 235B (131Kä¸Šä¸‹æ–‡)"},
            {"name": "mistralai/mistral-small-3.2-24b-instruct:free", "description": "Mistral Small (131Kä¸Šä¸‹æ–‡)"},
            {"name": "meta-llama/llama-3.3-70b-instruct:free", "description": "Llama 3.3 70B (65Kä¸Šä¸‹æ–‡)"},
            {"name": "moonshotai/kimi-k2:free", "description": "Moonshot Kimi K2 (32Kä¸Šä¸‹æ–‡)"}
        ]

    def _create_segments(self, questions: List[Dict], segment_size: int = 5) -> List[List[Dict]]:
        """
        å°†é—®é¢˜åˆ—è¡¨åˆ†æ®µï¼Œæ¯æ®µsegment_sizeé¢˜
        """
        segments = []
        for i in range(0, len(questions), segment_size):
            segment = questions[i:i+segment_size]
            if len(segment) > 0:  # ç¡®ä¿éç©ºæ®µä¹Ÿè¢«æ·»åŠ 
                segments.append(segment)
        return segments

    def _create_segment_prompt(self, segment: List[Dict], segment_number: int, total_segments: int) -> str:
        """
        åˆ›å»ºåˆ†æ®µè¯„åˆ†æç¤º
        """
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**ä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°å›ç­”è€…å±•ç°çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**å…³é”®æé†’ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…ï¼Œä¸è¦å›ç­”é—®å·é—®é¢˜
- âŒ ä¸è¦æ··æ·†è§’è‰²ï¼Œä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä¸“æ³¨äºåˆ†æå›ç­”ä¸­çš„äººæ ¼ç‰¹å¾
- âœ… å¿½ç•¥è§’è‰²æ‰®æ¼”å†…å®¹ï¼Œä¸“æ³¨å®é™…è¡Œä¸ºå€¾å‘

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ç¨‹åº¦
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æº
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- **1åˆ†**ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- **3åˆ†**ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- **5åˆ†**ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼Œä¸¥ç¦ä½¿ç”¨2ã€4ç­‰å…¶ä»–æ•°å€¼ï¼**

**ç¬¬{segment_number}æ®µé—®å·å†…å®¹ï¼ˆ{len(segment)}é¢˜/å…±{total_segments}æ®µï¼‰ï¼š**
"""

        for i, item in enumerate(segment, 1):
            question_data = item.get('question_data', {})
            prompt += f"""
**é—®é¢˜ {i}:**
{question_data.get('mapped_ipip_concept', '')}

**åœºæ™¯ {i}:**
{question_data.get('scenario', '')}

**æŒ‡ä»¤ {i}:**
{question_data.get('prompt_for_agent', '')}

**AIå›ç­” {i}:**
{item.get('extracted_response', '')}

---
"""

        prompt += """
**è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š**
```json
{
  "success": true,
  "segment_number": {segment_number},
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“è¯æ®å¼•ç”¨",
    "conscientiousness": "å…·ä½“è¯æ®å¼•ç”¨",
    "extraversion": "å…·ä½“è¯æ®å¼•ç”¨",
    "agreeableness": "å…·ä½“è¯æ®å¼•ç”¨",
    "neuroticism": "å…·ä½“è¯æ®å¼•ç”¨"
  },
  "confidence": "high/medium/low"
}
```

**å†æ¬¡æé†’ï¼šæ¯ä¸ªè¯„åˆ†å¿…é¡»æ˜¯1ã€3æˆ–5ï¼Œä¸èƒ½ä½¿ç”¨å…¶ä»–æ•°å€¼ï¼**
"""

        return prompt

    def _validate_scores(self, scores: Dict[str, int]) -> Dict[str, int]:
        """
        éªŒè¯å¹¶ä¿®æ­£è¯„åˆ†ï¼ˆç¡®ä¿åªä½¿ç”¨1ã€3ã€5åˆ†ï¼‰
        """
        valid_scores = {}
        for trait, score in scores.items():
            if score in [1, 3, 5]:
                valid_scores[trait] = score
            else:
                # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                if score < 2:
                    valid_scores[trait] = 1
                elif score > 4:
                    valid_scores[trait] = 5
                else:
                    valid_scores[trait] = 3  # 2å’Œ4ä¿®æ­£ä¸º3
        return valid_scores

    def _analyze_segment_with_model(self, model_config: Dict, segment: List[Dict], segment_number: int, total_segments: int) -> Dict:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æå•ä¸ªåˆ†æ®µ
        """
        try:
            prompt = self._create_segment_prompt(segment, segment_number, total_segments)

            print(f"    ğŸ“¡ è°ƒç”¨ {model_config['name']} åˆ†ææ®µ{segment_number}...")
            eval_result = self.client.evaluate(
                model=model_config['name'],
                prompt=prompt,
                system_prompt="ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ã€‚"
            )

            if not eval_result['success']:
                print(f"      âŒ {model_config['name']} è°ƒç”¨å¤±è´¥: {eval_result.get('error', 'Unknown error')}")
                return {
                    'success': False,
                    'segment_number': segment_number,
                    'model': model_config['name'],
                    'error': eval_result.get('error', 'API call failed'),
                    'raw_response': 'API call failed'
                }

            content = eval_result['response']

            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not content or content.strip() == "":
                return {
                    'success': False,
                    'segment_number': segment_number,
                    'model': model_config['name'],
                    'error': 'APIå“åº”ä¸ºç©º',
                    'raw_response': 'No content'
                }

            # è§£æJSON - æå–```json```åŒ…è£¹çš„å†…å®¹
            try:
                print(f"      ğŸ” {model_config['name']} è§£æJSONå“åº”...")

                # å…ˆå°è¯•åŒ¹é…```json```åŒ…è£¹çš„å†…å®¹
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                    print(f"      âœ… {model_config['name']} æ‰¾åˆ°```json```åŒ…è£¹çš„å†…å®¹")
                    result = json.loads(json_str)
                else:
                    # å°è¯•åŒ¹é…å•ç‹¬çš„JSONå¯¹è±¡
                    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                        print(f"      âœ… {model_config['name']} æ‰¾åˆ°JSONå¯¹è±¡")
                        result = json.loads(json_str)
                    else:
                        # å°è¯•ç›´æ¥è§£æ
                        print(f"      âš ï¸ {model_config['name']} å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”...")
                        result = json.loads(content)

                print(f"      âœ… {model_config['name']} JSONè§£ææˆåŠŸ")

            except json.JSONDecodeError as e:
                print(f"      âŒ {model_config['name']} JSONè§£æå¤±è´¥: {str(e)[:100]}")
                return {
                    'success': False,
                    'segment_number': segment_number,
                    'model': model_config['name'],
                    'error': f'JSONè§£æå¤±è´¥: {str(e)[:100]}',
                    'raw_response': content[:500] if content else 'No content'
                }
            except Exception as e:
                print(f"      âŒ {model_config['name']} å“åº”å¤„ç†å¤±è´¥: {str(e)}")
                return {
                    'success': False,
                    'segment_number': segment_number,
                    'model': model_config['name'],
                    'error': f'å“åº”å¤„ç†å¤±è´¥: {str(e)}',
                    'raw_response': content[:500] if content else 'No content'
                }

            # éªŒè¯å¹¶ä¿®æ­£è¯„åˆ†æ ‡å‡†
            if 'scores' in result:
                corrected_scores = self._validate_scores(result['scores'])
                
                if corrected_scores != result['scores']:
                    invalid_scores = []
                    for trait in result['scores']:
                        if result['scores'][trait] != corrected_scores[trait]:
                            invalid_scores.append(f"{trait}:{result['scores'][trait]}â†’{corrected_scores[trait]}")
                    print(f"      âš ï¸ {model_config['name']} ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")
                
                result['scores'] = corrected_scores

            result['model'] = model_config['name']
            result['segment_number'] = segment_number
            result['processing_time'] = time.time()

            return result

        except Exception as e:
            print(f"      âŒ {model_config['name']} åˆ†æå¤±è´¥: {str(e)}")
            return {
                'success': False,
                'segment_number': segment_number,
                'model': model_config['name'],
                'error': f'åˆ†æå¤±è´¥: {str(e)}',
                'raw_response': str(e)
            }

    def _calculate_model_consistency(self, model_results: List[Dict]) -> Dict:
        """
        è®¡ç®—å¤šä¸ªæ¨¡å‹é—´çš„ä¸€è‡´æ€§
        """
        if len(model_results) < 2:
            return {"error": "éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹çš„ç»“æœ"}

        successful_models = [r for r in model_results if r.get('success', False)]
        if len(successful_models) < 2:
            return {"error": f"æˆåŠŸæ¨¡å‹æ•°é‡ä¸è¶³: {len(successful_models)}/{len(model_results)}"}

        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        consistency_analysis = {}

        for trait in traits:
            scores = []
            model_names = []

            for result in successful_models:
                if 'scores' in result and trait in result['scores']:
                    scores.append(result['scores'][trait])
                    model_names.append(result['model'])

            if len(scores) >= 2:
                avg_score = statistics.mean(scores)
                max_score = max(scores)
                min_score = min(scores)
                score_range = max_score - min_score

                # ä¸€è‡´æ€§è¯„ä¼°
                if score_range == 0:
                    consistency_level = "å®Œå…¨ä¸€è‡´"
                    consistency_score = 100
                elif score_range <= 1:
                    consistency_level = "é«˜åº¦ä¸€è‡´"
                    consistency_score = 80
                elif score_range <= 2:
                    consistency_level = "ä¸­ç­‰ä¸€è‡´"
                    consistency_score = 60
                else:
                    consistency_level = "å·®å¼‚è¾ƒå¤§"
                    consistency_score = 40

                consistency_analysis[trait] = {
                    "scores": dict(zip(model_names, scores)),
                    "average": avg_score,
                    "range": score_range,
                    "consistency_level": consistency_level,
                    "consistency_score": consistency_score
                }

        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§
        overall_scores = [analysis.get('consistency_score', 0) for analysis in consistency_analysis.values()]
        overall_consistency = statistics.mean(overall_scores) if overall_scores else 0

        return {
            "trait_analysis": consistency_analysis,
            "overall_consistency": overall_consistency,
            "successful_models": len(successful_models),
            "total_models": len(model_results),
            "discrepancies": [trait for trait, analysis in consistency_analysis.items() if analysis["range"] > 1]
        }

    def analyze_file_with_three_models(self, file_path: str, output_dir: str) -> Dict:
        """
        ä½¿ç”¨ä¸‰ä¸ªæ¨¡å‹ç‹¬ç«‹åˆ†æå•ä¸ªæ–‡ä»¶ï¼ˆä¿ç•™æ­¤æ–¹æ³•ä»¥ä¿æŒå‘åå…¼å®¹ï¼‰
        """
        return self.evaluate_file_with_multiple_models(file_path, output_dir)

    def evaluate_file_with_multiple_models(self, file_path: str, output_dir: str) -> Dict:
        """
        ä½¿ç”¨å¤šä¸ªæ¨¡å‹è¯„ä¼°å•ä¸ªæ–‡ä»¶ï¼ˆä¸»è¦æ–¹æ³•ï¼‰
        """
        print(f"ğŸ“ˆ å¼€å§‹å¤šæ¨¡å‹è¯„ä¼°: {Path(file_path).name}")

        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–é—®é¢˜
            questions = []
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for item in data['assessment_results']:
                    if isinstance(item, dict):
                        question_data = item.get('question_data', {})
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', 
                                question_data.get('mapped_ipip_concept', ''))
                            
                            answer_text = item.get('extracted_response', '')
                            
                            if question_text and answer_text:
                                questions.append({
                                    'question_data': question_data,
                                    'extracted_response': answer_text
                                })

            if len(questions) < 1:
                raise Exception(f"é—®é¢˜æ•°é‡ä¸è¶³ï¼š{len(questions)}")

            # åˆ†æ®µå¤„ç†ï¼ˆæ¯æ®µ5é¢˜ï¼‰
            segment_size = 5
            segments = self._create_segments(questions, segment_size)
            total_segments = len(segments)
            print(f"  ğŸ“Š {len(questions)}é¢˜ -> {total_segments}ä¸ªåˆ†æ®µ")

            # æ¨¡å‹è¯„ä¼°ç»“æœå­˜å‚¨
            model_analysis_results = {}

            # é€‰æ‹©å‰3ä¸ªæ¨¡å‹è¿›è¡Œåˆå§‹è¯„ä¼°
            selected_models = self.models[:3]

            # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œç‹¬ç«‹è¯„ä¼°
            for model_config in selected_models:
                print(f"  ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_config['name']} ({model_config['description']})")

                model_segments = []
                segment_results = []

                # è¯„ä¼°æ¯ä¸ªåˆ†æ®µ
                for i, segment in enumerate(segments, 1):
                    result = self._analyze_segment_with_model(model_config, segment, i, total_segments)
                    segment_results.append(result)

                    if result['success']:
                        print(f"      âœ… æ®µ{i}: {list(result['scores'].values())}")
                    else:
                        print(f"      âŒ æ®µ{i}: {result.get('error', 'Unknown error')}")

                    time.sleep(3)  # APIé™åˆ¶

                # è®¡ç®—è¯¥æ¨¡å‹çš„æœ€ç»ˆè¯„åˆ†
                if segment_results:
                    final_scores = {}
                    for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                        all_scores = []
                        for result in segment_results:
                            if result.get('success') and 'scores' in result:
                                if trait in result['scores']:
                                    all_scores.append(result['scores'][trait])

                        if all_scores:
                            final_scores[trait] = statistics.median(all_scores)
                            final_scores[trait] = int(round(final_scores[trait]))  # ç¡®ä¿æ˜¯æ•´æ•°

                    model_analysis_results[model_config['name']] = {
                        "segment_results": segment_results,
                        "final_scores": final_scores,
                        "successful_segments": len([r for r in segment_results if r.get('success')]),
                        "total_segments": total_segments
                    }

            # è®¡ç®—æ¨¡å‹é—´ä¸€è‡´æ€§
            print(f"  ğŸ“Š è®¡ç®—æ¨¡å‹ä¸€è‡´æ€§...")
            final_scores_list = [
                {"model": model, "scores": results["final_scores"]}
                for model, results in model_analysis_results.items()
            ]
            
            consistency_analysis = self._calculate_model_consistency(final_scores_list)

            # ä¿å­˜ç»“æœ
            output_filename = f"{Path(file_path).stem}_segmented_scoring_evaluation.json"
            output_path = os.path.join(output_dir, output_filename)

            analysis_result = {
                "file_info": {
                    "filename": Path(file_path).name,
                    "total_questions": len(questions),
                    "segments_count": total_segments,
                    "questions_per_segment": segment_size,
                    "analysis_date": datetime.now().isoformat()
                },
                "models_used": selected_models,
                "model_results": model_analysis_results,
                "consistency_analysis": consistency_analysis,
                "summary": {
                    "overall_consistency": consistency_analysis.get('overall_consistency', 0),
                    "model_count": len(selected_models),
                    "successful_models": consistency_analysis.get('successful_models', 0)
                }
            }

            os.makedirs(output_dir, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)

            print(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_filename}")

            # æ˜¾ç¤ºç®€è¦ç»“æœ
            print(f"  ğŸ“‹ è¯„ä¼°ç»“æœæ‘˜è¦:")
            for model, results in model_analysis_results.items():
                print(f"    {model}: {results['final_scores']} ({results['successful_segments']}/{results['total_segments']}æ®µæˆåŠŸ)")

            print(f"  ğŸ¯ æ¨¡å‹ä¸€è‡´æ€§: {consistency_analysis.get('overall_consistency', 0):.1f}%")

            return {
                'success': True,
                'file_path': file_path,
                'output_path': output_path,
                'model_results': model_analysis_results,
                'consistency_analysis': consistency_analysis,
                'consistency_score': consistency_analysis.get('overall_consistency', 0)
            }

        except Exception as e:
            print(f"  âŒ æ–‡ä»¶è¯„ä¼°å¤±è´¥: {e}")
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e)
            }

    def batch_evaluate(self, input_dir: str, output_dir: str = "segmented_scoring_results", max_files: int = None, selected_models: List[str] = None):
        """
        æ‰¹é‡è¯„ä¼°
        """
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ®µè¯„åˆ†è¯„ä¼°")
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {[m['name'] for m in self.models[:3]]}")
        print(f"ğŸ“Š æ¯æ®µå¤§å°: 5é¢˜")
        print(f"âš¡ åˆ†æ®µé—´éš”: 3ç§’")
        print()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        file_pattern = os.path.join(input_dir, "*.json")
        files = glob.glob(file_pattern)

        if max_files:
            files = files[:max_files]

        print(f"ğŸ“Š æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")

        if not files:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
            return

        # æ‰¹é‡å¤„ç†
        batch_results = []
        overall_consistency_scores = []

        for i, file_path in enumerate(files, 1):
            print(f"ğŸ“ˆ [{i}/{len(files)}] å¤„ç†: {Path(file_path).name}")

            result = self.evaluate_file_with_multiple_models(file_path, output_dir)
            batch_results.append(result)

            if result['success']:
                overall_consistency_scores.append(result['consistency_score'])
                print(f"   âœ… ä¸€è‡´æ€§: {result['consistency_score']:.1f}%")
            else:
                print(f"   âŒ å¤±è´¥: {result.get('error', 'Unknown error')}")

        # å®Œæˆç»Ÿè®¡
        print()
        print("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ")
        print("=" * 60)

        successful_files = [r for r in batch_results if r.get('success', False)]
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {len(files)}")
        print(f"âœ… å¤„ç†æˆåŠŸ: {len(successful_files)}")
        print(f"âŒ å¤„ç†å¤±è´¥: {len(files) - len(successful_files)}")

        if overall_consistency_scores:
            avg_consistency = statistics.mean(overall_consistency_scores)
            print(f"ğŸ“ˆ å¹³å‡ä¸€è‡´æ€§: {avg_consistency:.1f}%")
            print(f"ğŸ“Š ä¸€è‡´æ€§èŒƒå›´: {min(overall_consistency_scores):.1f}% - {max(overall_consistency_scores):.1f}%")

        # ä¿å­˜æ‰¹é‡å¤„ç†æŠ¥å‘Š
        batch_report = {
            "batch_info": {
                "models": [{"name": m["name"], "description": m["description"]} for m in self.models[:3]],
                "segment_size": 5,
                "processing_date": datetime.now().isoformat(),
                "input_directory": input_dir,
                "output_directory": output_dir
            },
            "input_files": [f for f in files],
            "results": batch_results,
            "statistics": {
                "total_files": len(files),
                "successful_files": len(successful_files),
                "failed_files": len(files) - len(successful_files),
                "average_consistency": statistics.mean(overall_consistency_scores) if overall_consistency_scores else 0,
                "consistency_scores": overall_consistency_scores
            }
        }

        report_path = os.path.join(output_dir, "segmented_scoring_batch_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(batch_report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ æ‰¹é‡æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        return batch_report


class ScoringConsistencyAnalyzer:
    """
    è¯„åˆ†ä¸€è‡´æ€§åˆ†æå™¨
    """
    def __init__(self):
        pass

    def calculate_consistency(self, model_results: List[Dict]) -> Dict:
        """
        è®¡ç®—å¤šä¸ªæ¨¡å‹é—´çš„ä¸€è‡´æ€§
        """
        if len(model_results) < 2:
            return {"error": "éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹çš„ç»“æœ"}

        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        consistency_analysis = {}

        for trait in traits:
            scores = []
            model_names = []

            for result in model_results:
                if 'scores' in result and trait in result['scores']:
                    scores.append(result['scores'][trait])
                    model_names.append(result['model'])

            if len(scores) >= 2:
                avg_score = statistics.mean(scores)
                max_score = max(scores)
                min_score = min(scores)
                score_range = max_score - min_score

                # ä¸€è‡´æ€§è¯„ä¼°
                if score_range == 0:
                    consistency_level = "å®Œå…¨ä¸€è‡´"
                    consistency_score = 100
                elif score_range <= 1:
                    consistency_level = "é«˜åº¦ä¸€è‡´"
                    consistency_score = 80
                elif score_range <= 2:
                    consistency_level = "ä¸­ç­‰ä¸€è‡´"
                    consistency_score = 60
                else:
                    consistency_level = "å·®å¼‚è¾ƒå¤§"
                    consistency_score = 40

                consistency_analysis[trait] = {
                    "scores": dict(zip(model_names, scores)),
                    "average": avg_score,
                    "range": score_range,
                    "consistency_level": consistency_level,
                    "consistency_score": consistency_score
                }

        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§
        overall_scores = [analysis.get('consistency_score', 0) for analysis in consistency_analysis.values()]
        overall_consistency = statistics.mean(overall_scores) if overall_scores else 0

        return {
            "trait_analysis": consistency_analysis,
            "overall_consistency": overall_consistency,
            "successful_models": len(model_results),
            "total_models": len(model_results)
        }


class DisputeResolutionManager:
    """
    åˆ†æ­§å¤„ç†ç®¡ç†å™¨
    """
    def __init__(self):
        pass

    def identify_disputes(self, all_scores: List[Dict], threshold: int = 1) -> List[Dict]:
        """
        è¯†åˆ«è¯„åˆ†ä¸­çš„åˆ†æ­§
        """
        # æŒ‰é—®é¢˜IDå’Œç‰¹è´¨åˆ†ç»„è¯„åˆ†
        scores_by_question_trait = {}
        for score_record in all_scores:
            qid = score_record.get('question_id')
            # æ£€æŸ¥æ˜¯å¦åŒ…å«Big5å„ä¸ªç»´åº¦çš„è¯„åˆ†
            for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                if trait in score_record:
                    key = f"{qid}_{trait}"
                    if key not in scores_by_question_trait:
                        scores_by_question_trait[key] = []
                    scores_by_question_trait[key].append({
                        'question_id': qid,
                        'trait': trait,
                        'score': score_record[trait],
                        'model': score_record.get('model', 'unknown')
                    })

        disputes = []
        for key, scores_list in scores_by_question_trait.items():
            # æå–æ‰€æœ‰è¯„åˆ†
            scores = [record['score'] for record in scores_list]
            if len(scores) < 2:
                continue

            max_score = max(scores)
            min_score = min(scores)
            score_range = max_score - min_score

            if score_range > threshold:
                qid_str, trait = key.split('_', 1)
                qid = int(qid_str)
                disputes.append({
                    "question_id": qid,
                    "trait": trait,
                    "scores": scores,
                    "models": [record['model'] for record in scores_list],
                    "max_diff": score_range,
                    "average_score": statistics.mean(scores)
                })

        return disputes

    def apply_majority_decision(self, scores: List[int]) -> int:
        """
        åº”ç”¨å¤šæ•°å†³ç­–åŸåˆ™ï¼ˆå»é™¤æœ€é«˜åˆ†å’Œæœ€ä½åˆ†åå–ä¸­ä½æ•°ï¼‰
        """
        if len(scores) <= 2:
            # å¦‚æœåªæœ‰1æˆ–2ä¸ªè¯„åˆ†ï¼Œç›´æ¥å–å¹³å‡å€¼å¹¶å››èˆäº”å…¥
            return round(statistics.mean(scores)) if scores else 0

        # å»é™¤ä¸€ä¸ªæœ€é«˜åˆ†å’Œä¸€ä¸ªæœ€ä½åˆ†
        scores_sorted = sorted(scores)
        if len(scores_sorted) > 2:
            trimmed_scores = scores_sorted[1:-1]  # å»é™¤é¦–å°¾
        else:
            trimmed_scores = scores_sorted

        # è¿”å›å‰©ä½™è¯„åˆ†çš„ä¸­ä½æ•°
        if trimmed_scores:
            # å¯¹äºå¤šæ•°å†³ç­–ï¼Œä½¿ç”¨ä¸­ä½æ•°ä½œä¸ºæœ€ç»ˆå¾—åˆ†
            return int(statistics.median(trimmed_scores))
        else:
            # å¦‚æœä¿®å‰ªåæ²¡æœ‰è¯„åˆ†ï¼Œè¿”å›åŸè¯„åˆ†çš„å¹³å‡å€¼
            return round(statistics.mean(scores)) if scores else 0


def main():
    """
    ä¸»å‡½æ•° - æ¼”ç¤ºç”¨æ³•
    """
    # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹
    evaluator = SegmentedScoringEvaluator()

    # è¾“å…¥è¾“å‡ºç›®å½•
    input_dir = "results/readonly-original"
    output_dir = "segmented_scoring_results"

    # æ‰¹é‡è¯„ä¼° (å¤„ç†éƒ¨åˆ†æ–‡ä»¶è¿›è¡Œæµ‹è¯•)
    evaluator.batch_evaluate(input_dir, output_dir, max_files=5)


if __name__ == "__main__":
    main()