#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ollama TDDé©±åŠ¨çš„5é¢˜åˆ†æ®µä¸‰æ¨¡å‹å¹¶è¡Œåˆ†æå™¨
ä¿è¯5é¢˜åˆ†æ®µï¼ˆæ¯æ®µ5é¢˜ï¼Œæ¯ä¸ªæµ‹è¯„æŠ¥å‘Šåˆ†10æ®µï¼‰ï¼Œä¸‰ä¸ªOllamaæ¨¡å‹ç‹¬ç«‹å¹¶è¡Œè¯„ä¼°
åŸºäºæµ‹è¯•é©±åŠ¨å¼€å‘æ–¹æ³•ï¼Œç¡®ä¿é«˜ç½®ä¿¡åº¦å’Œä¸€è‡´æ€§
"""

import sys
import os
import json
import subprocess
import re
import time
import threading
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import statistics
import concurrent.futures

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['PYTHONIOENCODING'] = 'utf-8'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class OllamaTDD5SegmentAnalyzer:
    def __init__(self):
        # ä¸‰ä¸ªOllamaäº‘æ¨¡å‹é…ç½®
        self.models = [
            {"name": "deepseek-v3.1:671b-cloud", "description": "DeepSeek 671Bäº‘æ¨¡å‹"},
            {"name": "gpt-oss:120b-cloud", "description": "GPT OSS 120Bäº‘æ¨¡å‹"},
            {"name": "qwen3-coder:480b-cloud", "description": "Qwen3 Coder 480Bäº‘æ¨¡å‹"}
        ]

        # æµ‹è¯•ç”¨ä¾‹å®šä¹‰
        self.test_cases = self._define_test_cases()

        # éªŒè¯ç»“æœç¼“å­˜
        self.validation_cache = {}

    def _define_test_cases(self) -> Dict:
        """å®šä¹‰TDDæµ‹è¯•ç”¨ä¾‹"""
        return {
            "simple_json": {
                "prompt": "è¯·è¿”å›JSONæ ¼å¼ï¼š{\"score\": 3}",
                "expected_structure": {"score": int},
                "description": "åŸºç¡€JSONå“åº”æµ‹è¯•"
            },
            "big5_simple": {
                "prompt": """åˆ†æå›ç­”å¹¶è¿”å›JSONï¼š
é—®é¢˜ï¼šæˆ‘å–œæ¬¢å°è¯•æ–°äº‹ç‰©
å›ç­”ï¼šæ˜¯çš„ï¼Œæˆ‘ç»å¸¸å°è¯•æ–°çš„é¤å…

è¿”å›æ ¼å¼ï¼š
{
  "success": true,
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5
  }
}""",
                "expected_structure": {
                    "success": bool,
                    "scores": {"openness_to_experience": int}
                },
                "description": "Big5ç®€å•è¯„åˆ†æµ‹è¯•"
            },
            "big5_complete": {
                "prompt": """ä½œä¸ºå¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œåˆ†æä»¥ä¸‹å›ç­”ï¼š

é—®é¢˜1ï¼šæˆ‘å–œæ¬¢å°è¯•æ–°äº‹ç‰©
å›ç­”ï¼šæ˜¯çš„ï¼Œæˆ‘ç»å¸¸å°è¯•æ–°çš„é¤å…å’Œæ´»åŠ¨

é—®é¢˜2ï¼šæˆ‘åšäº‹å¾ˆæœ‰æ¡ç†
å›ç­”ï¼šæˆ‘æ€»æ˜¯åˆ¶å®šè¯¦ç»†è®¡åˆ’å¹¶æŒ‰æ—¶å®Œæˆ

è¯·è¿”å›JSONæ ¼å¼ï¼š
{
  "success": true,
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“è¯æ®",
    "conscientiousness": "å…·ä½“è¯æ®"
  },
  "confidence": "high/medium/low"
}""",
                "expected_structure": {
                    "success": bool,
                    "scores": {
                        "openness_to_experience": int,
                        "conscientiousness": int
                    },
                    "evidence": {
                        "openness_to_experience": str,
                        "conscientiousness": str
                    },
                    "confidence": str
                },
                "description": "å®Œæ•´Big5åˆ†ææµ‹è¯•"
            }
        }

    def _execute_ollama_command(self, model_name: str, prompt: str, timeout: int = 180) -> Tuple[bool, str, float]:
        """
        æ‰§è¡ŒOllamaå‘½ä»¤çš„å¥å£®æ–¹æ³•
        è¿”å›ï¼š(æˆåŠŸæ ‡å¿—, å“åº”å†…å®¹, å¤„ç†æ—¶é—´)
        """
        try:
            # æ„å»ºå‘½ä»¤ - ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼
            cmd = ['ollama', 'run', model_name, prompt, '--format', 'json']

            start_time = time.time()

            # ä½¿ç”¨encoding='utf-8'å’Œerrors='ignore'æ¥å¤„ç†ç¼–ç é—®é¢˜
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                encoding='utf-8',
                errors='ignore'
            )

            end_time = time.time()
            processing_time = end_time - start_time

            if result.returncode == 0:
                # æ¸…ç†å“åº”ä¸­çš„ç»ˆç«¯æ§åˆ¶å­—ç¬¦
                cleaned_response = self._clean_terminal_output(result.stdout)
                return True, cleaned_response, processing_time
            else:
                return False, f"å‘½ä»¤å¤±è´¥: {result.stderr}", processing_time

        except subprocess.TimeoutExpired:
            return False, "è¯·æ±‚è¶…æ—¶", timeout
        except Exception as e:
            return False, f"æ‰§è¡Œé”™è¯¯: {str(e)}", 0

    def _clean_terminal_output(self, text: str) -> str:
        """æ¸…ç†ç»ˆç«¯è¾“å‡ºä¸­çš„æ§åˆ¶å­—ç¬¦"""
        if not text:
            return ""

        # ç§»é™¤ANSIè½¬ä¹‰åºåˆ—
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        cleaned = ansi_escape.sub('', text)

        # ç§»é™¤å…¶ä»–æ§åˆ¶å­—ç¬¦ï¼Œä½†ä¿ç•™JSONç›¸å…³å­—ç¬¦
        cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', cleaned)

        return cleaned.strip()

    def _parse_json_response(self, response_text: str) -> Dict:
        """
        å¤šç­–ç•¥JSONè§£æå™¨
        """
        if not response_text:
            return {"success": False, "error": "å“åº”ä¸ºç©º"}

        # æ¸…ç†å“åº”æ–‡æœ¬
        response_text = self._clean_terminal_output(response_text)

        # è§£æç­–ç•¥åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        strategies = [
            ("ç›´æ¥è§£æ", self._direct_json_parse),
            ("ä»£ç å—æå–", self._extract_json_from_codeblock),
            ("æ­£åˆ™æå–", self._extract_json_with_regex),
            ("æ™ºèƒ½ä¿®å¤", self._smart_json_fix),
            ("æ¨¡ç³ŠåŒ¹é…", self._fuzzy_score_extract)
        ]

        for strategy_name, strategy_func in strategies:
            try:
                result = strategy_func(response_text)
                if result and self._validate_json_structure(result):
                    return {"success": True, "method": strategy_name, "data": result}
            except Exception as e:
                continue

        return {
            "success": False,
            "error": "æ‰€æœ‰è§£æç­–ç•¥å¤±è´¥",
            "raw_response": response_text[:500] if response_text else "ç©ºå“åº”"
        }

    def _direct_json_parse(self, text: str) -> Optional[Dict]:
        """ç›´æ¥JSONè§£æ"""
        text = text.strip()
        if text.startswith('{') and text.endswith('}'):
            return json.loads(text)
        return None

    def _extract_json_from_codeblock(self, text: str) -> Optional[Dict]:
        """ä»ä»£ç å—æå–JSON"""
        patterns = [
            r'```json\s*\n?(\{.*?\})\s*```',
            r'```\s*\n?(\{.*?\})\s*```',
            r'`(\{.*?\})`'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                try:
                    return json.loads(match.strip())
                except:
                    continue
        return None

    def _extract_json_with_regex(self, text: str) -> Optional[Dict]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSON"""
        patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # æ ‡å‡†JSONå¯¹è±¡
            r'\{(?:[^{}"]|"[^"]*")*\}',          # åŒ…å«å­—ç¬¦ä¸²çš„JSON
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                try:
                    return json.loads(match)
                except:
                    continue
        return None

    def _smart_json_fix(self, text: str) -> Optional[Dict]:
        """æ™ºèƒ½ä¿®å¤JSONæ ¼å¼é—®é¢˜"""
        try:
            # ç§»é™¤BOM
            text = text.lstrip('\ufeff').strip()

            # æ‰¾åˆ°JSONå¯¹è±¡
            if '{' in text and '}' in text:
                start = text.find('{')
                brace_count = 0
                end = start
                for i, char in enumerate(text[start:], start):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            end = i + 1
                            break

                text = text[start:end]

            # ä¿®å¤å¸¸è§é—®é¢˜
            text = re.sub(r'(\w+):', r'"\1":', text)  # é”®åŠ å¼•å·
            text = re.sub(r':\s*([a-zA-Z_][a-zA-Z0-9_]*)', r': "\1"', text)  # å€¼åŠ å¼•å·
            text = re.sub(r'//.*?\n', '', text)  # ç§»é™¤æ³¨é‡Š
            text = re.sub(r'/\*.*?\*/', '', text, flags=re.DOTALL)  # ç§»é™¤å—æ³¨é‡Š
            text = re.sub(r',\s*}', '}', text)  # ç§»é™¤å°¾éƒ¨é€—å·
            text = re.sub(r',\s*]', ']', text)  # ç§»é™¤æ•°ç»„å°¾éƒ¨é€—å·

            return json.loads(text)
        except:
            return None

    def _fuzzy_score_extract(self, text: str) -> Optional[Dict]:
        """æ¨¡ç³Šæå–è¯„åˆ†ä¿¡æ¯"""
        scores = {}

        # Big5ç»´åº¦è¯„åˆ†æå–æ¨¡å¼
        patterns = [
            (r'openness_to_experience["\s]*:["\s]*([1-5])', 'openness_to_experience'),
            (r'conscientiousness["\s]*:["\s]*([1-5])', 'conscientiousness'),
            (r'extraversion["\s]*:["\s]*([1-5])', 'extraversion'),
            (r'agreeableness["\s]*:["\s]*([1-5])', 'agreeableness'),
            (r'neuroticism["\s]*:["\s]*([1-5])', 'neuroticism')
        ]

        for pattern, trait in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                scores[trait] = int(match.group(1))

        if scores:
            return {
                "success": True,
                "scores": scores,
                "extraction_method": "fuzzy",
                "confidence": "medium"
            }

        return None

    def _validate_json_structure(self, data: Dict) -> bool:
        """éªŒè¯JSONæ•°æ®ç»“æ„"""
        if not isinstance(data, dict):
            return False

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        if 'success' not in data:
            return False

        # å¦‚æœæœ‰scoreså­—æ®µï¼ŒéªŒè¯å…¶ç»“æ„
        if 'scores' in data:
            scores = data['scores']
            if not isinstance(scores, dict):
                return False

            # éªŒè¯è¯„åˆ†å€¼
            for trait, score in scores.items():
                if not isinstance(score, int) or score not in [1, 3, 5]:
                    return False

        return True

    def _validate_model_with_test_case(self, model_name: str, test_case_name: str) -> Dict:
        """ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹éªŒè¯æ¨¡å‹"""
        if test_case_name not in self.test_cases:
            return {"success": False, "error": f"æœªçŸ¥æµ‹è¯•ç”¨ä¾‹: {test_case_name}"}

        test_case = self.test_cases[test_case_name]
        cache_key = f"{model_name}_{test_case_name}"

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.validation_cache:
            return self.validation_cache[cache_key]

        print(f"  ğŸ§ª æµ‹è¯• {model_name} - {test_case['description']}")

        success, response, processing_time = self._execute_ollama_command(
            model_name,
            test_case["prompt"]
        )

        if not success:
            result = {
                "success": False,
                "error": response,
                "processing_time": processing_time
            }
        else:
            parse_result = self._parse_json_response(response)
            result = {
                "success": parse_result["success"],
                "data": parse_result.get("data"),
                "method": parse_result.get("method"),
                "processing_time": processing_time,
                "raw_response": response[:200] if response else ""
            }

        # ç¼“å­˜ç»“æœ
        self.validation_cache[cache_key] = result
        return result

    def run_model_validation(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„æ¨¡å‹éªŒè¯æµ‹è¯•"""
        print("ğŸ”¬ TDDæ¨¡å‹éªŒè¯æµ‹è¯•å¼€å§‹")
        print("=" * 50)

        validation_results = {}

        for model in self.models:
            model_name = model["name"]
            print(f"\nğŸ“‹ éªŒè¯æ¨¡å‹: {model_name}")

            model_results = {}

            # è¿è¡Œæ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
            for test_name in ["simple_json", "big5_simple", "big5_complete"]:
                test_result = self._validate_model_with_test_case(model_name, test_name)
                model_results[test_name] = test_result

                if test_result["success"]:
                    print(f"    âœ… {test_name}: é€šè¿‡ ({test_result.get('method', 'æœªçŸ¥æ–¹æ³•')})")
                else:
                    print(f"    âŒ {test_name}: å¤±è´¥ - {test_result.get('error', 'æœªçŸ¥é”™è¯¯')}")

            validation_results[model_name] = model_results

        # ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report = self._generate_validation_report(validation_results)

        # ä¿å­˜éªŒè¯ç»“æœ
        report_file = f"ollama_tdd_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report

    def _generate_validation_report(self, validation_results: Dict) -> Dict:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report = {
            "validation_time": datetime.now().isoformat(),
            "models_tested": list(validation_results.keys()),
            "summary": {},
            "detailed_results": validation_results,
            "recommendations": []
        }

        # è®¡ç®—æˆåŠŸç‡
        total_tests = 0
        passed_tests = 0

        for model_name, model_results in validation_results.items():
            model_passed = 0
            model_total = len(model_results)

            for test_name, test_result in model_results.items():
                total_tests += 1
                if test_result["success"]:
                    passed_tests += 1
                    model_passed += 1

            report["summary"][model_name] = {
                "tests_passed": model_passed,
                "tests_total": model_total,
                "success_rate": model_passed / model_total if model_total > 0 else 0
            }

        overall_success_rate = passed_tests / total_tests if total_tests > 0 else 0
        report["summary"]["overall"] = {
            "tests_passed": passed_tests,
            "tests_total": total_tests,
            "success_rate": overall_success_rate
        }

        # ç”Ÿæˆå»ºè®®
        if overall_success_rate >= 0.8:
            report["recommendations"].append("âœ… æ¨¡å‹éªŒè¯é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹5é¢˜åˆ†æ®µåˆ†æ")
        elif overall_success_rate >= 0.5:
            report["recommendations"].append("âš ï¸ éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œå»ºè®®ä¼˜åŒ–è§£æé€»è¾‘")
        else:
            report["recommendations"].append("âŒ éªŒè¯å¤±è´¥ï¼Œéœ€è¦é‡å¤§ä¿®å¤")

        return report

    def create_5segment_prompt(self, segment: List[Dict], segment_number: int, total_segments: int) -> str:
        """åˆ›å»º5é¢˜åˆ†æ®µåˆ†ææç¤º"""
        prompt = """ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**ä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°å›ç­”è€…å±•ç°çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**å…³é”®æé†’ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…ï¼Œä¸è¦å›ç­”é—®å·é—®é¢˜
- âŒ ä¸è¦æ··æ·†è§’è‰²ï¼Œä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä¸“æ³¨äºåˆ†æå›ç­”ä¸­çš„äººæ ¼ç‰¹å¾
- âœ… å¿½ç•¥è§’è‰²æ‰®æ¼”å†…å®¹ï¼Œä¸“æ³¨å®é™…è¡Œä¸ºå€¾å‘

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ç¨‹åº¦
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æº
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- **1åˆ†**ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- **3åˆ†**ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- **5åˆ†**ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼Œç¦æ­¢ä½¿ç”¨2ã€4ç­‰å…¶ä»–æ•°å€¼ï¼**

**ç¬¬""" + str(segment_number) + """æ®µé—®å·å†…å®¹ï¼ˆ""" + str(len(segment)) + """é¢˜/å…±""" + str(total_segments) + """æ®µï¼‰ï¼š**
"""

        for i, item in enumerate(segment, 1):
            prompt += """
**é—®é¢˜ """ + str(i) + """ï¼š**
""" + item['question'] + """

**å›ç­” """ + str(i) + """ï¼š**
""" + item['answer'] + """

---
"""

        prompt += """
**è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š**
```json
{
  "success": true,
  "segment_number": """ + str(segment_number) + """,
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“è¯æ®å¼•ç”¨",
    "conscientiousness": "å…·ä½“è¯æ®å¼•ç”¨",
    "extraversion": "å…·ä½“è¯æ®å¼•ç”¨",
    "agreeableness": "å…·ä½“è¯æ®å¼•ç”¨",
    "neuroticism": "å…·ä½“è¯æ®å¼•ç”¨"
  },
  "confidence": "high/medium/low"
}
```

**å†æ¬¡æé†’ï¼šæ¯ä¸ªè¯„åˆ†å¿…é¡»æ˜¯1ã€3æˆ–5ï¼Œä¸èƒ½ä½¿ç”¨å…¶ä»–æ•°å€¼ï¼**
"""

        return prompt

    def analyze_segment_with_model(self, model_name: str, segment: List[Dict], segment_number: int, total_segments: int) -> Dict:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æå•ä¸ªåˆ†æ®µ"""
        prompt = self.create_5segment_prompt(segment, segment_number, total_segments)

        success, response, processing_time = self._execute_ollama_command(model_name, prompt)

        if not success:
            return {
                'success': False,
                'model': model_name,
                'segment_number': segment_number,
                'error': response,
                'processing_time': processing_time
            }

        # è§£æJSONå“åº”
        parse_result = self._parse_json_response(response)

        if not parse_result['success']:
            return {
                'success': False,
                'model': model_name,
                'segment_number': segment_number,
                'error': f"JSONè§£æå¤±è´¥: {parse_result['error']}",
                'raw_response': response[:500] if response else '',
                'processing_time': processing_time
            }

        # éªŒè¯è¯„åˆ†æ ‡å‡†
        data = parse_result['data']
        if 'scores' in data:
            invalid_scores = []
            for trait, score in data['scores'].items():
                if score not in [1, 3, 5]:
                    invalid_scores.append(f"{trait}:{score}")
                    # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                    if score < 2:
                        data['scores'][trait] = 1
                    elif score > 4:
                        data['scores'][trait] = 5
                    else:
                        data['scores'][trait] = 3

            if invalid_scores:
                print(f"      âš ï¸ {model_name} ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")

        result = {
            'success': True,
            'model': model_name,
            'segment_number': segment_number,
            'data': data,
            'parsing_method': parse_result['method'],
            'processing_time': processing_time
        }

        return result

    def _calculate_mbti_type(self, scores: Dict) -> str:
        """æ ¹æ®Big5è¯„åˆ†è®¡ç®—MBTIç±»å‹"""
        try:
            openness = scores.get('openness_to_experience', 3)
            conscientiousness = scores.get('conscientiousness', 3)
            extraversion = scores.get('extraversion', 3)
            agreeableness = scores.get('agreeableness', 3)
            neuroticism = scores.get('neuroticism', 3)

            I_E = 'I' if extraversion <= 3 else 'E'
            S_N = 'N' if openness >= 4 else 'S'
            T_F = 'F' if agreeableness >= 4 else 'T'
            J_P = 'J' if conscientiousness >= 4 else 'P'

            return f"{I_E}{S_N}{T_F}{J_P}"
        except Exception as e:
            return "UNKNOWN"

    def analyze_file_with_three_models(self, file_path: str, output_dir: str) -> Dict:
        """ä½¿ç”¨ä¸‰ä¸ªOllamaæ¨¡å‹ç‹¬ç«‹åˆ†æå•ä¸ªæ–‡ä»¶ï¼ˆ5é¢˜åˆ†æ®µï¼Œ10æ®µï¼‰"""
        print(f"ğŸ“ˆ å¼€å§‹5é¢˜åˆ†æ®µä¸‰æ¨¡å‹åˆ†æ: {Path(file_path).name}")

        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–é—®é¢˜
            questions = []
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for item in data['assessment_results']:
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', question_data.get('mapped_ipip_concept', ''))

                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']
                            elif 'conversation_log' in item and isinstance(item['conversation_log'], list):
                                for msg in item['conversation_log']:
                                    if isinstance(msg, dict) and msg.get('role') == 'assistant':
                                        answer_text = msg.get('content', '')
                                        break

                            if question_text and answer_text:
                                questions.append({
                                    'question': question_text,
                                    'answer': answer_text
                                })

            if len(questions) < 5:
                raise Exception(f"é—®é¢˜æ•°é‡ä¸è¶³ï¼š{len(questions)}")

            # åˆ†æ®µå¤„ç†ï¼ˆæ¯æ®µ5é¢˜ï¼Œç¡®ä¿10æ®µï¼‰
            segment_size = 5
            segments = []

            # å–å‰50é¢˜ï¼Œåˆ†æˆ10æ®µ
            questions_to_process = questions[:50]
            for i in range(0, len(questions_to_process), segment_size):
                segment = questions_to_process[i:i+segment_size]
                if len(segment) == segment_size:
                    segments.append(segment)

            total_segments = len(segments)
            print(f"  ğŸ“Š {len(questions)}é¢˜ -> {total_segments}æ®µ (æ¯æ®µ5é¢˜)")

            # ä¸‰æ¨¡å‹å¹¶å‘åˆ†æ
            model_analysis_results = {}
            total_start_time = time.time()

            # ä½¿ç”¨çº¿ç¨‹æ± å®ç°å¹¶å‘
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºåˆ†æä»»åŠ¡
                future_to_model = {}

                for model in self.models:
                    model_name = model["name"]
                    print(f"  ğŸŒ å¯åŠ¨æ¨¡å‹: {model_name}")

                    # ä¸ºè¯¥æ¨¡å‹çš„æ‰€æœ‰åˆ†æ®µåˆ›å»ºä»»åŠ¡
                    model_futures = []
                    for i, segment in enumerate(segments, 1):
                        future = executor.submit(
                            self.analyze_segment_with_model,
                            model_name,
                            segment,
                            i,
                            total_segments
                        )
                        model_futures.append(future)

                    future_to_model[model_name] = model_futures

                # æ”¶é›†ç»“æœ
                for model_name, futures in future_to_model.items():
                    print(f"  ğŸ” æ”¶é›† {model_name} ç»“æœ...")

                    segment_results = []
                    successful_segments = 0
                    total_model_time = 0

                    for future in futures:
                        try:
                            result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                            segment_results.append(result)

                            if result['success']:
                                successful_segments += 1
                                print(f"      âœ… æ®µ{result['segment_number']}: {list(result['data']['scores'].values())} ({result.get('processing_time', 0):.1f}s)")
                            else:
                                print(f"      âŒ æ®µ{result['segment_number']}: {result.get('error', 'Unknown error')}")

                            total_model_time += result.get('processing_time', 0)

                        except Exception as e:
                            print(f"      âš ï¸ æ®µå¤„ç†å¼‚å¸¸: {e}")

                    # è®¡ç®—è¯¥æ¨¡å‹çš„æœ€ç»ˆè¯„åˆ†
                    if segment_results:
                        final_scores = {}
                        for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                            all_scores = []
                            for result in segment_results:
                                if result['success'] and 'data' in result and 'scores' in result['data']:
                                    all_scores.append(result['data']['scores'][trait])

                            if all_scores:
                                final_scores[trait] = int(statistics.median(all_scores))

                        # ç”ŸæˆMBTIç±»å‹
                        mbti_type = self._calculate_mbti_type(final_scores)

                        model_analysis_results[model_name] = {
                            "segment_results": segment_results,
                            "final_scores": final_scores,
                            "mbti_type": mbti_type,
                            "successful_segments": successful_segments,
                            "total_segments": total_segments,
                            "success_rate": successful_segments / total_segments,
                            "total_processing_time": total_model_time,
                            "average_time_per_segment": total_model_time / total_segments if total_segments > 0 else 0
                        }

            total_time = time.time() - total_start_time

            # è®¡ç®—ä¸€è‡´æ€§åˆ†æ
            consistency_analysis = self._calculate_model_consistency(model_analysis_results)

            # ä¿å­˜ç»“æœ
            output_filename = f"{Path(file_path).stem}_ollama_tdd_5segment_analysis.json"
            output_path = os.path.join(output_dir, output_filename)

            analysis_result = {
                "file_info": {
                    "filename": Path(file_path).name,
                    "total_questions": len(questions),
                    "segments_analyzed": total_segments,
                    "questions_per_segment": segment_size,
                    "analysis_date": datetime.now().isoformat(),
                    "analysis_method": "5é¢˜åˆ†æ®µï¼Œä¸‰æ¨¡å‹å¹¶è¡Œ"
                },
                "models_used": [{"name": m["name"], "description": m["description"]} for m in self.models],
                "model_results": model_analysis_results,
                "consistency_analysis": consistency_analysis,
                "performance_metrics": {
                    "total_processing_time": total_time,
                    "models_count": len(model_analysis_results),
                    "average_time_per_model": sum(r.get('total_processing_time', 0) for r in model_analysis_results.values()) / len(model_analysis_results) if model_analysis_results else 0
                },
                "summary": {
                    "successful_models": len([r for r in model_analysis_results.values() if r.get('success_rate', 0) > 0]),
                    "average_success_rate": sum(r.get('success_rate', 0) for r in model_analysis_results.values()) / len(model_analysis_results) if model_analysis_results else 0,
                    "consensus_mbti": consistency_analysis.get('consensus_mbti', 'UNKNOWN'),
                    "high_confidence": consistency_analysis.get('high_confidence_consensus', False)
                }
            }

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)

            print(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_filename}")

            # æ˜¾ç¤ºç®€è¦ç»“æœ
            print(f"  ğŸ“‹ åˆ†æç»“æœæ‘˜è¦:")
            for model, results in model_analysis_results.items():
                print(f"    {model}: {results['final_scores']} -> {results['mbti_type']} ({results['successful_segments']}/{results['total_segments']}æ®µæˆåŠŸ, {results['success_rate']:.1%}æˆåŠŸç‡)")

            print(f"  ğŸ¯ ä¸€è‡´æ€§åˆ†æ: {consistency_analysis.get('consensus_mbti', 'UNKNOWN')} ({'é«˜ç½®ä¿¡åº¦' if consistency_analysis.get('high_confidence_consensus', False) else 'éœ€è¦è¿›ä¸€æ­¥éªŒè¯'})")

            return {
                'success': True,
                'file_path': file_path,
                'output_path': output_path,
                'model_results': model_analysis_results,
                'consistency_analysis': consistency_analysis,
                'total_time': total_time
            }

        except Exception as e:
            print(f"  âŒ æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e)
            }

    def _calculate_model_consistency(self, model_results: Dict) -> Dict:
        """è®¡ç®—ä¸‰ä¸ªæ¨¡å‹é—´çš„ä¸€è‡´æ€§"""
        if len(model_results) < 2:
            return {"error": "éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹ç»“æœè¿›è¡Œä¸€è‡´æ€§åˆ†æ"}

        # æ”¶é›†MBTIç±»å‹
        mbti_types = []
        scores_data = {}

        for model, results in model_results.items():
            if 'mbti_type' in results and results['mbti_type'] != 'UNKNOWN':
                mbti_types.append(results['mbti_type'])

            if 'final_scores' in results:
                for trait, score in results['final_scores'].items():
                    if trait not in scores_data:
                        scores_data[trait] = []
                    scores_data[trait].append(score)

        # è®¡ç®—MBTIä¸€è‡´æ€§
        mbti_consensus = "UNKNOWN"
        high_confidence_consensus = False

        if mbti_types:
            from collections import Counter
            mbti_counts = Counter(mbti_types)
            most_common_mbti, count = mbti_counts.most_common(1)[0]

            if count >= len(mbti_types):
                mbti_consensus = most_common_mbti
                high_confidence_consensus = True
            elif count >= 2:
                mbti_consensus = most_common_mbti
                high_confidence_consensus = False

        # è®¡ç®—è¯„åˆ†ä¸€è‡´æ€§
        trait_consistency = {}
        for trait, scores in scores_data.items():
            if len(scores) >= 2:
                std_dev = statistics.stdev(scores) if len(scores) > 1 else 0
                mean_score = statistics.mean(scores)

                if std_dev <= 0.8:
                    consistency_level = "é«˜"
                elif std_dev <= 1.5:
                    consistency_level = "ä¸­"
                else:
                    consistency_level = "ä½"

                trait_consistency[trait] = {
                    "mean_score": mean_score,
                    "std_deviation": std_dev,
                    "consistency_level": consistency_level,
                    "scores": scores
                }

        # è®¡ç®—æ•´ä½“ä¸€è‡´æ€§
        consistency_levels = [info["consistency_level"] for info in trait_consistency.values()]
        high_consistency_count = consistency_levels.count("é«˜")

        if high_consistency_count >= 4:
            overall_consistency = "é«˜"
        elif high_consistency_count >= 2:
            overall_consistency = "ä¸­"
        else:
            overall_consistency = "ä½"

        return {
            "consensus_mbti": mbti_consensus,
            "high_confidence_consensus": high_confidence_consensus,
            "mbti_distribution": dict(Counter(mbti_types)) if mbti_types else {},
            "trait_consistency": trait_consistency,
            "overall_consistency": overall_consistency,
            "models_analyzed": len(model_results),
            "analysis_timestamp": datetime.now().isoformat()
        }

def main():
    """ä¸»å‡½æ•° - TDDé©±åŠ¨éªŒè¯å’Œ5é¢˜åˆ†æ®µåˆ†æ"""
    print("ğŸš€ Ollama TDDé©±åŠ¨çš„5é¢˜åˆ†æ®µä¸‰æ¨¡å‹å¹¶è¡Œåˆ†æå™¨")
    print("=" * 60)

    analyzer = OllamaTDD5SegmentAnalyzer()

    # æ­¥éª¤1: è¿è¡ŒTDDéªŒè¯
    print("\nğŸ“‹ æ­¥éª¤1: TDDæ¨¡å‹éªŒè¯")
    validation_report = analyzer.run_model_validation()

    # æ£€æŸ¥Big5åˆ†æéªŒè¯ç»“æœ
    big5_success_rate = 0
    total_big5_tests = 0
    passed_big5_tests = 0

    for model_name, model_results in validation_report["detailed_results"].items():
        if "big5_complete" in model_results and model_results["big5_complete"]["success"]:
            passed_big5_tests += 1
        total_big5_tests += 1
        if "big5_simple" in model_results and model_results["big5_simple"]["success"]:
            passed_big5_tests += 1
        total_big5_tests += 1

    if total_big5_tests > 0:
        big5_success_rate = passed_big5_tests / total_big5_tests

    if big5_success_rate < 0.8:
        print(f"\nâš ï¸ Big5åˆ†æéªŒè¯æˆåŠŸç‡è¾ƒä½ ({big5_success_rate:.1%})")
        print("å»ºè®®å…ˆä¿®å¤Big5åˆ†æé—®é¢˜å†è¿›è¡Œ5é¢˜åˆ†æ®µåˆ†æ")
        return

    print(f"\nâœ… Big5åˆ†æéªŒè¯é€šè¿‡ ({big5_success_rate:.1%} æˆåŠŸç‡)")
    print(f"   æ•´ä½“éªŒè¯æˆåŠŸç‡: {validation_report['summary']['overall']['success_rate']:.1%}")

    # æ­¥éª¤2: 5é¢˜åˆ†æ®µåˆ†ææµ‹è¯•
    print(f"\nğŸ“‹ æ­¥éª¤2: 5é¢˜åˆ†æ®µåˆ†ææµ‹è¯•")

    # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
    results_dir = "results/results"
    test_files = list(Path(results_dir).glob("*.json"))[:1]  # æµ‹è¯•1ä¸ªæ–‡ä»¶

    if not test_files:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "ollama_tdd_5segment_results"
    os.makedirs(output_dir, exist_ok=True)

    # åˆ†ææµ‹è¯•æ–‡ä»¶
    test_file = test_files[0]
    result = analyzer.analyze_file_with_three_models(str(test_file), output_dir)

    if result['success']:
        print(f"\nğŸ‰ 5é¢˜åˆ†æ®µåˆ†ææµ‹è¯•æˆåŠŸ!")
        print(f"   æ–‡ä»¶: {Path(test_file).name}")
        print(f"   å¤„ç†æ—¶é—´: {result['total_time']:.1f}ç§’")
        print(f"   ä¸€è‡´æ€§: {result['consistency_analysis'].get('overall_consistency', 'æœªçŸ¥')}")

        if result['consistency_analysis'].get('high_confidence_consensus', False):
            print(f"   âœ… é«˜ç½®ä¿¡åº¦ä¸€è‡´: {result['consistency_analysis'].get('consensus_mbti', 'UNKNOWN')}")
        else:
            print(f"   âš ï¸ éœ€è¦è¿›ä¸€æ­¥éªŒè¯")
    else:
        print(f"\nâŒ 5é¢˜åˆ†æ®µåˆ†ææµ‹è¯•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

if __name__ == "__main__":
    main()