#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æè„šæœ¬ - æ”¯æŒæ–­ç‚¹ç»­è·‘
å¤„ç†å¤šä¸ªæµ‹è¯„æŠ¥å‘Šæ–‡ä»¶ï¼Œæ”¯æŒä¸­æ–­åç»§ç»­è¿è¡Œ
"""

import sys
import os
import json
import argparse
from pathlib import Path
from datetime import datetime
import time
import pickle
from typing import List, Dict, Any

# æ·»åŠ åŒ…ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from single_report_pipeline import TransparentPipeline


class BatchAnalyzer:
    """æ‰¹é‡åˆ†æå™¨ - æ”¯æŒæ–­ç‚¹ç»­è·‘"""
    
    def __init__(self, input_dir: str, output_dir: str, checkpoint_interval: int = 5):
        """
        åˆå§‹åŒ–æ‰¹é‡åˆ†æå™¨
        
        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            checkpoint_interval: æ£€æŸ¥ç‚¹é—´éš”ï¼ˆå¤„ç†å¤šå°‘æ–‡ä»¶åä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.checkpoint_interval = checkpoint_interval
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        self.checkpoint_file = self.output_dir / "batch_analysis_checkpoint.pkl"
        self.results_file = self.output_dir / "batch_analysis_results.json"
        
        # åˆ›å»ºæµæ°´çº¿å®ä¾‹
        self.pipeline = TransparentPipeline()
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.processed_files = set()
        self.results = []
        self.current_file_index = 0
        self.total_files = 0
        self.start_time = None
        
        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.load_checkpoint()
    
    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'rb') as f:
                    checkpoint_data = pickle.load(f)
                
                self.processed_files = set(checkpoint_data.get('processed_files', []))
                self.results = checkpoint_data.get('results', [])
                self.current_file_index = checkpoint_data.get('current_file_index', 0)
                self.total_files = checkpoint_data.get('total_files', 0)
                
                # è§£ææ—¶é—´æˆ³
                start_time_str = checkpoint_data.get('start_time')
                if start_time_str:
                    try:
                        self.start_time = datetime.fromisoformat(start_time_str)
                    except:
                        self.start_time = datetime.now()
                else:
                    self.start_time = datetime.now()
                
                print(f"âœ… å·²åŠ è½½æ£€æŸ¥ç‚¹: å¤„ç†äº† {len(self.processed_files)} ä¸ªæ–‡ä»¶")
                
            except Exception as e:
                print(f"âš ï¸  åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                self.processed_files = set()
                self.results = []
                self.current_file_index = 0
                self.total_files = 0
                self.start_time = datetime.now()
        else:
            print("â„¹ï¸  æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå¼€å§‹å…¨æ–°åˆ†æ")
            self.processed_files = set()
            self.results = []
            self.current_file_index = 0
            self.total_files = 0
            self.start_time = datetime.now()
    
    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_data = {
            'processed_files': list(self.processed_files),
            'results': self.results,
            'current_file_index': self.current_file_index,
            'total_files': self.total_files,
            'start_time': self.start_time.isoformat() if self.start_time else datetime.now().isoformat()
        }
        
        try:
            with open(self.checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint_data, f)
            print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        results_data = {
            'analysis_info': {
                'start_time': self.start_time.isoformat() if self.start_time else datetime.now().isoformat(),
                'end_time': datetime.now().isoformat(),
                'total_files': self.total_files,
                'processed_files': len(self.processed_files),
                'remaining_files': self.total_files - len(self.processed_files),
                'duration_seconds': (datetime.now() - self.start_time).total_seconds() if self.start_time else 0
            },
            'results': self.results
        }
        
        try:
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def find_json_files(self, pattern: str = "*.json") -> List[Path]:
        """
        æŸ¥æ‰¾JSONæ–‡ä»¶
        
        Args:
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            
        Returns:
            JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        json_files = list(self.input_dir.glob(pattern))
        json_files.sort()  # æŒ‰æ–‡ä»¶åæ’åºç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´
        return json_files
    
    def process_single_report(self, file_path: Path) -> Dict:
        """
        å¤„ç†å•ä¸ªæµ‹è¯„æŠ¥å‘Š
        
        Args:
            file_path: æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœ
        """
        print(f"ğŸ” å¤„ç†: {file_path.name}")
        
        try:
            # å¤„ç†æµ‹è¯„æŠ¥å‘Š
            result = self.pipeline.process_single_report(str(file_path))
            
            if result and result.get('success', False):
                print(f"  âœ… å®Œæˆ")
                print(f"    å¤§äº”äººæ ¼: {result.get('big5_scores', {})}")
                print(f"    MBTIç±»å‹: {result.get('mbti_type', 'Unknown')}")
                return result
            else:
                print(f"  âŒ å¤±è´¥")
                error_msg = result.get('error', 'Unknown error') if result else 'No result'
                return {
                    'success': False,
                    'file_path': str(file_path),
                    'error': error_msg
                }
                
        except Exception as e:
            print(f"  âŒ å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'file_path': str(file_path),
                'error': str(e)
            }
    
    def run_batch_analysis(self, pattern: str = "*.json", limit: int = None, resume: bool = True):
        """
        è¿è¡Œæ‰¹é‡åˆ†æ
        
        Args:
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            limit: é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡
            resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        """
        print("ğŸš€ æ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æå™¨ - æ–­ç‚¹ç»­è·‘ç‰ˆ")
        print("="*80)
        print(f"è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"æ£€æŸ¥ç‚¹é—´éš”: æ¯ {self.checkpoint_interval} ä¸ªæ–‡ä»¶")
        print()
        
        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if resume:
            self.load_checkpoint()
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        print("ğŸ“‚ æŸ¥æ‰¾æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶...")
        json_files = self.find_json_files(pattern)
        
        if not json_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
            return False
        
        self.total_files = len(json_files)
        if limit:
            json_files = json_files[:limit]
            self.total_files = len(json_files)
        
        print(f"  æ‰¾åˆ° {len(json_files)} ä¸ªæµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
        print(f"  å·²å¤„ç†: {len(self.processed_files)} ä¸ª")
        print(f"  å‰©ä½™: {len(json_files) - len(self.processed_files)} ä¸ª")
        print()
        
        # ç¡®å®šèµ·å§‹ä½ç½®
        start_index = self.current_file_index if resume else 0
        print(f"â–¶ï¸  ä»ç¬¬ {start_index + 1} ä¸ªæ–‡ä»¶å¼€å§‹å¤„ç†")
        print()
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        processed_count = 0
        success_count = 0
        failed_count = 0
        
        for i, file_path in enumerate(json_files[start_index:], start_index):
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
            if str(file_path) in self.processed_files:
                print(f"â­ï¸  è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}")
                continue
            
            print(f"ğŸ“ˆ [{i+1}/{len(json_files)}] å¤„ç†æ–‡ä»¶: {file_path.name}")
            
            # å¤„ç†æ–‡ä»¶
            result = self.process_single_report(file_path)
            
            # æ›´æ–°çŠ¶æ€
            self.processed_files.add(str(file_path))
            self.results.append(result)
            self.current_file_index = i + 1
            
            if result.get('success', False):
                success_count += 1
            else:
                failed_count += 1
            
            processed_count += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if processed_count % 10 == 0:
                print(f"  ğŸ“Š è¿›åº¦: {processed_count} ä¸ªæ–‡ä»¶å·²å¤„ç† "
                      f"(æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count})")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if processed_count % self.checkpoint_interval == 0:
                print(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...")
                self.save_checkpoint()
                self.save_results()
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIè¿‡è½½
            time.sleep(1)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        print(f"\nğŸ æ‰¹é‡åˆ†æå®Œæˆ!")
        print("="*80)
        print(f"æ€»æ–‡ä»¶æ•°: {len(json_files)}")
        print(f"å·²å¤„ç†æ•°: {processed_count}")
        print(f"æˆåŠŸå¤„ç†: {success_count}")
        print(f"å¤„ç†å¤±è´¥: {failed_count}")
        print(f"æˆåŠŸç‡: {success_count/processed_count*100:.1f}%" if processed_count > 0 else "N/A")
        
        self.save_checkpoint()
        self.save_results()
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")
        print(f"ğŸ” å¦‚éœ€ç»§ç»­å¤„ç†å‰©ä½™æ–‡ä»¶ï¼Œè¯·é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æå™¨ - æ”¯æŒæ–­ç‚¹ç»­è·‘')
    parser.add_argument('--input-dir', default='../results/readonly-original',
                       help='è¾“å…¥ç›®å½• (é»˜è®¤: ../results/readonly-original)')
    parser.add_argument('--output-dir', default='../results/batch-analysis-results',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: ../results/batch-analysis-results)')
    parser.add_argument('--pattern', default='*.json',
                       help='æ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: *.json)')
    parser.add_argument('--limit', type=int,
                       help='é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡')
    parser.add_argument('--checkpoint-interval', type=int, default=5,
                       help='æ£€æŸ¥ç‚¹é—´éš” (é»˜è®¤: æ¯5ä¸ªæ–‡ä»¶)')
    parser.add_argument('--no-resume', action='store_true',
                       help='ä¸ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œé‡æ–°å¼€å§‹')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰¹é‡åˆ†æå™¨
    analyzer = BatchAnalyzer(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        checkpoint_interval=args.checkpoint_interval
    )
    
    # è¿è¡Œæ‰¹é‡åˆ†æ
    success = analyzer.run_batch_analysis(
        pattern=args.pattern,
        limit=args.limit,
        resume=not args.no_resume
    )
    
    if success:
        print(f"\n{'='*80}")
        print("ğŸ‰ æ‰¹é‡åˆ†ææˆåŠŸå®Œæˆ!")
        print(f"{'='*80}")
        return 0
    else:
        print(f"\n{'='*80}")
        print("âŒ æ‰¹é‡åˆ†æå¤±è´¥!")
        print(f"{'='*80}")
        return 1


if __name__ == "__main__":
    sys.exit(main())