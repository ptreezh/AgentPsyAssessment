#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº‘æ¨¡å‹5é¢˜åˆ†æ®µè¯„ä¼°å™¨ - OpenRouterç‰ˆæœ¬
ä½¿ç”¨æŒ‡å®šçš„äº‘æ¨¡å‹è¿›è¡Œ5é¢˜åˆ†æ®µè¯„ä¼°ï¼ŒåŠ¨æ€å¢åŠ æ¨¡å‹ä»¥æé«˜ä¸€è‡´æ€§
è¯„åˆ†ç­–ç•¥ï¼šå‰”é™¤æœ€é«˜åˆ†å’Œæœ€ä½åˆ†åè®¡ç®—å‡åˆ†
"""

import sys
import os
import json
import time
import requests
import re
import statistics
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from collections import Counter
import concurrent.futures
from dotenv import load_dotenv

# å¯¼å…¥å¼¹æ€§JSONåºåˆ—åŒ–å™¨
from resilient_json_serializer import safe_json_dumps, safe_json_loads

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['PYTHONIOENCODING'] = 'utf-8'

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class CloudModelSegmentEvaluator:
    def __init__(self):
        """åˆå§‹åŒ–äº‘æ¨¡å‹åˆ†æ®µè¯„ä¼°å™¨"""
        # è¯»å–OpenRouteré…ç½®
        self.read_openrouter_config()

        # ä¸»è¦ä¸‰ä¸ªæ¨¡å‹
        self.primary_models = [
            {
                "name": "alibaba/tongyi-deepresearch-30b-a3b",
                "description": "Tongyi DeepResearch 30B A3B"
            },
            {
                "name": "deepseek/deepseek-chat-v3.1",
                "description": "DeepSeek Chat V3.1"
            },
            {
                "name": "openai/gpt-oss-20b",
                "description": "OpenAI GPT-OSS 20B"
            }
        ]

        # å¤‡ç”¨æ¨¡å‹ï¼ˆåˆ†æ­§å¤§æ—¶ä½¿ç”¨ï¼‰
        self.backup_models = [
            {
                "name": "moonshotai/kimi-k2",
                "description": "Moonshot Kimi K2"
            },
            {
                "name": "google/gemma-3-27b-it",
                "description": "Google Gemma 3 27B"
            }
        ]

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_segments': 0,
            'successful_segments': 0,
            'high_confidence_files': 0,
            'medium_confidence_files': 0,
            'low_confidence_files': 0,
            'processing_start': None,
            'processing_end': None
        }

    def read_openrouter_config(self):
        """ä».envæ–‡ä»¶è¯»å–OpenRouteré…ç½®"""
        try:
            self.api_key = os.getenv('OPENROUTER_API_KEY')
            self.base_url = os.getenv('OPENROUTER_BASE_URL')

            if self.api_key and self.base_url:
                print(f"âœ… å·²ä».envè¯»å–OpenRouteré…ç½®: {self.base_url}")
            else:
                print(f"âŒ .envæ–‡ä»¶ä¸­ç¼ºå°‘OpenRouteré…ç½®")
                self.api_key = None
                self.base_url = None

        except Exception as e:
            print(f"âŒ è¯»å–.envé…ç½®å¤±è´¥: {e}")
            self.api_key = None
            self.base_url = None

    def create_api_headers(self) -> Dict:
        """åˆ›å»ºAPIè¯·æ±‚å¤´"""
        return {
            "HTTP-Referer": "https://localhost",
            "X-Title": "Portable PsyAgent Evaluator",
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    def call_cloud_model(self, model_name: str, prompt: str, timeout: int = 120) -> Tuple[bool, str, float]:
        """è°ƒç”¨äº‘æ¨¡å‹API"""
        if not self.api_key or not self.base_url:
            return False, "APIé…ç½®ç¼ºå¤±", 0

        try:
            url = f"{self.base_url}/chat/completions"

            payload = {
                "model": model_name,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 0.3,
                "max_tokens": 2000,
                "response_format": {"type": "json_object"}
            }

            headers = self.create_api_headers()
            start_time = time.time()

            response = requests.post(
                url,
                json=payload,
                headers=headers,
                timeout=timeout
            )

            end_time = time.time()
            processing_time = end_time - start_time

            if response.status_code == 200:
                data = response.json()
                if 'choices' in data and len(data['choices']) > 0:
                    content = data['choices'][0]['message']['content']
                    return True, content, processing_time
                else:
                    return False, "å“åº”æ ¼å¼é”™è¯¯", processing_time
            else:
                error_msg = f"APIé”™è¯¯ ({response.status_code}): {response.text}"
                return False, error_msg, processing_time

        except requests.exceptions.Timeout:
            return False, "è¯·æ±‚è¶…æ—¶", timeout
        except Exception as e:
            return False, f"è¯·æ±‚å¼‚å¸¸: {str(e)}", 0

    def parse_json_response(self, response_text: str) -> Dict:
        """å¤šç­–ç•¥JSONè§£æå™¨"""
        if not response_text:
            return {"success": False, "error": "å“åº”ä¸ºç©º"}

        # è§£æç­–ç•¥
        strategies = [
            ("ç›´æ¥è§£æ", self.direct_json_parse),
            ("ä»£ç å—æå–", self.extract_json_from_codeblock),
            ("æ­£åˆ™æå–", self.extract_json_with_regex),
            ("æ™ºèƒ½ä¿®å¤", self.smart_json_fix),
            ("æ¨¡ç³ŠåŒ¹é…", self.fuzzy_score_extract)
        ]

        for strategy_name, strategy_func in strategies:
            try:
                result = strategy_func(response_text)
                if result and self.validate_json_structure(result):
                    return {"success": True, "method": strategy_name, "data": result}
            except Exception:
                continue

        return {
            "success": False,
            "error": "æ‰€æœ‰è§£æç­–ç•¥å¤±è´¥",
            "raw_response": response_text[:500] if response_text else "ç©ºå“åº”"
        }

    def direct_json_parse(self, text: str) -> Optional[Dict]:
        """ç›´æ¥JSONè§£æ"""
        text = text.strip()
        if text.startswith('{') and text.endswith('}'):
            return safe_json_loads(text)
        return None

    def extract_json_from_codeblock(self, text: str) -> Optional[Dict]:
        """ä»ä»£ç å—æå–JSON"""
        patterns = [
            r'```json\s*\n?(\{.*?\})\s*```',
            r'```\s*\n?(\{.*?\})\s*```',
            r'`(\{.*?\})`'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                try:
                    return safe_json_loads(match.strip())
                except:
                    continue
        return None

    def extract_json_with_regex(self, text: str) -> Optional[Dict]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSON"""
        patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',
            r'\{(?:[^{}"]|"[^"]*")*\}',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                try:
                    return safe_json_loads(match)
                except:
                    continue
        return None

    def smart_json_fix(self, text: str) -> Optional[Dict]:
        """æ™ºèƒ½ä¿®å¤JSONæ ¼å¼é—®é¢˜"""
        try:
            text = text.lstrip('\ufeff').strip()

            if '{' in text and '}' in text:
                start = text.find('{')
                brace_count = 0
                end = start
                for i, char in enumerate(text[start:], start):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            end = i + 1
                            break

                text = text[start:end]

            # ä¿®å¤å¸¸è§é—®é¢˜
            text = re.sub(r'(\w+):', r'"\1":', text)
            text = re.sub(r':\s*([a-zA-Z_][a-zA-Z0-9_]*)', r': "\1"', text)
            text = re.sub(r'//.*?\n', '', text)
            text = re.sub(r'/\*.*?\*/', '', text, flags=re.DOTALL)
            text = re.sub(r',\s*}', '}', text)
            text = re.sub(r',\s*]', ']', text)

            return safe_json_loads(text)
        except:
            return None

    def fuzzy_score_extract(self, text: str) -> Optional[Dict]:
        """æ¨¡ç³Šæå–è¯„åˆ†ä¿¡æ¯"""
        scores = {}

        patterns = [
            (r'openness_to_experience["\s]*:["\s]*([1-5])', 'openness_to_experience'),
            (r'conscientiousness["\s]*:["\s]*([1-5])', 'conscientiousness'),
            (r'extraversion["\s]*:["\s]*([1-5])', 'extraversion'),
            (r'agreeableness["\s]*:["\s]*([1-5])', 'agreeableness'),
            (r'neuroticism["\s]*:["\s]*([1-5])', 'neuroticism')
        ]

        for pattern, trait in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                scores[trait] = int(match.group(1))

        if scores:
            return {
                "success": True,
                "scores": scores,
                "extraction_method": "fuzzy",
                "confidence": "medium"
            }

        return None

    def validate_json_structure(self, data: Dict) -> bool:
        """éªŒè¯JSONæ•°æ®ç»“æ„"""
        if not isinstance(data, dict):
            return False

        if 'success' not in data:
            return False

        if 'scores' in data:
            scores = data['scores']
            if not isinstance(scores, dict):
                return False

            for trait, score in scores.items():
                if not isinstance(score, int) or score not in [1, 2, 3, 4, 5]:
                    return False

        return True

    def create_5segment_prompt(self, segment: List[Dict], segment_number: int, total_segments: int) -> str:
        """åˆ›å»º5é¢˜åˆ†æ®µåˆ†ææç¤º"""
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**ä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°å›ç­”è€…å±•ç°çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**å…³é”®æé†’ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…ï¼Œä¸è¦å›ç­”é—®å·é—®é¢˜
- âŒ ä¸è¦æ··æ·†è§’è‰²ï¼Œä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä¸“æ³¨äºåˆ†æå›ç­”ä¸­çš„äººæ ¼ç‰¹å¾
- âœ… å¿½ç•¥è§’è‰²æ‰®æ¼”å†…å®¹ï¼Œä¸“æ³¨å®é™…è¡Œä¸ºå€¾å‘

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ç¨‹åº¦
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æº
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- **1åˆ†**ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- **2åˆ†**ï¼šä½è¡¨ç° - å€¾å‘ç¼ºä¹è¯¥ç‰¹è´¨
- **3åˆ†**ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- **4åˆ†**ï¼šé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨
- **5åˆ†**ï¼šæé«˜è¡¨ç° - å¼ºçƒˆå…·å¤‡è¯¥ç‰¹è´¨

**ç¬¬{segment_number}æ®µé—®å·å†…å®¹ï¼ˆ{len(segment)}é¢˜/å…±{total_segments}æ®µï¼‰ï¼š**
"""

        for i, item in enumerate(segment, 1):
            prompt += f"""
**é—®é¢˜ {i}ï¼š**
{item['question']}

**å›ç­” {i}ï¼š**
{item['answer']}

---
"""

        prompt += """
**è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š**
```json
{
  "success": true,
  "segment_number": """ + str(segment_number) + """,
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {
    "openness_to_experience": 1æˆ–2æˆ–3æˆ–4æˆ–5,
    "conscientiousness": 1æˆ–2æˆ–3æˆ–4æˆ–5,
    "extraversion": 1æˆ–2æˆ–3æˆ–4æˆ–5,
    "agreeableness": 1æˆ–2æˆ–3æˆ–4æˆ–5,
    "neuroticism": 1æˆ–2æˆ–3æˆ–4æˆ–5
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“è¯æ®å¼•ç”¨",
    "conscientiousness": "å…·ä½“è¯æ®å¼•ç”¨",
    "extraversion": "å…·ä½“è¯æ®å¼•ç”¨",
    "agreeableness": "å…·ä½“è¯æ®å¼•ç”¨",
    "neuroticism": "å…·ä½“è¯æ®å¼•ç”¨"
  },
  "confidence": "high/medium/low"
}
```

**å†æ¬¡æé†’ï¼šæ¯ä¸ªè¯„åˆ†å¿…é¡»æ˜¯1-5çš„æ•´æ•°ï¼**
"""

        return prompt

    def analyze_segment_with_model(self, model_name: str, segment: List[Dict], segment_number: int, total_segments: int) -> Dict:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æå•ä¸ªåˆ†æ®µ"""
        prompt = self.create_5segment_prompt(segment, segment_number, total_segments)

        success, response, processing_time = self.call_cloud_model(model_name, prompt)

        if not success:
            return {
                'success': False,
                'model': model_name,
                'segment_number': segment_number,
                'error': response,
                'processing_time': processing_time
            }

        # è§£æJSONå“åº”
        parse_result = self.parse_json_response(response)

        if not parse_result['success']:
            return {
                'success': False,
                'model': model_name,
                'segment_number': segment_number,
                'error': f"JSONè§£æå¤±è´¥: {parse_result['error']}",
                'raw_response': response[:500] if response else '',
                'processing_time': processing_time
            }

        # éªŒè¯è¯„åˆ†æ ‡å‡†
        data = parse_result['data']
        if 'scores' in data:
            invalid_scores = []
            for trait, score in data['scores'].items():
                if score not in [1, 2, 3, 4, 5]:
                    invalid_scores.append(f"{trait}:{score}")
                    # ä¿®æ­£æ— æ•ˆè¯„åˆ†åˆ°æœ€è¿‘çš„æœ‰æ•ˆå€¼
                    if score < 1:
                        data['scores'][trait] = 1
                    elif score > 5:
                        data['scores'][trait] = 5

            if invalid_scores:
                print(f"      âš ï¸ {model_name} ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")

        result = {
            'success': True,
            'model': model_name,
            'segment_number': segment_number,
            'data': data,
            'parsing_method': parse_result['method'],
            'processing_time': processing_time
        }

        return result

    def calculate_trimmed_mean_scores(self, all_model_results: List[Dict]) -> Dict:
        """è®¡ç®—å‰”é™¤æœ€é«˜åˆ†å’Œæœ€ä½åˆ†åçš„å‡åˆ†"""
        if not all_model_results:
            return {}

        # æ”¶é›†æ‰€æœ‰æ¨¡å‹å¯¹æ¯ä¸ªç‰¹è´¨çš„è¯„åˆ†
        trait_scores = {}
        for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
            scores = []
            for result in all_model_results:
                if result['success'] and 'data' in result and 'scores' in result['data']:
                    score = result['data']['scores'].get(trait)
                    if score is not None:
                        scores.append(score)

            if scores:
                trait_scores[trait] = scores

        # è®¡ç®—å‰”é™¤æœ€é«˜åˆ†å’Œæœ€ä½åˆ†åçš„å‡åˆ†
        final_scores = {}
        for trait, scores in trait_scores.items():
            if len(scores) >= 3:
                # å‰”é™¤æœ€é«˜åˆ†å’Œæœ€ä½åˆ†
                scores_sorted = sorted(scores)
                trimmed_scores = scores_sorted[1:-1]  # å»æ‰ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ª
                final_scores[trait] = int(statistics.mean(trimmed_scores))
            elif len(scores) >= 2:
                # å¦‚æœåªæœ‰2ä¸ªè¯„åˆ†ï¼Œå–å¹³å‡å€¼
                final_scores[trait] = int(statistics.mean(scores))
            elif len(scores) == 1:
                # å¦‚æœåªæœ‰1ä¸ªè¯„åˆ†ï¼Œç›´æ¥ä½¿ç”¨
                final_scores[trait] = scores[0]

        return final_scores

    def calculate_mbti_type(self, scores: Dict) -> str:
        """æ ¹æ®Big5è¯„åˆ†è®¡ç®—MBTIç±»å‹"""
        try:
            openness = scores.get('openness_to_experience', 3)
            conscientiousness = scores.get('conscientiousness', 3)
            extraversion = scores.get('extraversion', 3)
            agreeableness = scores.get('agreeableness', 3)
            neuroticism = scores.get('neuroticism', 3)

            # I/Eç»´åº¦
            I_E = 'I' if extraversion <= 3 else 'E'

            # S/Nç»´åº¦
            S_N = 'N' if openness >= 4 else 'S'

            # T/Fç»´åº¦
            T_F = 'F' if agreeableness >= 4 else 'T'

            # J/Pç»´åº¦
            J_P = 'J' if conscientiousness >= 4 else 'P'

            return f"{I_E}{S_N}{T_F}{J_P}"
        except Exception:
            return "UNKNOWN"

    def check_model_divergence(self, model_results: List[Dict]) -> Dict:
        """æ£€æŸ¥æ¨¡å‹é—´çš„åˆ†æ­§ç¨‹åº¦"""
        if len(model_results) < 2:
            return {"divergence_level": "low", "need_backup": False}

        # è®¡ç®—æ¯ä¸ªç‰¹è´¨çš„æ ‡å‡†å·®
        trait_std_devs = []
        for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
            scores = []
            for result in model_results:
                if result['success'] and 'data' in result and 'scores' in result['data']:
                    score = result['data']['scores'].get(trait)
                    if score is not None:
                        scores.append(score)

            if len(scores) >= 2:
                std_dev = statistics.stdev(scores)
                trait_std_devs.append(std_dev)

        if trait_std_devs:
            avg_std_dev = statistics.mean(trait_std_devs)

            # åˆ¤æ–­åˆ†æ­§ç¨‹åº¦
            if avg_std_dev >= 1.5:
                return {"divergence_level": "high", "need_backup": True, "avg_std_dev": avg_std_dev}
            elif avg_std_dev >= 1.0:
                return {"divergence_level": "medium", "need_backup": False, "avg_std_dev": avg_std_dev}
            else:
                return {"divergence_level": "low", "need_backup": False, "avg_std_dev": avg_std_dev}

        return {"divergence_level": "unknown", "need_backup": False}

    def extract_questions_from_file(self, file_path: str) -> List[Dict]:
        """ä»è¯„ä¼°æ–‡ä»¶ä¸­æå–é—®é¢˜"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            questions = []
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for item in data['assessment_results']:
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', question_data.get('mapped_ipip_concept', ''))

                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']
                            elif 'conversation_log' in item and isinstance(item['conversation_log'], list):
                                for msg in item['conversation_log']:
                                    if isinstance(msg, dict) and msg.get('role') == 'assistant':
                                        answer_text = msg.get('content', '')
                                        break

                            if question_text and answer_text:
                                questions.append({
                                    'question': question_text,
                                    'answer': answer_text
                                })

            return questions

        except Exception as e:
            print(f"  âŒ æå–é—®é¢˜å¤±è´¥: {e}")
            return []

    def analyze_file_with_cloud_models(self, file_path: str, output_dir: str) -> Dict:
        """ä½¿ç”¨äº‘æ¨¡å‹åˆ†æå•ä¸ªæ–‡ä»¶"""
        print(f"ğŸ“ˆ å¼€å§‹äº‘æ¨¡å‹åˆ†æ®µåˆ†æ: {Path(file_path).name}")

        try:
            # æå–é—®é¢˜
            questions = self.extract_questions_from_file(file_path)

            if len(questions) < 5:
                raise Exception(f"é—®é¢˜æ•°é‡ä¸è¶³ï¼š{len(questions)}")

            # åˆ†æ®µå¤„ç†ï¼ˆæ¯æ®µ5é¢˜ï¼Œå–å‰50é¢˜ï¼‰
            segment_size = 5
            questions_to_process = questions[:50]  # å–å‰50é¢˜
            segments = []

            for i in range(0, len(questions_to_process), segment_size):
                segment = questions_to_process[i:i+segment_size]
                if len(segment) == segment_size:
                    segments.append(segment)

            total_segments = len(segments)
            print(f"  ğŸ“Š {len(questions)}é¢˜ -> {total_segments}æ®µ (æ¯æ®µ5é¢˜)")

            # åˆ†ææ‰€æœ‰åˆ†æ®µ
            all_segment_results = []
            total_start_time = time.time()

            for i, segment in enumerate(segments, 1):
                print(f"  ğŸ” åˆ†æç¬¬{i}/{total_segments}æ®µ...")

                # å…ˆç”¨ä¸»è¦ä¸‰ä¸ªæ¨¡å‹åˆ†æ
                primary_results = []
                with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                    future_to_model = {}

                    for model in self.primary_models:
                        model_name = model["name"]
                        print(f"    ğŸŒ è°ƒç”¨æ¨¡å‹: {model_name}")

                        future = executor.submit(
                            self.analyze_segment_with_model,
                            model_name,
                            segment,
                            i,
                            total_segments
                        )
                        future_to_model[future] = model_name

                    # æ”¶é›†ä¸»è¦æ¨¡å‹ç»“æœ
                    for future in concurrent.futures.as_completed(future_to_model, timeout=300):
                        model_name = future_to_model[future]
                        try:
                            result = future.result()
                            primary_results.append(result)

                            if result['success']:
                                scores = result['data']['scores']
                                print(f"      âœ… {model_name}: {list(scores.values())} ({result.get('processing_time', 0):.1f}s)")
                            else:
                                print(f"      âŒ {model_name}: {result.get('error', 'Unknown error')}")

                        except Exception as e:
                            print(f"      âš ï¸ {model_name}: å¤„ç†å¼‚å¸¸ - {e}")

                # æ£€æŸ¥åˆ†æ­§ç¨‹åº¦
                successful_primary = [r for r in primary_results if r['success']]
                divergence_info = self.check_model_divergence(successful_primary)

                all_results_for_segment = successful_primary.copy()

                # å¦‚æœåˆ†æ­§å¤ªå¤§ï¼Œæ·»åŠ å¤‡ç”¨æ¨¡å‹
                if divergence_info['need_backup'] and len(successful_primary) >= 2:
                    print(f"    ğŸ”„ åˆ†æ­§è¾ƒå¤§(æ ‡å‡†å·®={divergence_info['avg_std_dev']:.2f})ï¼Œæ·»åŠ å¤‡ç”¨æ¨¡å‹...")

                    backup_results = []
                    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                        future_to_model = {}

                        for model in self.backup_models:
                            model_name = model["name"]
                            print(f"    ğŸŒ è°ƒç”¨å¤‡ç”¨æ¨¡å‹: {model_name}")

                            future = executor.submit(
                                self.analyze_segment_with_model,
                                model_name,
                                segment,
                                i,
                                total_segments
                            )
                            future_to_model[future] = model_name

                        # æ”¶é›†å¤‡ç”¨æ¨¡å‹ç»“æœ
                        for future in concurrent.futures.as_completed(future_to_model, timeout=300):
                            model_name = future_to_model[future]
                            try:
                                result = future.result()
                                backup_results.append(result)

                                if result['success']:
                                    scores = result['data']['scores']
                                    print(f"      âœ… {model_name}: {list(scores.values())} ({result.get('processing_time', 0):.1f}s)")
                                else:
                                    print(f"      âŒ {model_name}: {result.get('error', 'Unknown error')}")

                            except Exception as e:
                                print(f"      âš ï¸ {model_name}: å¤„ç†å¼‚å¸¸ - {e}")

                    # æ·»åŠ æˆåŠŸçš„å¤‡ç”¨æ¨¡å‹ç»“æœ
                    successful_backup = [r for r in backup_results if r['success']]
                    all_results_for_segment.extend(successful_backup)

                # è®¡ç®—å‰”é™¤æœ€é«˜æœ€ä½åˆ†åçš„å‡åˆ†
                if len(all_results_for_segment) >= 2:
                    final_scores = self.calculate_trimmed_mean_scores(all_results_for_segment)
                    print(f"      ğŸ“Š æœ€ç»ˆè¯„åˆ†(å‰”é™¤æå€¼): {list(final_scores.values())}")
                else:
                    # å¦‚æœåªæœ‰ä¸€ä¸ªæˆåŠŸç»“æœï¼Œç›´æ¥ä½¿ç”¨
                    if all_results_for_segment:
                        final_scores = all_results_for_segment[0]['data']['scores']
                        print(f"      ğŸ“Š ä½¿ç”¨å•ä¸€æ¨¡å‹è¯„åˆ†: {list(final_scores.values())}")
                    else:
                        final_scores = {}
                        print(f"      âŒ è¯¥æ®µæ— æœ‰æ•ˆè¯„åˆ†")

                # ç”Ÿæˆåˆ†æ®µç»“æœ
                segment_result = {
                    'segment_number': i,
                    'primary_results': primary_results,
                    'backup_results': backup_results if 'backup_results' in locals() else [],
                    'divergence_info': divergence_info,
                    'all_successful_results': all_results_for_segment,
                    'final_scores': final_scores,
                    'models_used': len(all_results_for_segment)
                }

                all_segment_results.append(segment_result)
                self.stats['successful_segments'] += 1
                self.stats['total_segments'] += 1

            # è®¡ç®—æ–‡ä»¶æ€»ä½“è¯„åˆ†
            if all_segment_results:
                # èšåˆæ‰€æœ‰åˆ†æ®µçš„æœ€ç»ˆè¯„åˆ†
                file_final_scores = {}
                for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                    all_scores = []
                    for segment_result in all_segment_results:
                        if segment_result['final_scores'] and trait in segment_result['final_scores']:
                            all_scores.append(segment_result['final_scores'][trait])

                    if all_scores:
                        file_final_scores[trait] = int(statistics.median(all_scores))

                # ç”ŸæˆMBTIç±»å‹
                mbti_type = self.calculate_mbti_type(file_final_scores)

                # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
                avg_models_per_segment = sum(r['models_used'] for r in all_segment_results) / len(all_segment_results)
                segments_with_backup = sum(1 for r in all_segment_results if r['divergence_info']['need_backup'])

                total_time = time.time() - total_start_time

                # ä¿å­˜ç»“æœ
                output_filename = f"{Path(file_path).stem}_cloud_segment_analysis.json"
                output_path = os.path.join(output_dir, output_filename)

                analysis_result = {
                    "file_info": {
                        "filename": Path(file_path).name,
                        "total_questions": len(questions),
                        "segments_analyzed": total_segments,
                        "questions_per_segment": segment_size,
                        "analysis_date": datetime.now().isoformat(),
                        "analysis_method": "5é¢˜åˆ†æ®µï¼Œäº‘æ¨¡å‹è¯„ä¼°ï¼Œå‰”é™¤æå€¼å‡åˆ†"
                    },
                    "models_config": {
                        "primary_models": [{"name": m["name"], "description": m["description"]} for m in self.primary_models],
                        "backup_models": [{"name": m["name"], "description": m["description"]} for m in self.backup_models]
                    },
                    "segment_results": all_segment_results,
                    "file_summary": {
                        "final_scores": file_final_scores,
                        "mbti_type": mbti_type,
                        "total_segments": total_segments,
                        "successful_segments": len(all_segment_results),
                        "avg_models_per_segment": round(avg_models_per_segment, 1),
                        "segments_needed_backup": segments_with_backup,
                        "backup_usage_rate": round(segments_with_backup / total_segments * 100, 1) if total_segments > 0 else 0
                    },
                    "performance_metrics": {
                        "total_processing_time": total_time,
                        "avg_time_per_segment": total_time / total_segments if total_segments > 0 else 0
                    }
                }

                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(safe_json_dumps(analysis_result, indent=2))

                print(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_filename}")
                print(f"  ğŸ“‹ åˆ†æç»“æœæ‘˜è¦:")
                print(f"    æœ€ç»ˆè¯„åˆ†: {file_final_scores}")
                print(f"    MBTIç±»å‹: {mbti_type}")
                print(f"    å¹³å‡ä½¿ç”¨æ¨¡å‹æ•°: {avg_models_per_segment:.1f}")
                print(f"    å¤‡ç”¨æ¨¡å‹ä½¿ç”¨ç‡: {segments_with_backup/total_segments*100:.1f}%")

                return {
                    'success': True,
                    'file_path': file_path,
                    'output_path': output_path,
                    'final_scores': file_final_scores,
                    'mbti_type': mbti_type,
                    'total_segments': total_segments,
                    'successful_segments': len(all_segment_results),
                    'avg_models_per_segment': avg_models_per_segment,
                    'backup_usage_rate': segments_with_backup / total_segments if total_segments > 0 else 0,
                    'total_time': total_time
                }

            else:
                raise Exception("æ²¡æœ‰æˆåŠŸçš„åˆ†æ®µåˆ†æç»“æœ")

        except Exception as e:
            print(f"  âŒ æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
            self.stats['failed_files'] += 1
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e)
            }

    def batch_analyze(self, input_dir: str, output_dir: str = "cloud_segment_results", max_files: int = None):
        """æ‰¹é‡åˆ†æå¤šä¸ªæ–‡ä»¶"""
        print("ğŸš€ äº‘æ¨¡å‹5é¢˜åˆ†æ®µè¯„ä¼°å™¨")
        print("=" * 50)
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¤– ä¸»è¦æ¨¡å‹: {', '.join([m['name'] for m in self.primary_models])}")
        print(f"ğŸ”„ å¤‡ç”¨æ¨¡å‹: {', '.join([m['name'] for m in self.backup_models])}")
        print(f"ğŸ“Š åˆ†æ®µæ–¹å¼: 5é¢˜åˆ†æ®µ")
        print(f"ğŸ“ˆ è¯„åˆ†ç­–ç•¥: å‰”é™¤æœ€é«˜åˆ†å’Œæœ€ä½åˆ†åè®¡ç®—å‡åˆ†")
        print(f"ğŸ¯ åˆ†åˆ†æ­§æ£€æµ‹: è‡ªåŠ¨å¢åŠ å¤‡ç”¨æ¨¡å‹")
        print()

        # æ£€æŸ¥é…ç½®
        if not self.api_key or not self.base_url:
            print("âŒ OpenRouteré…ç½®ç¼ºå¤±ï¼Œè¯·æ£€æŸ¥openrouter.txtæ–‡ä»¶")
            return

        self.stats['processing_start'] = datetime.now()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        input_path = Path(input_dir)
        if input_path.is_file():
            files = [input_path]
        elif input_path.is_dir():
            files = list(input_path.glob("*.json"))
        else:
            print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_dir}")
            return

        if max_files:
            files = files[:max_files]

        self.stats['total_files'] = len(files)
        print(f"ğŸ“Š æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")

        if not files:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
            return

        # æ‰¹é‡å¤„ç†
        batch_results = []

        for i, file_path in enumerate(files, 1):
            print(f"ğŸ“ˆ [{i}/{len(files)}] å¤„ç†: {file_path.name}")

            result = self.analyze_file_with_cloud_models(str(file_path), output_dir)
            batch_results.append(result)

            if result['success']:
                self.stats['processed_files'] += 1
            else:
                self.stats['failed_files'] += 1

            # æ˜¾ç¤ºè¿›åº¦
            successful = len([r for r in batch_results if r.get('success', False)])
            print(f"   è¿›åº¦: {successful}/{len(batch_results)} æˆåŠŸ")
            print()

        # å®Œæˆç»Ÿè®¡
        self.stats['processing_end'] = datetime.now()
        if self.stats['processing_start'] and self.stats['processing_end']:
            processing_time = (self.stats['processing_end'] - self.stats['processing_start']).total_seconds()
        else:
            processing_time = 0

        print("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ")
        print("=" * 50)
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        print(f"âœ… å¤„ç†æˆåŠŸ: {self.stats['processed_files']}")
        print(f"âŒ å¤„ç†å¤±è´¥: {self.stats['failed_files']}")
        print(f"ğŸ“Š æ€»åˆ†æ®µæ•°: {self.stats['total_segments']}")
        print(f"âœ… æˆåŠŸåˆ†æ®µ: {self.stats['successful_segments']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {self.stats['successful_segments']/max(1, self.stats['total_segments'])*100:.1f}%")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")

        # å‡†å¤‡ç»Ÿè®¡æ•°æ®ï¼ˆè½¬æ¢datetimeå¯¹è±¡ä¸ºISOå­—ç¬¦ä¸²ï¼‰
        safe_stats = self.stats.copy()
        if safe_stats.get('processing_start'):
            safe_stats['processing_start'] = safe_stats['processing_start'].isoformat()
        if safe_stats.get('processing_end'):
            safe_stats['processing_end'] = safe_stats['processing_end'].isoformat()

        # ä¿å­˜æ‰¹é‡å¤„ç†æŠ¥å‘Š
        batch_report = {
            "batch_info": {
                "primary_models": [{"name": m["name"], "description": m["description"]} for m in self.primary_models],
                "backup_models": [{"name": m["name"], "description": m["description"]} for m in self.backup_models],
                "segment_size": 5,
                "scoring_strategy": "trimmed_mean_remove_extremes",
                "processing_date": datetime.now().isoformat(),
                "processing_time": processing_time,
                "analysis_method": "5é¢˜åˆ†æ®µï¼Œäº‘æ¨¡å‹è¯„ä¼°ï¼Œåˆ†æ­§æ£€æµ‹ï¼Œå‰”é™¤æå€¼å‡åˆ†"
            },
            "input_files": [str(f) for f in files],
            "results": batch_results,
            "statistics": safe_stats,
            "summary": {
                "total_files": self.stats['total_files'],
                "successful_files": self.stats['processed_files'],
                "failed_files": self.stats['failed_files'],
                "success_rate": self.stats['processed_files']/max(1, self.stats['total_files'])*100,
                "avg_backup_usage_rate": sum(r.get('backup_usage_rate', 0) for r in batch_results if r.get('success')) / max(1, len([r for r in batch_results if r.get('success')])) * 100
            }
        }

        with open(os.path.join(output_dir, "cloud_segment_batch_report.json"), 'w', encoding='utf-8') as f:
            f.write(safe_json_dumps(batch_report, indent=2))

        print(f"ğŸ“„ æ‰¹é‡æŠ¥å‘Šå·²ä¿å­˜: cloud_segment_batch_report.json")

        return batch_report

def main():
    """ä¸»å‡½æ•°"""
    evaluator = CloudModelSegmentEvaluator()

    # è¾“å…¥è¾“å‡ºç›®å½•
    input_dir = "results/results"  # å¯æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    output_dir = "cloud_segment_results"

    # æ‰¹é‡åˆ†æ - å¤„ç†å…¨éƒ¨æ–‡ä»¶
    evaluator.batch_analyze(input_dir, output_dir, max_files=None)  # å¤„ç†æ‰€æœ‰æ–‡ä»¶

if __name__ == "__main__":
    main()