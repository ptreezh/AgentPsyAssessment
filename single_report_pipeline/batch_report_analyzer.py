#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æå™¨ - æ”¯æŒæ–­ç‚¹ç»­è·‘
å¤„ç†å¤šä¸ªæµ‹è¯„æŠ¥å‘Šæ–‡ä»¶ï¼Œæ”¯æŒä¸­æ–­åç»§ç»­è¿è¡Œ
"""

import sys
import os
import json
import argparse
from pathlib import Path
from datetime import datetime
import time
import pickle
from typing import List, Dict, Any

# æ·»åŠ åŒ…ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from single_report_pipeline import TransparentPipeline


class BatchReportAnalyzer:
    """æ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æå™¨ - æ”¯æŒæ–­ç‚¹ç»­è·‘"""
    
    def __init__(self, input_dir: str, output_dir: str, checkpoint_interval: int = 5):
        """
        åˆå§‹åŒ–æ‰¹é‡åˆ†æå™¨
        
        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            checkpoint_interval: æ£€æŸ¥ç‚¹é—´éš”ï¼ˆå¤„ç†å¤šå°‘æ–‡ä»¶åä¿å­˜æ£€æŸ¥ç‚¹ï¼‰
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.checkpoint_interval = checkpoint_interval
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        self.checkpoint_file = self.output_dir / "batch_analysis_checkpoint.pkl"
        self.results_file = self.output_dir / "batch_analysis_results.json"
        self.summary_file = self.output_dir / "batch_analysis_summary.md"
        
        # åˆ›å»ºæµæ°´çº¿å®ä¾‹
        self.pipeline = TransparentPipeline()
        
        # åˆå§‹åŒ–çŠ¶æ€å˜é‡
        self.processed_files = set()
        self.results = []
        self.start_time = datetime.now()
        self.total_files = 0
        self.current_file_index = 0
        
        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        self.load_checkpoint()
    
    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'rb') as f:
                    checkpoint_data = pickle.load(f)
                
                self.processed_files = set(checkpoint_data.get('processed_files', []))
                self.results = checkpoint_data.get('results', [])
                self.start_time = checkpoint_data.get('start_time', datetime.now())
                self.total_files = checkpoint_data.get('total_files', 0)
                self.current_file_index = checkpoint_data.get('current_file_index', 0)
                
                print(f"âœ… å·²åŠ è½½æ£€æŸ¥ç‚¹: å¤„ç†äº† {len(self.processed_files)} ä¸ªæ–‡ä»¶")
                print(f"   å½“å‰ç´¢å¼•: {self.current_file_index}")
                print(f"   æ€»æ–‡ä»¶æ•°: {self.total_files}")
                return True
                
            except Exception as e:
                print(f"âš ï¸  åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                self.reset_state()
                return False
        else:
            print("â„¹ï¸  æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå¼€å§‹å…¨æ–°åˆ†æ")
            self.reset_state()
            return False
    
    def reset_state(self):
        """é‡ç½®çŠ¶æ€"""
        self.processed_files = set()
        self.results = []
        self.start_time = datetime.now()
        self.total_files = 0
        self.current_file_index = 0
    
    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_data = {
            'processed_files': list(self.processed_files),
            'results': self.results,
            'start_time': self.start_time,
            'total_files': self.total_files,
            'current_file_index': self.current_file_index
        }
        
        try:
            with open(self.checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint_data, f)
            print("âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        results_data = {
            'analysis_info': {
                'start_time': self.start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'total_files': self.total_files,
                'processed_files': len(self.processed_files),
                'remaining_files': self.total_files - len(self.processed_files),
                'duration_seconds': (datetime.now() - self.start_time).total_seconds()
            },
            'results': self.results
        }
        
        try:
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False
    
    def save_summary_report(self):
        """ä¿å­˜æ‘˜è¦æŠ¥å‘Š"""
        if not self.results:
            return False
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_processed = len(self.processed_files)
        total_files = self.total_files
        duration = (datetime.now() - self.start_time).total_seconds()
        
        # è®¡ç®—å¤§äº”äººæ ¼ç»Ÿè®¡
        big5_stats = {
            'openness_to_experience': [],
            'conscientiousness': [],
            'extraversion': [],
            'agreeableness': [],
            'neuroticism': []
        }
        
        for result in self.results:
            if result.get('success', False):
                big5_scores = result.get('big5_scores', {})
                for trait in big5_stats:
                    if trait in big5_scores:
                        big5_stats[trait].append(big5_scores[trait])
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        summary_lines = [
            "# æ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†ææ‘˜è¦",
            "",
            "## åŸºæœ¬ä¿¡æ¯",
            f"- **å¼€å§‹æ—¶é—´**: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"- **ç»“æŸæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"- **å¤„ç†æ—¶é•¿**: {duration:.1f} ç§’",
            f"- **æ€»æ–‡ä»¶æ•°**: {total_files}",
            f"- **å·²å¤„ç†æ•°**: {total_processed}",
            f"- **å‰©ä½™æ–‡ä»¶**: {total_files - total_processed}",
            f"- **å¤„ç†é€Ÿç‡**: {total_processed/duration*60:.1f} æ–‡ä»¶/åˆ†é’Ÿ" if duration > 0 else "-",
            "",
            "## å¤§äº”äººæ ¼ç»Ÿè®¡",
        ]
        
        for trait, scores in big5_stats.items():
            if scores:
                avg_score = sum(scores) / len(scores)
                min_score = min(scores)
                max_score = max(scores)
                std_dev = (sum((s - avg_score)**2 for s in scores) / len(scores))**0.5 if len(scores) > 1 else 0
                
                trait_name = {
                    'openness_to_experience': 'å¼€æ”¾æ€§',
                    'conscientiousness': 'å°½è´£æ€§',
                    'extraversion': 'å¤–å‘æ€§',
                    'agreeableness': 'å®œäººæ€§',
                    'neuroticism': 'ç¥ç»è´¨'
                }.get(trait, trait)
                
                summary_lines.extend([
                    f"### {trait_name}",
                    f"- **å¹³å‡åˆ†**: {avg_score:.2f}",
                    f"- **æœ€å°å€¼**: {min_score:.2f}",
                    f"- **æœ€å¤§å€¼**: {max_score:.2f}",
                    f"- **æ ‡å‡†å·®**: {std_dev:.2f}",
                    f"- **æ ·æœ¬æ•°**: {len(scores)}",
                    ""
                ])
        
        # æ·»åŠ å¤„ç†è¯¦æƒ…
        summary_lines.extend([
            "## å¤„ç†è¯¦æƒ…",
            "| æ–‡ä»¶å | çŠ¶æ€ | å¤„ç†æ—¶é—´ | å¤§äº”å¾—åˆ† | MBTIç±»å‹ |",
            "|-------|------|----------|---------|----------|"
        ])
        
        for result in self.results[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªç»“æœ
            filename = Path(result.get('file_path', '')).name
            status = "âœ… æˆåŠŸ" if result.get('success', False) else "âŒ å¤±è´¥"
            processing_time = result.get('processing_time', 0)
            big5_scores = result.get('big5_scores', {})
            mbti_type = result.get('mbti_type', 'Unknown')
            
            # ç®€åŒ–å¤§äº”å¾—åˆ†æ˜¾ç¤º
            big5_str = ", ".join([f"{k[:1]}:{v}" for k, v in big5_scores.items()]) if big5_scores else "N/A"
            
            summary_lines.append(
                f"| {filename} | {status} | {processing_time:.1f}s | {big5_str} | {mbti_type} |"
            )
        
        if len(self.results) > 20:
            summary_lines.append(f"| ...è¿˜æœ‰ {len(self.results) - 20} ä¸ªæ–‡ä»¶... | | | | |")
        
        summary_lines.extend([
            "",
            "## å¤„ç†å®Œæˆ",
            f"âœ… æ‰¹é‡åˆ†æå·²å®Œæˆ {total_processed}/{total_files} ä¸ªæ–‡ä»¶",
            f"ğŸ” å¦‚éœ€ç»§ç»­å¤„ç†å‰©ä½™æ–‡ä»¶ï¼Œè¯·é‡æ–°è¿è¡Œæ­¤è„šæœ¬"
        ])
        
        try:
            with open(self.summary_file, 'w', encoding='utf-8') as f:
                f.write("\n".join(summary_lines))
            print(f"âœ… æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜åˆ°: {self.summary_file}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ‘˜è¦æŠ¥å‘Šå¤±è´¥: {e}")
            return False
    
    def find_json_files(self, pattern: str = "*.json") -> List[Path]:
        """
        æŸ¥æ‰¾JSONæ–‡ä»¶
        
        Args:
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            
        Returns:
            JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        json_files = list(self.input_dir.glob(pattern))
        json_files.sort()  # æŒ‰æ–‡ä»¶åæ’åºç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´
        return json_files
    
    def process_single_report(self, file_path: Path) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæµ‹è¯„æŠ¥å‘Š
        
        Args:
            file_path: æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœ
        """
        print(f"ğŸ” å¤„ç†: {file_path.name}")
        
        start_time = time.time()
        
        try:
            # å¤„ç†æµ‹è¯„æŠ¥å‘Š
            result = self.pipeline.process_single_report(str(file_path))
            
            processing_time = time.time() - start_time
            
            if result and result.get('success', False):
                print(f"  âœ… å®Œæˆ: {file_path.name}")
                print(f"    å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")
                print(f"    å¤§äº”äººæ ¼: {result.get('big5_scores', {})}")
                print(f"    MBTIç±»å‹: {result.get('mbti_type', 'Unknown')}")
                return {
                    **result,
                    'file_path': str(file_path),
                    'processing_time': round(processing_time, 1),
                    'success': True
                }
            else:
                print(f"  âŒ å¤±è´¥: {file_path.name}")
                error_msg = result.get('error', 'Unknown error') if result else 'No result'
                print(f"    é”™è¯¯: {error_msg}")
                return {
                    'success': False,
                    'file_path': str(file_path),
                    'error': error_msg,
                    'processing_time': round(processing_time, 1)
                }
                
        except Exception as e:
            processing_time = time.time() - start_time
            print(f"  ğŸ’¥ å¼‚å¸¸: {file_path.name} - {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'file_path': str(file_path),
                'error': str(e),
                'processing_time': round(processing_time, 1)
            }
    
    def run_batch_analysis(self, pattern: str = "*.json", limit: int = None, 
                          resume: bool = True, no_save: bool = False) -> bool:
        """
        è¿è¡Œæ‰¹é‡åˆ†æ
        
        Args:
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            limit: é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡
            resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
            no_save: æ˜¯å¦ä¸ä¿å­˜ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        print("ğŸš€ æ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æå™¨ - æ–­ç‚¹ç»­è·‘ç‰ˆ")
        print("="*80)
        print(f"è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"æ£€æŸ¥ç‚¹é—´éš”: æ¯ {self.checkpoint_interval} ä¸ªæ–‡ä»¶")
        print(f"æ¢å¤æ¨¡å¼: {'å¯ç”¨' if resume else 'ç¦ç”¨'}")
        print()
        
        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if resume:
            self.load_checkpoint()
        
        # æŸ¥æ‰¾æ–‡ä»¶
        print("ğŸ“‚ æŸ¥æ‰¾æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶...")
        json_files = self.find_json_files(pattern)
        
        if not json_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
            return False
        
        self.total_files = len(json_files)
        if limit:
            json_files = json_files[:limit]
            self.total_files = len(json_files)
        
        print(f"  æ‰¾åˆ° {len(json_files)} ä¸ªæµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
        print(f"  å·²å¤„ç†: {len(self.processed_files)} ä¸ª")
        print(f"  å‰©ä½™: {len(json_files) - len(self.processed_files)} ä¸ª")
        print()
        
        # ç¡®å®šèµ·å§‹ä½ç½®
        start_index = 0
        if resume and self.current_file_index < len(json_files):
            start_index = self.current_file_index
        
        print(f"â–¶ï¸  ä»ç¬¬ {start_index + 1} ä¸ªæ–‡ä»¶å¼€å§‹å¤„ç†")
        print()
        
        # å¤„ç†æ–‡ä»¶
        processed_count = 0
        success_count = 0
        failed_count = 0
        
        for i, file_path in enumerate(json_files[start_index:], start_index):
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
            if str(file_path) in self.processed_files:
                print(f"â­ï¸  è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}")
                continue
            
            # å¤„ç†æ–‡ä»¶
            result = self.process_single_report(file_path)
            
            # æ›´æ–°çŠ¶æ€
            self.processed_files.add(str(file_path))
            self.results.append(result)
            self.current_file_index = i + 1
            
            if result.get('success', False):
                success_count += 1
            else:
                failed_count += 1
            
            processed_count += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if processed_count % 10 == 0:
                print(f"  ğŸ“Š è¿›åº¦: {processed_count} ä¸ªæ–‡ä»¶å·²å¤„ç† "
                      f"(æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count})")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if processed_count % self.checkpoint_interval == 0 and not no_save:
                print(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...")
                self.save_checkpoint()
                self.save_results()
                self.save_summary_report()
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIè¿‡è½½
            time.sleep(1)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        print(f"\nğŸ æ‰¹é‡åˆ†æå®Œæˆ!")
        print("="*80)
        print(f"æ€»æ–‡ä»¶æ•°: {len(json_files)}")
        print(f"å·²å¤„ç†æ•°: {processed_count}")
        print(f"æˆåŠŸå¤„ç†: {success_count}")
        print(f"å¤„ç†å¤±è´¥: {failed_count}")
        print(f"æˆåŠŸç‡: {success_count/processed_count*100:.1f}%" if processed_count > 0 else "N/A")
        
        if not no_save:
            self.save_checkpoint()
            self.save_results()
            self.save_summary_report()
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        print(f"ğŸ” å¦‚éœ€ç»§ç»­å¤„ç†å‰©ä½™æ–‡ä»¶ï¼Œè¯·é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æå™¨ - æ”¯æŒæ–­ç‚¹ç»­è·‘')
    parser.add_argument('--input-dir', default='../results/readonly-original',
                       help='è¾“å…¥ç›®å½• (é»˜è®¤: ../results/readonly-original)')
    parser.add_argument('--output-dir', default='../results/batch-analysis-results',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: ../results/batch-analysis-results)')
    parser.add_argument('--pattern', default='*.json',
                       help='æ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: *.json)')
    parser.add_argument('--limit', type=int,
                       help='é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡')
    parser.add_argument('--checkpoint-interval', type=int, default=5,
                       help='æ£€æŸ¥ç‚¹é—´éš” (é»˜è®¤: æ¯5ä¸ªæ–‡ä»¶)')
    parser.add_argument('--no-resume', action='store_true',
                       help='ä¸ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œé‡æ–°å¼€å§‹')
    parser.add_argument('--no-save', action='store_true',
                       help='ä¸ä¿å­˜ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰¹é‡åˆ†æå™¨
    analyzer = BatchReportAnalyzer(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        checkpoint_interval=args.checkpoint_interval
    )
    
    # è¿è¡Œæ‰¹é‡åˆ†æ
    success = analyzer.run_batch_analysis(
        pattern=args.pattern,
        limit=args.limit,
        resume=not args.no_resume,
        no_save=args.no_save
    )
    
    if success:
        print("\nğŸ‰ æ‰¹é‡åˆ†ææˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("\nâŒ æ‰¹é‡åˆ†æå¤±è´¥!")
        return 1


if __name__ == "__main__":
    sys.exit(main())