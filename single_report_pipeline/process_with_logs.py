"""
çœŸå®æµ‹è¯„æŠ¥å‘Šå¤„ç†è„šæœ¬ - è¯¦ç»†è®°å½•æ¨¡å‹è°ƒç”¨æ—¥å¿—
"""
import json
import os
import sys
import time

# Add the src directory to the path so we can import our modules
sys.path.insert(0, os.path.join(os.getcwd(), 'src'))

from src.scoring import call_llm_api, parse_score_from_response, score_segment
from src.analysis import calculate_big_five, generate_report


def extract_questions_and_responses_from_assessment(assessment_file_path):
    """ä»çœŸå®JSONè¯„ä¼°æ–‡ä»¶ä¸­æå–é—®é¢˜å’Œå›ç­”"""
    with open(assessment_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    segments_data = []
    for item in data['assessment_results']:
        question_data = item['question_data']
        response = item['extracted_response']
        
        dimension = question_data['dimension']
        concept = question_data['mapped_ipip_concept']
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯åå‘è®¡åˆ†é¢˜
        is_reversed = 'Reversed' in concept
        
        trait_map = {
            'Extraversion': 'E',
            'Agreeableness': 'A', 
            'Conscientiousness': 'C',
            'Neuroticism': 'N', 
            'Openness to Experience': 'O'
        }
        trait = trait_map.get(dimension, 'U')
        
        segments_data.append({
            'question_id': item['question_id'],
            'question': f'{concept}',
            'answer': response,
            'dimension': dimension,
            'trait': trait,
            'is_reversed': is_reversed
        })
    
    return segments_data


def process_single_question_with_logging(segment, model='deepseek-r1:8b'):
    """å¤„ç†å•ä¸ªé—®é¢˜å¹¶è®°å½•è¯¦ç»†æ—¥å¿—"""
    print(f"  ğŸ“ å¤„ç†é—®é¢˜ {segment['question_id']+1}: {segment['question'][:50]}...")
    
    # æ„å»ºè¯„ä¼°æç¤º
    criteria = f"æ ¹æ®å¤§äº”äººæ ¼ç»´åº¦è¯„ä¼°æ­¤å›ç­”ï¼š{segment['dimension']}ã€‚æ ¹æ®è¯„ä¼°é‡è¡¨ä»1-5è¿›è¡Œè¯„åˆ†ã€‚"
    prompt = f"""
    {segment['question']}
    {segment['answer']}
    
    Criteria: {criteria}
    
    Evaluate the response according to the criteria above and provide a numeric score.
    Respond with only the score in the format "Score: X".
    """
    
    print(f"    ğŸ¯ è¯„ä¼°ç»´åº¦: {segment['dimension']} (ç‰¹è´¨: {segment['trait']})")
    print(f"    â†©ï¸  åå‘è®¡åˆ†: {segment['is_reversed']}")
    
    # è®°å½•æ¨¡å‹è°ƒç”¨å¼€å§‹æ—¶é—´
    start_time = time.time()
    print(f"    â±ï¸  å¼€å§‹æ¨¡å‹è°ƒç”¨: {time.strftime('%H:%M:%S', time.localtime(start_time))}")
    
    try:
        # çœŸå®è°ƒç”¨å¤§æ¨¡å‹
        response = call_llm_api(prompt.strip(), model)
        end_time = time.time()
        
        actual_duration = end_time - start_time
        print(f"    âœ… æ¨¡å‹è°ƒç”¨æˆåŠŸå®Œæˆ")
        print(f"    â±ï¸  å®é™…è€—æ—¶: {actual_duration:.2f}ç§’")
        print(f"    ğŸ¤– æ¨¡å‹å“åº”: {repr(response)[:100]}...")
        
        # è§£æåˆ†æ•°
        score = parse_score_from_response(response)
        print(f"    ğŸ¯ è§£æåˆ†æ•°: {score}")
        
        return score, response, actual_duration
        
    except Exception as e:
        end_time = time.time()
        actual_duration = end_time - start_time
        print(f"    âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        print(f"    â±ï¸  è€—æ—¶: {actual_duration:.2f}ç§’")
        # è¿”å›é»˜è®¤åˆ†æ•°
        return 3.0, f"Error: {str(e)}", actual_duration


def process_assessment_report_with_logs(assessment_path, model='deepseek-r1:8b'):
    """å¤„ç†å®Œæ•´çš„è¯„ä¼°æŠ¥å‘Šå¹¶è®°å½•è¯¦ç»†æ—¥å¿—"""
    print(f"ğŸ“ å¤„ç†è¯„ä¼°æŠ¥å‘Š: {os.path.basename(assessment_path)}")
    print("="*80)
    
    # æå–é—®é¢˜æ•°æ®
    segments_data = extract_questions_and_responses_from_assessment(assessment_path)
    print(f"ğŸ“Š å·²æå– {len(segments_data)} ä¸ªé—®é¢˜-å›ç­”å¯¹")
    
    # å¤„ç†æ¯ä¸ªé—®é¢˜å¹¶è®°å½•è¯¦ç»†ä¿¡æ¯
    scores = []
    responses = []
    durations = []
    trait_mapping = {}
    reverse_scoring_map = {}
    
    print(f"\nğŸ”„ å¼€å§‹é€é¢˜è¯„ä¼° (ä½¿ç”¨æ¨¡å‹: {model})")
    print("-" * 60)
    
    for i, segment in enumerate(segments_data):
        print(f"\nç¬¬ {i+1}/50 é¢˜:")
        
        score, response, duration = process_single_question_with_logging(segment, model)
        
        scores.append(score)
        responses.append(response)
        durations.append(duration)
        trait_mapping[len(scores)-1] = segment['trait']
        
        if segment['is_reversed']:
            reverse_scoring_map[len(scores)-1] = True
            print(f"    ğŸ”„ å·²åº”ç”¨åå‘è®¡åˆ†")
    
    print(f"\n{'='*80}")
    print("âœ… æ‰€æœ‰é¢˜ç›®è¯„ä¼°å®Œæˆ")
    print(f"ğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
    print(f"   - æ€»é¢˜ç›®æ•°: {len(scores)}")
    print(f"   - å¹³å‡å¤„ç†æ—¶é—´: {sum(durations)/len(durations):.2f}ç§’/é¢˜")
    print(f"   - æ€»å¤„ç†æ—¶é—´: {sum(durations):.2f}ç§’")
    print(f"   - è¯„åˆ†èŒƒå›´: {min(scores):.1f} - {max(scores):.1f}")
    print(f"   - å¹³å‡åˆ†: {sum(scores)/len(scores):.2f}")
    
    # åº”ç”¨åå‘è®¡åˆ†å¹¶è®¡ç®—å¤§äº”äººæ ¼ç»“æœ
    print(f"\nğŸ”„ åº”ç”¨åå‘è®¡åˆ† (åå‘é¢˜ç›®æ•°: {len(reverse_scoring_map)})")
    
    # è®¡ç®—å¤§äº”äººæ ¼åˆ†æ•°
    big_five_scores = calculate_big_five(
        scores, 
        trait_mapping, 
        reverse_scoring_map, 
        scale_range=(1, 5)
    )
    
    print(f"\nğŸ† å¤§äº”äººæ ¼è¯„ä¼°ç»“æœ:")
    for trait, score in big_five_scores.items():
        trait_names = {'O': 'Openness', 'C': 'Conscientiousness', 
                      'E': 'Extraversion', 'A': 'Agreeableness', 'N': 'Neuroticism'}
        print(f"   {trait_names.get(trait, trait)}: {score:.2f}")
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    metadata = {
        'report_id': os.path.basename(assessment_path),
        'subject_id': 'REAL_ASSESSMENT',
        'date': time.strftime('%Y-%m-%d')
    }
    
    analysis_results = {
        'big_five': big_five_scores,
        'aggregate_score': sum(scores)/len(scores),
        'discrepancy_detected': max(scores) - min(scores) > 3,
        'individual_scores': scores,
        'segment_count': len(segments_data)
    }
    
    final_report = generate_report(metadata, analysis_results)
    print(f"\nğŸ“„ å·²ç”Ÿæˆæœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")
    print("="*80)
    
    return {
        'scores': scores,
        'responses': responses,
        'durations': durations,
        'big_five': big_five_scores,
        'final_report': final_report
    }


def main():
    """ä¸»å‡½æ•° - å¤„ç†çœŸå®æµ‹è¯„æŠ¥å‘Š"""
    print("ğŸ” å¤§æ¨¡å‹è°ƒç”¨ä¸è¯„ä¼°æ—¥å¿—è®°å½•ç³»ç»Ÿ")
    print("===============================================")
    
    # æŒ‡å®šè¯„ä¼°æ–‡ä»¶è·¯å¾„
    assessment_dir = r'D:\AIDevelop\portable_psyagent\results\readonly-original'
    assessment_file = 'asses_gemma3_latest_agent_big_five_50_complete2_def_e0_t0_0_09201.json'
    assessment_path = os.path.join(assessment_dir, assessment_file)
    
    print(f"ğŸ“ è¯„ä¼°æ–‡ä»¶: {assessment_file}")
    print(f"ğŸ“ è·¯å¾„: {assessment_path}")
    
    if not os.path.exists(assessment_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {assessment_path}")
        return
    
    print(f"âœ… æ–‡ä»¶å­˜åœ¨ï¼Œå¼€å§‹å¤„ç†...")
    
    # å¤„ç†è¯„ä¼°æŠ¥å‘Š
    result = process_assessment_report_with_logs(assessment_path, model='deepseek-r1:8b')
    
    print(f"\nğŸ å¤„ç†å®Œæˆï¼")
    print(f"âœ… æ¨¡å‹è°ƒç”¨æˆåŠŸ")
    print(f"âœ… æ¯é¢˜éƒ½ç»è¿‡çœŸå®AIè¯„ä¼°") 
    print(f"âœ… å·²è®°å½•å“åº”æ—¥å¿—")
    print(f"âœ… å·²å®Œæˆå¤§äº”äººæ ¼è®¡ç®—")
    print(f"âœ… å·²ç”Ÿæˆå®Œæ•´æŠ¥å‘Š")


if __name__ == "__main__":
    main()