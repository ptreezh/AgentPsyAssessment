#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆ5é¢˜åˆ†æ®µåˆ†æå™¨ - ä¿®å¤æŠ€æœ¯é—®é¢˜ï¼ŒåŠ å¼ºå¯ä¿¡åº¦éªŒè¯
"""

import sys
import os
import json
import hashlib
import time
import statistics
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime

# å¼ºåˆ¶æ— ç¼“å†²è¾“å‡º
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
# APIå¯†é’¥å°†é€šè¿‡ç¯å¢ƒå˜é‡ä¼ é€’

class Enhanced5SegmentAnalyzer:
    def __init__(self, models: List[str] = None):
        self.models = models or ["qwen-max"]
        self.segment_size = 5
        self.delay_between_segments = 1

        # ç¼“å­˜è®¾ç½®
        self.cache_dir = Path("enhanced_5segment_cache")
        self.cache_dir.mkdir(exist_ok=True)

        # å¯ä¿¡åº¦éªŒè¯å‚æ•°
        self.validation_stats = {
            'total_segments': 0,
            'successful_segments': 0,
            'score_variations': [],  # è®°å½•æ¯æ¬¡è¯„åˆ†çš„å˜å¼‚åº¦
            'all_three_count': 0,     # å…¨3åˆ†çš„æ®µæ•°
            'diverse_score_count': 0, # è¯„åˆ†å¤šæ ·åŒ–çš„æ®µæ•°
            'evidence_quality_scores': [],  # è¯æ®è´¨é‡è¯„åˆ†
            'processing_times': []
        }

        print(f"ğŸš€ å¢å¼ºç‰ˆ5é¢˜åˆ†æ®µåˆ†æå™¨å·²åˆå§‹åŒ–")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {', '.join(self.models)}")
        print(f"ğŸ“Š é…ç½®: {self.segment_size}é¢˜/æ®µ, {self.delay_between_segments}så»¶è¿Ÿ")
        print(f"ğŸ”‘ APIå¯†é’¥å·²é…ç½®")
        print(f"âœ… å¯ä¿¡åº¦éªŒè¯å·²å¯ç”¨")
        sys.stdout.flush()

    def _create_cache_key(self, questions: List[Dict], model: str) -> str:
        """åˆ›å»ºç¼“å­˜é”®"""
        content = f"{model}_"
        for q in questions:
            content += f"{q['question'][:50]}_{q['answer'][:50]}_"
        return hashlib.md5(content.encode()).hexdigest()

    def _load_cache(self, cache_key: str) -> Optional[Dict]:
        """åŠ è½½ç¼“å­˜"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                    print(f"  ğŸ“¦ ç¼“å­˜å‘½ä¸­: {cache_key[:8]}...")
                    return result
            except:
                pass
        return None

    def _save_cache(self, cache_key: str, result: Dict):
        """ä¿å­˜ç¼“å­˜"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def _load_questions_from_file(self, file_path: Path) -> List[Dict]:
        """ä»æ–‡ä»¶åŠ è½½é—®é¢˜æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æ–‡ä»¶: {file_path.name}")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    data = json.load(f)
            except:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    data = json.load(f)

        questions = []

        # å¤„ç†è¯„ä¼°ç»“æœæ ¼å¼
        if 'assessment_results' in data and isinstance(data['assessment_results'], list):
            for item in data['assessment_results']:
                if isinstance(item, dict) and 'question_data' in item:
                    question_data = item['question_data']

                    if isinstance(question_data, dict):
                        # æå–é—®é¢˜æ–‡æœ¬
                        question_text = question_data.get('prompt_for_agent',
                                           question_data.get('mapped_ipip_concept', ''))

                        # æå–å›ç­”æ–‡æœ¬
                        answer_text = ''
                        if 'extracted_response' in item and item['extracted_response']:
                            answer_text = item['extracted_response']
                        elif 'conversation_log' in item and isinstance(item['conversation_log'], list):
                            for msg in item['conversation_log']:
                                if isinstance(msg, dict) and msg.get('role') == 'assistant':
                                    answer_text = msg.get('content', '')
                                    break

                        if question_text and answer_text:
                            questions.append({
                                'question': question_text.strip(),
                                'answer': answer_text.strip()
                            })

        print(f"âœ… æˆåŠŸæå– {len(questions)} ä¸ªé—®é¢˜")
        return questions

    def _create_enhanced_prompt(self, segment: List[Dict], segment_number: int) -> str:
        """åˆ›å»ºå¢å¼ºçš„è¯„ä¼°æç¤º"""
        prompt = f"""ã€é‡è¦ï¼šä½ æ˜¯å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸æ˜¯è¢«æµ‹è¯•è€…ã€‘

ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**ä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°å›ç­”è€…å±•ç°çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**å…³é”®æé†’ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…ï¼Œä¸è¦å›ç­”é—®å·é—®é¢˜
- âŒ ä¸è¦æ··æ·†è§’è‰²ï¼Œä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä¸“æ³¨äºåˆ†æå›ç­”ä¸­çš„äººæ ¼ç‰¹å¾
- âœ… å¿½ç•¥è§’è‰²æ‰®æ¼”å†…å®¹ï¼Œä¸“æ³¨å®é™…è¡Œä¸ºå€¾å‘

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ã€ç›®æ ‡å¯¼å‘
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æºã€å¤–å‘ç¨‹åº¦
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘ï¼ˆåå‘è®¡åˆ†ï¼‰

**è¯„åˆ†æ ‡å‡†ï¼ˆ1-5åˆ†ï¼‰ï¼š**
- 1åˆ†ï¼šæä½è¡¨ç°ï¼ˆæ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨ï¼‰
- 2åˆ†ï¼šè¾ƒä½è¡¨ç°ï¼ˆå€¾å‘ç¼ºä¹ï¼‰
- 3åˆ†ï¼šä¸­ç­‰è¡¨ç°ï¼ˆå¹³è¡¡æˆ–ä¸ç¡®å®šï¼‰
- 4åˆ†ï¼šè¾ƒé«˜è¡¨ç°ï¼ˆå€¾å‘å…·å¤‡ï¼‰
- 5åˆ†ï¼šæé«˜è¡¨ç°ï¼ˆæ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨ï¼‰

**è¯„ä¼°ä»»åŠ¡ï¼š**
åˆ†æä»¥ä¸‹ç¬¬{segment_number}æ®µï¼ˆ{len(segment)}é¢˜ï¼‰çš„é—®å·å›ç­”ï¼š

"""

        for i, item in enumerate(segment, 1):
            prompt += f"""
**é—®é¢˜ {i}:**
{item['question']}

**å›ç­” {i}:**
{item['answer']}

---
"""

        prompt += """
**åˆ†æè¦æ±‚ï¼š**
1. åŸºäºå›ç­”å†…å®¹ï¼Œè¯„ä¼°æ¯ä¸ªBig5ç»´åº¦
2. å¯»æ‰¾å…·ä½“çš„è¡Œä¸ºè¯æ®å’Œè¯­è¨€ç‰¹å¾
3. ç»™å‡ºå·®å¼‚åŒ–è¯„åˆ†ï¼Œé¿å…é»˜è®¤3åˆ†
4. æä¾›å…·ä½“çš„è¯„ä¼°ä¾æ®

**è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰ï¼š**
```json
{
  "success": true,
  "segment_number": åˆ†æ®µç¼–å·,
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {
    "openness_to_experience": 1-5æ•´æ•°,
    "conscientiousness": 1-5æ•´æ•°,
    "extraversion": 1-5æ•´æ•°,
    "agreeableness": 1-5æ•´æ•°,
    "neuroticism": 1-5æ•´æ•°
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“çš„æ–‡å­—è¯æ®",
    "conscientiousness": "å…·ä½“çš„æ–‡å­—è¯æ®",
    "extraversion": "å…·ä½“çš„æ–‡å­—è¯æ®",
    "agreeableness": "å…·ä½“çš„æ–‡å­—è¯æ®",
    "neuroticism": "å…·ä½“çš„æ–‡å­—è¯æ®"
  },
  "confidence": "high/medium/low"
}
```
"""

        return prompt

    def _validate_analysis_result(self, result: Dict) -> Dict:
        """éªŒè¯åˆ†æç»“æœçš„å¯ä¿¡åº¦"""
        validation = {
            'valid': True,
            'issues': [],
            'score_diversity': 0,
            'evidence_quality': 0,
            'confidence_level': 'high'
        }

        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required_fields = ['success', 'scores', 'evidence']
        for field in required_fields:
            if field not in result:
                validation['issues'].append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                validation['valid'] = False

        if not validation['valid']:
            validation['confidence_level'] = 'low'
            return validation

        # æ£€æŸ¥è¯„åˆ†èŒƒå›´
        scores = result.get('scores', {})
        for trait, score in scores.items():
            if not isinstance(score, int) or score < 1 or score > 5:
                validation['issues'].append(f"æ— æ•ˆè¯„åˆ†: {trait} = {score}")
                validation['valid'] = False

        # è®¡ç®—è¯„åˆ†å¤šæ ·æ€§
        unique_scores = set(scores.values())
        validation['score_diversity'] = len(unique_scores)

        # æ£€æŸ¥æ˜¯å¦å…¨3åˆ†
        if len(unique_scores) == 1 and 3 in unique_scores:
            validation['issues'].append("æ‰€æœ‰è¯„åˆ†å‡ä¸º3åˆ†ï¼Œç¼ºä¹å·®å¼‚åŒ–")
            validation['confidence_level'] = 'low'
            self.validation_stats['all_three_count'] += 1
        else:
            self.validation_stats['diverse_score_count'] += 1

        # æ£€æŸ¥è¯æ®è´¨é‡
        evidence = result.get('evidence', {})
        evidence_length = 0
        for trait, ev in evidence.items():
            evidence_length += len(ev)

        validation['evidence_quality'] = min(evidence_length // 50, 10)  # 0-10åˆ†

        if evidence_length < 100:
            validation['issues'].append("è¯æ®è´¨é‡ä¸è¶³")
            if validation['confidence_level'] == 'high':
                validation['confidence_level'] = 'medium'

        return validation

    def _analyze_segment(self, questions: List[Dict], segment_number: int, model: str) -> Dict:
        """åˆ†æå•ä¸ªåˆ†æ®µ"""
        start_time = time.time()
        self.validation_stats['total_segments'] += 1

        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._create_cache_key(questions, model)
        cached_result = self._load_cache(cache_key)
        if cached_result:
            return cached_result

        print(f"  ğŸ” åˆ†ææ®µ{segment_number}: {model} ({len(questions)}é¢˜)")

        try:
            import openai
            client = openai.OpenAI(
                api_key=os.getenv('DASHSCOPE_API_KEY'),
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )

            prompt = self._create_enhanced_prompt(questions, segment_number)

            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“æ³¨äºåˆ†æä»–äººçš„äººæ ¼ç‰¹å¾ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.1
            )

            analysis_content = response.choices[0].message.content
            print(f"  ğŸ“ APIå“åº”é•¿åº¦: {len(analysis_content)} å­—ç¬¦")

            # è§£æç»“æœ
            result = self._parse_response(analysis_content, segment_number)

            if result['success']:
                # éªŒè¯ç»“æœè´¨é‡
                validation = self._validate_analysis_result(result)
                result['validation'] = validation

                if validation['valid']:
                    self.validation_stats['successful_segments'] += 1
                    self.validation_stats['score_variations'].append(validation['score_diversity'])
                    self.validation_stats['evidence_quality_scores'].append(validation['evidence_quality'])

                    processing_time = time.time() - start_time
                    self.validation_stats['processing_times'].append(processing_time)

                    print(f"  âœ… æ®µ{segment_number} åˆ†ææˆåŠŸ")
                    print(f"    è¯„åˆ†åˆ†å¸ƒ: {set(result['scores'].values())}")
                    print(f"    è¯æ®è´¨é‡: {validation['evidence_quality']}/10")
                    print(f"    ç½®ä¿¡åº¦: {validation['confidence_level']}")

                    # ä¿å­˜ç¼“å­˜
                    self._save_cache(cache_key, result)

                else:
                    print(f"  âš ï¸ æ®µ{segment_number} éªŒè¯å¤±è´¥: {', '.join(validation['issues'])}")
                    result['success'] = False
                    result['error'] = '; '.join(validation['issues'])

            else:
                print(f"  âŒ æ®µ{segment_number} åˆ†æå¤±è´¥: {result.get('error', 'Unknown error')}")

            return result

        except Exception as e:
            print(f"  ğŸ’¥ æ®µ{segment_number} å¼‚å¸¸: {e}")
            return {
                'success': False,
                'segment_number': segment_number,
                'model': model,
                'error': str(e)
            }

    def _parse_response(self, response_content: str, segment_number: int) -> Dict:
        """è§£æAPIå“åº”"""
        try:
            # æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', response_content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å°è¯•ç›´æ¥è§£æ
                json_str = response_content

            # æ¸…ç†JSON
            json_str = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', json_str)
            json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)

            result = json.loads(json_str)
            result['segment_number'] = segment_number
            result['raw_response_length'] = len(response_content)

            return result

        except Exception as e:
            return {
                'success': False,
                'segment_number': segment_number,
                'error': f'JSONè§£æå¤±è´¥: {str(e)}',
                'raw_response_length': len(response_content)
            }

    def analyze_file(self, file_path: Path, output_dir: Path) -> Dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        start_time = time.time()
        print(f"\nğŸš€ å¼€å§‹5é¢˜åˆ†æ®µåˆ†æ: {file_path.name}")

        try:
            # åŠ è½½é—®é¢˜
            questions = self._load_questions_from_file(file_path)
            if not questions:
                return {
                    'success': False,
                    'file': str(file_path),
                    'error': 'æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆé—®é¢˜'
                }

            # åˆ†æ®µ
            segments = []
            for i in range(0, len(questions), self.segment_size):
                segment = questions[i:i+self.segment_size]
                if len(segment) == self.segment_size:
                    segments.append(segment)

            if not segments:
                return {
                    'success': False,
                    'file': str(file_path),
                    'error': f'é—®é¢˜æ•°é‡({len(questions)})ä¸æ˜¯{self.segment_size}çš„å€æ•°'
                }

            print(f"ğŸ“Š {len(questions)}é¢˜åˆ†ä¸º{len(segments)}æ®µ")

            # åˆ†æå„æ®µ
            all_results = {}
            for model in self.models:
                print(f"\nğŸ¤– å¤„ç†æ¨¡å‹: {model}")
                model_results = []

                for segment in segments:
                    segment_num = len(model_results) + 1
                    result = self._analyze_segment(segment, segment_num, model)
                    model_results.append(result)

                    # æ®µé—´å»¶è¿Ÿ
                    if segment_num < len(segments):
                        time.sleep(self.delay_between_segments)

                all_results[model] = model_results

            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            final_scores = self._calculate_final_scores(all_results)
            mbti_type = self._calculate_mbti(final_scores)

            # è®¡ç®—æ¨¡å‹ä¸€è‡´æ€§
            model_consistency = self._calculate_model_consistency(all_results)

            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time

            result = {
                'success': True,
                'file': str(file_path),
                'processing_time': processing_time,
                'analysis_info': {
                    'segment_size': self.segment_size,
                    'total_segments': len(segments),
                    'models_count': len(self.models),
                    'total_questions': len(questions)
                },
                'final_scores': final_scores,
                'mbti_type': mbti_type,
                'model_consistency': model_consistency,
                'model_results': all_results,
                'validation_stats': self._get_validation_summary()
            }

            # ä¿å­˜ç»“æœ
            output_file = output_dir / f"{file_path.stem}_enhanced_5segment.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"\nâœ… 5é¢˜åˆ†æ®µåˆ†æå®Œæˆ: {output_file}")
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {processing_time:.1f} ç§’")
            print(f"ğŸ¯ æœ€ç»ˆè¯„åˆ†: {final_scores}")
            print(f"ğŸ§  MBTI: {mbti_type}")

            return result

        except Exception as e:
            return {
                'success': False,
                'file': str(file_path),
                'error': str(e),
                'processing_time': time.time() - start_time
            }

    def _calculate_final_scores(self, all_results: Dict) -> Dict:
        """è®¡ç®—æœ€ç»ˆè¯„åˆ†"""
        trait_scores = {}

        for model, results in all_results.items():
            for result in results:
                if result.get('success', False) and 'scores' in result:
                    for trait, score in result['scores'].items():
                        if trait not in trait_scores:
                            trait_scores[trait] = []
                        trait_scores[trait].append(score)

        # è®¡ç®—å¹³å‡åˆ†
        final_scores = {}
        for trait, scores in trait_scores.items():
            if scores:
                final_scores[trait] = round(statistics.mean(scores))
            else:
                final_scores[trait] = 3

        return final_scores

    def _calculate_mbti(self, scores: Dict) -> str:
        """è®¡ç®—MBTIç±»å‹"""
        try:
            e_i = "E" if scores.get('extraversion', 3) > 3 else "I"
            s_n = "S" if scores.get('openness_to_experience', 3) < 4 else "N"
            t_f = "T" if scores.get('agreeableness', 3) < 3 else "F"
            j_p = "J" if scores.get('conscientiousness', 3) > 3 else "P"

            return f"{e_i}{s_n}{t_f}{j_p}"
        except:
            return "UNKNOWN"

    def _calculate_model_consistency(self, all_results: Dict) -> Dict:
        """è®¡ç®—æ¨¡å‹ä¸€è‡´æ€§"""
        consistency = {}
        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']

        for trait in traits:
            model_scores = []

            for model, results in all_results.items():
                trait_avg = []
                for result in results:
                    if result.get('success', False) and 'scores' in result:
                        score = result['scores'].get(trait, 3)
                        trait_avg.append(score)

                if trait_avg:
                    model_scores.append(statistics.mean(trait_avg))

            if len(model_scores) >= 2:
                max_score = max(model_scores)
                min_score = min(model_scores)
                consistency[trait] = max(0, 100 - (max_score - min_score) * 20)
            else:
                consistency[trait] = 100

        return consistency

    def _get_validation_summary(self) -> Dict:
        """è·å–éªŒè¯ç»Ÿè®¡æ‘˜è¦"""
        stats = self.validation_stats

        if stats['total_segments'] == 0:
            return {'status': 'no_data'}

        success_rate = (stats['successful_segments'] / stats['total_segments']) * 100
        diverse_rate = (stats['diverse_score_count'] / max(1, stats['successful_segments'])) * 100
        avg_processing_time = statistics.mean(stats['processing_times']) if stats['processing_times'] else 0
        avg_evidence_quality = statistics.mean(stats['evidence_quality_scores']) if stats['evidence_quality_scores'] else 0

        return {
            'total_segments': stats['total_segments'],
            'successful_segments': stats['successful_segments'],
            'success_rate': success_rate,
            'diverse_score_rate': diverse_rate,
            'all_three_count': stats['all_three_count'],
            'avg_processing_time': avg_processing_time,
            'avg_evidence_quality': avg_evidence_quality,
            'credibility_score': self._calculate_credibility_score()
        }

    def _calculate_credibility_score(self) -> int:
        """è®¡ç®—æ•´ä½“å¯ä¿¡åº¦åˆ†æ•°ï¼ˆ0-100ï¼‰"""
        stats = self.validation_stats

        if stats['total_segments'] == 0:
            return 0

        success_rate = stats['successful_segments'] / stats['total_segments']
        diverse_rate = stats['diverse_score_count'] / max(1, stats['successful_segments'])
        avg_evidence = statistics.mean(stats['evidence_quality_scores']) if stats['evidence_quality_scores'] else 0

        # ç»¼åˆè¯„åˆ†
        credibility = (success_rate * 40 + diverse_rate * 40 + (avg_evidence / 10) * 20)
        return min(100, int(credibility))

def test_enhanced_5segment():
    """æµ‹è¯•å¢å¼ºç‰ˆ5é¢˜åˆ†æ®µåˆ†æå™¨"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºç‰ˆ5é¢˜åˆ†æ®µåˆ†æå™¨...")

    analyzer = Enhanced5SegmentAnalyzer(models=["qwen-max"])

    # æµ‹è¯•æ–‡ä»¶
    test_file = Path("results/results/asses_deepseek_r1_70b_agent_big_five_50_complete2_a10_e0_t0_0_09271.json")
    output_dir = Path("enhanced_5segment_results")
    output_dir.mkdir(exist_ok=True)

    if not test_file.exists():
        print(f"âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        return

    # æ‰§è¡Œåˆ†æ
    result = analyzer.analyze_file(test_file, output_dir)

    if result['success']:
        print(f"\nğŸ‰ æµ‹è¯•æˆåŠŸ!")
        print(f"ğŸ“Š å¯ä¿¡åº¦åˆ†æ•°: {result['validation_stats']['credibility_score']}/100")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {result['validation_stats']['success_rate']:.1f}%")
        print(f"ğŸ¯ è¯„åˆ†å¤šæ ·æ€§: {result['validation_stats']['diverse_score_rate']:.1f}%")
        print(f"ğŸ“ å¹³å‡è¯æ®è´¨é‡: {result['validation_stats']['avg_evidence_quality']:.1f}/10")

        # æ£€æŸ¥æ˜¯å¦æœ‰å…¨3åˆ†é—®é¢˜
        if result['validation_stats']['all_three_count'] > 0:
            print(f"âš ï¸ è­¦å‘Š: å‘ç° {result['validation_stats']['all_three_count']} ä¸ªå…¨3åˆ†æ®µ")
        else:
            print(f"âœ… æ²¡æœ‰å…¨3åˆ†æ®µé—®é¢˜")

    else:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    test_enhanced_5segment()