#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èšç„¦å¤šæ¨¡å‹å·®å¼‚éªŒè¯ - é‡ç‚¹éªŒè¯æœ€å…³é”®çš„å·®å¼‚é¢˜ç›®
åªéªŒè¯é¢˜25ã€é¢˜9ã€é¢˜1è¿™ä¸‰ä¸ªå·®å¼‚æœ€å¤§çš„é¢˜ç›®
"""

import sys
import os
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
import statistics

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['DASHSCOPE_API_KEY'] = 'sk-ded837735b3c44599a9bc138da561c27'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class FocusedMultiModelValidator:
    def __init__(self):
        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

        # ä¸‰ä¸ªæ¨¡å‹é…ç½®
        self.models = [
            {"name": "qwen-long", "description": "é€šä¹‰åƒé—®é•¿æ–‡æœ¬æ¨¡å‹"},
            {"name": "qwen-max", "description": "é€šä¹‰åƒé—®æœ€å¼ºæ¨¡å‹"},
            {"name": "qwen-turbo", "description": "é€šä¹‰åƒé—®å¿«é€Ÿæ¨¡å‹"}
        ]

        # é‡ç‚¹éªŒè¯çš„å·®å¼‚æœ€å¤§çš„3ä¸ªé¢˜ç›®
        self.critical_questions = [25, 9, 1]

    def load_test_data(self, data_file: str) -> Dict:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“‹ åŠ è½½æµ‹è¯„æ•°æ®: {data_file}")

        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            questions = {}
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for i, item in enumerate(data['assessment_results']):
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', '')
                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']

                            if question_text and answer_text:
                                questions[i + 1] = {  # 1-based indexing
                                    'question': question_text,
                                    'answer': answer_text,
                                    'question_index': i + 1,
                                    'original_segment_2': (i // 2) + 1,
                                    'original_segment_5': (i // 5) + 1
                                }

            print(f"  ğŸ“Š æˆåŠŸæå– {len(questions)} ä¸ªé—®é¢˜")
            return questions

        except Exception as e:
            print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return {}

    def create_focused_prompt(self, question: Dict, segment_size: int) -> str:
        """åˆ›å»ºèšç„¦åˆ†ææç¤º"""

        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚åˆ†æä»¥ä¸‹{segment_size}é¢˜åˆ†æ®µçš„é—®å·å›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ã€‚

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- 1åˆ†ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- 3åˆ†ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- 5åˆ†ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼**

é—®é¢˜ {question['question_index']}:
{question['question']}

å›ç­”:
{question['answer']}

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "success": true,
  "analysis_summary": "ç®€è¦åˆ†æè¯´æ˜å’Œå…³é”®è¯„åˆ†ä¾æ®",
  "scores": {{
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  }},
  "key_evidence": "å…·ä½“è¯„åˆ†çš„å…³é”®è¯æ®",
  "confidence": "high/medium/low"
}}
"""
        return prompt

    def analyze_with_model(self, model_config: Dict, question: Dict, segment_size: int) -> Dict:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æ"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )

            prompt = self.create_focused_prompt(question, segment_size)

            response = client.chat.completions.create(
                model=model_config['name'],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ï¼Œå¹¶æä¾›å…³é”®è¯æ®è¯´æ˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1500,
                temperature=0.1
            )

            content = response.choices[0].message.content

            # è§£æJSON
            try:
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                    result = json.loads(json_str)
                else:
                    result = json.loads(content)
            except json.JSONDecodeError:
                return {
                    'success': False,
                    'model': model_config['name'],
                    'error': 'JSONè§£æå¤±è´¥',
                    'raw_response': content[:300]
                }

            # éªŒè¯è¯„åˆ†æ ‡å‡†
            if 'scores' in result and result['scores']:
                invalid_scores = []
                for trait, score in result['scores'].items():
                    if score not in [1, 3, 5]:
                        invalid_scores.append(f"{trait}:{score}")
                        # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                        if score < 2:
                            result['scores'][trait] = 1
                        elif score > 4:
                            result['scores'][trait] = 5
                        else:
                            result['scores'][trait] = 3

                if invalid_scores:
                    print(f"      âš ï¸ ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")

            result['model'] = model_config['name']
            result['question_index'] = question['question_index']
            result['segment_size'] = segment_size

            return result

        except Exception as e:
            return {
                'success': False,
                'model': model_config['name'],
                'error': str(e),
                'question_index': question['question_index']
            }

    def run_focused_validation(self, data_file: str) -> Dict:
        """è¿è¡Œèšç„¦éªŒè¯"""
        print("ğŸš€ å¼€å§‹èšç„¦å¤šæ¨¡å‹å·®å¼‚éªŒè¯")
        print("=" * 60)
        print(f"ğŸ¯ éªŒè¯é¢˜ç›®: {self.critical_questions} (å·®å¼‚æœ€å¤§çš„3é¢˜)")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {[m['name'] for m in self.models]}")

        # åŠ è½½æ•°æ®
        questions = self.load_test_data(data_file)
        if not questions:
            return {'success': False, 'error': 'æ•°æ®åŠ è½½å¤±è´¥'}

        validation_results = {}

        # éªŒè¯æ¯ä¸ªå…³é”®é¢˜ç›®
        for question_index in self.critical_questions:
            if question_index not in questions:
                print(f"  âŒ é¢˜{question_index} æ•°æ®ä¸å­˜åœ¨")
                continue

            question = questions[question_index]
            print(f"\nğŸ“‹ éªŒè¯é¢˜ {question_index} (å·®å¼‚æœ€å¤§é¢˜ç›®ä¹‹ä¸€)")
            print(f"ğŸ“ é—®é¢˜: {question['question'][:100]}...")

            question_results = {
                'question_index': question_index,
                'question_preview': question['question'][:100] + "...",
                'answer_preview': question['answer'][:100] + "...",
                'models_2segment': {},
                'models_5segment': {}
            }

            # 2é¢˜åˆ†æ®µåˆ†æ
            print(f"  ğŸ” 2é¢˜åˆ†æ®µåˆ†æ...")
            for model_config in self.models:
                print(f"    æ¨¡å‹: {model_config['name']}...")
                result = self.analyze_with_model(model_config, question, 2)
                question_results['models_2segment'][model_config['name']] = result
                if result['success']:
                    print(f"      âœ… è¯„åˆ†: {result['scores']}")
                else:
                    print(f"      âŒ å¤±è´¥: {result.get('error', 'Unknown error')}")
                time.sleep(2)

            # 5é¢˜åˆ†æ®µåˆ†æ
            print(f"  ğŸ” 5é¢˜åˆ†æ®µåˆ†æ...")
            for model_config in self.models:
                print(f"    æ¨¡å‹: {model_config['name']}...")
                result = self.analyze_with_model(model_config, question, 5)
                question_results['models_5segment'][model_config['name']] = result
                if result['success']:
                    print(f"      âœ… è¯„åˆ†: {result['scores']}")
                else:
                    print(f"      âŒ å¤±è´¥: {result.get('error', 'Unknown error')}")
                time.sleep(2)

            validation_results[question_index] = question_results

            print(f"  âœ… é¢˜{question_index}éªŒè¯å®Œæˆ")

        # åˆ†æç»“æœ
        print(f"\nğŸ“Š éªŒè¯ç»“æœåˆ†æ:")
        print("=" * 50)

        for question_index, results in validation_results.items():
            print(f"\nğŸ“‹ é¢˜{question_index} æ¨¡å‹å¯¹æ¯”:")

            # æ”¶é›†2é¢˜åˆ†æ®µç»“æœ
            seg_2_scores = {}
            for model_name, result in results['models_2segment'].items():
                if result['success'] and 'scores' in result:
                    seg_2_scores[model_name] = result['scores']
                    evidence = result.get('key_evidence', 'æ— ')
                    print(f"   {model_name} (2é¢˜): {result['scores']}")
                    print(f"    è¯æ®: {evidence[:50]}...")

            # æ”¶é›†5é¢˜åˆ†æ®µç»“æœ
            seg_5_scores = {}
            for model_name, result in results['models_5segment'].items():
                if result['success'] and 'scores' in result:
                    seg_5_scores[model_name] = result['scores']
                    evidence = result.get('key_evidence', 'æ— ')
                    print(f"  {model_name} (5é¢˜): {result['scores']}")
                    print(f"    è¯æ®: {evidence[:50]}...")

            # è®¡ç®—æ¨¡å‹ä¸€è‡´æ€§
            print(f"\n  ğŸ” æ¨¡å‹ä¸€è‡´æ€§åˆ†æ:")
            self._analyze_model_consistency(seg_2_scores, seg_5_scores, question_index)

        return validation_results

    def _analyze_model_consistency(self, seg_2_scores: Dict, seg_5_scores: Dict, question_index: int):
        """åˆ†ææ¨¡å‹ä¸€è‡´æ€§"""
        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']

        print(f"    ğŸ“Š ç‰¹è´¨ä¸€è‡´æ€§:")

        for trait in traits:
            seg_2_values = []
            seg_5_values = []

            for model, scores in seg_2_scores.items():
                if trait in scores:
                    seg_2_values.append(scores[trait])

            for model, scores in seg_5_scores.items():
                if trait in scores:
                    seg_5_values.append(scores[trait])

            if seg_2_values and seg_5_values:
                avg_2 = statistics.mean(seg_2_values)
                avg_5 = statistics.mean(seg_5_values)
                diff = abs(avg_2 - avg_5)

                consistency = "âœ… ä¸€è‡´" if diff < 1 else "âš ï¸ æœ‰å·®å¼‚"
                print(f"      {trait}: 2é¢˜={avg_2:.1f}, 5é¢˜={avg_5:.1f}, å·®å¼‚={diff:.1f} {consistency}")

    def save_results(self, validation_results: Dict):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        output_file = f"focused_multi_model_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ éªŒè¯ç»“æœå·²ä¿å­˜: {output_file}")
        return output_file

def main():
    """ä¸»å‡½æ•°"""
    validator = FocusedMultiModelValidator()

    # é€‰æ‹©æµ‹è¯•æ–‡ä»¶
    data_file = "results/results/asses_deepseek_r1_70b_agent_big_five_50_complete2_a10_e0_t0_0_09271.json"

    print(f"ğŸ¯ é€‰æ‹©æµ‹è¯•æ–‡ä»¶:")
    print(f"  æ•°æ®æ–‡ä»¶: {data_file}")

    # æ‰§è¡Œèšç„¦éªŒè¯
    result = validator.run_focused_validation(data_file)

    if result['success']:
        output_file = validator.save_results(result)
        print(f"\nğŸ‰ èšç„¦å¤šæ¨¡å‹éªŒè¯å®Œæˆ!")
        print(f"  ğŸ“„ ç»“æœæ–‡ä»¶: {output_file}")
    else:
        print(f"\nâŒ éªŒè¯å¤±è´¥: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()