# 实施计划：独立评估器分段评分系统

## 概述

本实施计划描述了一个新的评估系统，其中独立评估器只对原始测评报告进行分段评分，给出分数和依据，不进行后续的大五人格和MBTI分析。评分结果将用于验证信度，只有在满足信度要求后，才进行后续分析。

## 核心原则

1. **分段评分**：每个原始测评报告会被分成5题一段，每段独立评分
2. **独立评估**：每个评估器独立工作，不参考其他评估器的评分
3. **评分依据**：每个评分都必须包含具体依据和说明
4. **信度验证**：仅在评分一致且满足信度要求后，才进行后续分析
5. **大上下文模型**：使用大上下文长度模型，能够处理完整的原始测评报告

## 评分流程

### 阶段1：初始评分（三评估器并行）

1. **选择三个不同的评估器**：
   - Qwen-Long（长文本模型，上下文大）
   - OpenRouter提供的免费大模型（如DeepSeek R1、Llama 3.3 70B等）
   - 本地Ollama模型（如Llama 3.2 3B，作为备选）

2. **分段处理**：
   - 将原始测评报告分为每5题一段
   - 每段包含详细的问题、场景、提示和AI响应
   - 每个评估器独立对每段进行评分

3. **评分标准**：
   - 每题评分1-5分，严格按照评估标准执行
   - 评分需包含具体依据和说明
   - 每个Big Five维度独立评分

### 阶段2：一致性检验

1. **逐题对比**：对比三个评估器对每道题的评分
2. **分歧识别**：识别评分差异超过阈值的题目
3. **一致性计算**：
   - 信度系数计算
   - 评分一致性百分比

### 阶段3：分歧处理机制

1. **分歧阈值**：如果评分差异超过1分，则视为分歧
2. **增加评估器**：对分歧大的题目增加2个额外评估器
   - 新增评估器：Moonshot Kimi K2 + Google Gemma 3
3. **多数一致原则**：去除最高分和最低分后，其余评分形成多数一致
4. **迭代处理**：如果仍然存在分歧，继续增加评估器直到达成一致
5. **多轮评估机制**：
   - 第一轮：3个评估器
   - 第二轮：对分歧题目增加2个评估器（共5个）
   - 第三轮：对仍然分歧的题目再增加2个评估器（共7个）
   - 依此类推，直到所有题目达到信度要求
6. **动态阈值调整**：根据评估器数量调整一致性判定标准
7. **分歧追踪**：记录每题的分歧历史和最终解决方式

### 阶段4：信度验证

1. **整体一致性**：计算整体评分一致性
2. **信度阈值**：要求一致性≥80%才通过验证
3. **可信评分**：只有通过验证的评分才能用于后续分析

### 阶段5：后续分析（仅在信度满足后）

1. **大五人格分析**：基于可信评分计算Big Five维度
2. **MBTI分析**：基于Big Five评分转换为MBTI类型
3. **综合报告**：生成详细的分析报告

## 评估器配置

### 云模型评估器

1. **OpenRouter接口**：
   - API密钥：`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`
   - Base URL：`https://openrouter.ai/api/v1`

2. **主评估器**（按优先级，全部为大上下文模型）：
   - `google/gemini-2.0-flash-exp:free` - Gemini 2.0 Flash（上下文1,048,576）- 优先使用
   - `deepseek/deepseek-r1:free` - DeepSeek R1免费版（上下文163,840）
   - `qwen/qwen3-235b-a22b:free` - Qwen3 235B（上下文131,072）
   - `mistralai/mistral-small-3.2-24b-instruct:free` - Mistral Small（上下文131,072）
   - `meta-llama/llama-3.3-70b-instruct:free` - Llama 3.3 70B（上下文65,536）- 备选

3. **备选评估器**：
   - `moonshotai/kimi-k2:free` - Moonshot Kimi K2（上下文32,768）- 仅在其他模型不可用时使用

### 本地模型评估器

1. **Ollama配置**：
   - `llama3.2:3b` - 作为备选和重试选项
   - `qwen3:8b` - 作为备选评估器

## 错误处理和重试机制

1. **模型调用失败**：
   - 自动切换到备选模型
   - 记录失败模型信息
   - 实现指数退避重试机制

2. **上下文不足**：
   - 自动选择更大上下文模型
   - 分段处理（如果必须）
   - 优先选择超大上下文模型如Gemini 2.0 Flash

3. **评估器真实调用验证**：
   - 每次调用后验证响应有效性（检查响应格式、内容相关性等）
   - 无效响应自动重试（最多3次）
   - 多次失败更换评估器
   - 实时监控API调用成功率

4. **响应质量验证**：
   - 检查响应是否符合预期JSON格式
   - 验证评分是否在合理范围（1-5分）
   - 确认评分依据是否充分
   - 对低质量响应进行重试或更换模型

5. **API密钥轮换**：
   - 支持多个API密钥轮换使用
   - 自动检测并切换到可用密钥
   - 避免单点故障

## 分析指标

### 评分一致性指标

1. **Cohen's Kappa系数**：衡量评估器间的一致性
2. **评分差异范围**：每题评分的最大差异
3. **平均绝对差值**：评分间平均差异程度

### 信度指标

1. **内部一致性信度**：Cronbach's Alpha系数
2. **评分者信度**：评估器间一致性
3. **稳定性系数**：重测信度

## 实施步骤

### 步骤1：开发分段评分模块
- 实现原始测评报告的分段功能
- 创建标准化的评分输入格式

### 步骤2：集成云模型评估器
- 集成OpenRouter API
- 实现多模型支持
- 添加错误处理机制

### 步骤3：实现一致性检验算法
- 开发评分对比算法
- 实现分歧识别和处理逻辑

### 步骤4：构建分歧处理机制
- 实现动态增加评估器逻辑
- 实现多数一致判定

### 步骤5：开发信度验证模块
- 实现信度系数计算
- 设置验证阈值

### 步骤6：实现后续分析功能
- 仅在信度通过后执行
- 生成最终报告

## 评估器选择策略

### 主评估器（初始3个）
1. `deepseek/deepseek-r1:free` - 高质量长文本处理能力
2. `meta-llama/llama-3.3-70b-instruct:free` - 高性能推理
3. `qwen/qwen3-235b-a22b:free` - 大参数量保证

### 备选评估器（分歧处理时添加）
4. `google/gemini-2.0-flash-exp:free` - 超大上下文
5. `mistralai/mistral-small-3.2-24b-instruct:free` - 高性能小模型
6. `moonshotai/kimi-k2:free` - 中文处理能力强

### 本地备选（调用失败时）
- `ollama/llama3.2:3b` - 本地模型，稳定可靠
- `ollama/qwen3:8b` - 本地模型，中文处理好

## 质量控制

1. **模型调用验证**：每次调用后验证响应有效性和质量
2. **评分合理性检查**：验证评分是否在合理范围内
3. **依据充分性**：确保每个评分都有充分的依据说明
4. **一致性监控**：实时监控评估器间的一致性

## 预期输出

1. **分段评分结果**：每段的详细评分和依据
2. **一致性报告**：评估器间的一致性分析
3. **分歧处理记录**：分歧题目和处理过程
4. **信度验证结果**：信度系数和验证状态
5. **最终分析报告**：仅在信度通过后生成

## 时间安排

- **第1-2周**：完成分段评分模块和云模型集成
- **第3周**：实现一致性检验和分歧处理机制
- **第4周**：完成信度验证和后续分析功能
- **第5周**：系统测试和优化

## 风险控制

1. **API限制**：监控API调用频率，避免超出限制
2. **模型可用性**：准备多个备选模型，确保系统稳定
3. **上下文管理**：优化输入格式，确保不超过模型限制
4. **数据完整性**：确保所有评分数据被正确保存和处理