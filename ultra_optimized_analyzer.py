#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¶…çº§ä¼˜åŒ–åˆ†æå™¨ - é¡¶çº§LLMç¨‹åºå‘˜ä¼˜åŒ–æ–¹æ¡ˆ
"""

import sys
import os
import json
import hashlib
import asyncio
import aiohttp
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
import time

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DASHSCOPE_API_KEY'] = 'sk-3f16ac9d87e34ca88bf3925c3651624f'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class UltraOptimizedAnalyzer:
    def __init__(self, models: List[str] = None, cache_dir: str = "smart_cache"):
        self.models = models or ["qwen-max", "deepseek-v3.2-exp", "Moonshot-Kimi-K2-Instruct"]
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'cache_hits': 0,
            'api_calls': 0,
            'total_segments': 0,
            'processing_time': 0
        }

    def _get_smart_cache_key(self, question: str, answer: str, model: str) -> str:
        """æ™ºèƒ½ç¼“å­˜é”® - åŸºäºé—®é¢˜å†…å®¹è€Œéåˆ†æ®µ"""
        # æ ‡å‡†åŒ–æ–‡æœ¬
        normalized = f"{question.strip().lower()}_{answer.strip().lower()}_{model}"
        return hashlib.sha256(normalized.encode()).hexdigest()

    def _get_batch_cache_key(self, questions_answers: List[Tuple[str, str]], model: str) -> str:
        """æ‰¹é‡ç¼“å­˜é”®"""
        content = f"{model}_{hash(str(questions_answers))}"
        return hashlib.md5(content.encode()).hexdigest()

    def _load_from_cache(self, cache_key: str) -> Optional[Dict]:
        """ä»ç¼“å­˜åŠ è½½ç»“æœ"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return None

    def _save_to_cache(self, cache_key: str, result: Dict):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    async def _call_api_async(self, prompt: str, model: str) -> str:
        """å¼‚æ­¥APIè°ƒç”¨"""
        self.stats['api_calls'] += 1

        # è¿™é‡Œéœ€è¦å®ç°å…·ä½“çš„å¼‚æ­¥APIè°ƒç”¨é€»è¾‘
        # ç”±äºåŸå§‹ä»£ç æ˜¯åŒæ­¥çš„ï¼Œè¿™é‡Œç”¨çº¿ç¨‹æ± æ¨¡æ‹Ÿ
        from enhanced_cloud_analyzer import EnhancedCloudAnalyzer
        analyzer = EnhancedCloudAnalyzer(model=model)

        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥APIè°ƒç”¨
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor(max_workers=1) as executor:
            result = await loop.run_in_executor(
                executor,
                analyzer._call_api,
                prompt
            )
        return result

    def _create_batch_prompt(self, segments: List[str]) -> str:
        """åˆ›å»ºæ‰¹é‡åˆ†ææç¤º"""
        prompt = """è¯·åŒæ—¶åˆ†æä»¥ä¸‹å¤šç»„é—®é¢˜ï¼Œè¿”å›JSONæ•°ç»„æ ¼å¼çš„ç»“æœï¼š

"""

        for i, segment in enumerate(segments, 1):
            prompt += f"\n=== ç»„ {i} ===\n{segment}\n"

        prompt += """

è¯·è¿”å›æ ¼å¼ï¼š
[
  {
    "segment_number": 1,
    "scores": {
      "openness_to_experience": 1-5,
      "conscientiousness": 1-5,
      "extraversion": 1-5,
      "agreeableness": 1-5,
      "neuroticism": 1-5
    },
    "evidence": {
      "openness_to_experience": ["è¯æ®1", "è¯æ®2"],
      ...
    }
  },
  ...
]

ç¡®ä¿æ¯ä¸ªç»„çš„è¯„åˆ†éƒ½åœ¨1-5ä¹‹é—´ã€‚"""

        return prompt

    async def _analyze_batch_segments(self, segments: List[str], model: str, batch_size: int = 3) -> List[Dict]:
        """æ‰¹é‡åˆ†æåˆ†æ®µ"""
        results = []

        # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹é‡ç¼“å­˜
        batch_cache_key = self._get_batch_cache_key(
            [(seg, "") for seg in segments], model
        )
        cached_batch = self._load_from_cache(batch_cache_key)
        if cached_batch:
            self.stats['cache_hits'] += len(segments)
            return cached_batch.get('results', [])

        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(segments), batch_size):
            batch = segments[i:i+batch_size]

            # åˆ›å»ºæ‰¹é‡æç¤º
            batch_prompt = self._create_batch_prompt(batch)

            try:
                # è°ƒç”¨API
                response = await self._call_api_async(batch_prompt, model)

                # è§£ææ‰¹é‡å“åº”
                batch_results = self._parse_batch_response(response, i + 1)
                results.extend(batch_results)

            except Exception as e:
                # å¦‚æœæ‰¹é‡å¤±è´¥ï¼Œé™çº§ä¸ºå•ä¸ªå¤„ç†
                print(f"æ‰¹é‡å¤„ç†å¤±è´¥ï¼Œé™çº§ä¸ºå•ä¸ªå¤„ç†: {e}")
                for j, segment in enumerate(batch):
                    single_result = await self._analyze_single_segment_async(segment, i + j + 1, model)
                    results.append(single_result)

        # ä¿å­˜æ‰¹é‡ç¼“å­˜
        self._save_to_cache(batch_cache_key, {'results': results})

        return results

    async def _analyze_single_segment_async(self, segment_text: str, segment_num: int, model: str) -> Dict:
        """å¼‚æ­¥å•ä¸ªåˆ†æ®µåˆ†æ"""
        # æ£€æŸ¥å•ä¸ªç¼“å­˜
        cache_key = self._get_smart_cache_key(segment_text, "", model)
        cached_result = self._load_from_cache(cache_key)

        if cached_result:
            self.stats['cache_hits'] += 1
            return cached_result

        # æ„å»ºæç¤º
        prompt = f"""è¯·åˆ†æä»¥ä¸‹é—®é¢˜å›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ï¼š

{segment_text}

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "segment_number": {segment_num},
  "scores": {{
    "openness_to_experience": 1-5,
    "conscientiousness": 1-5,
    "extraversion": 1-5,
    "agreeableness": 1-5,
    "neuroticism": 1-5
  }},
  "evidence": {{
    "openness_to_experience": ["å…·ä½“è¯æ®"],
    ...
  }}
}}"""

        try:
            response = await self._call_api_async(prompt, model)
            result = self._parse_segment_response(response, segment_num)

            # ä¿å­˜ç¼“å­˜
            self._save_to_cache(cache_key, result)

            return result

        except Exception as e:
            return {
                'success': False,
                'segment_number': segment_num,
                'error': str(e),
                'model': model
            }

    def _parse_batch_response(self, response: str, start_segment: int) -> List[Dict]:
        """è§£ææ‰¹é‡å“åº”"""
        try:
            # å°è¯•è§£æJSONæ•°ç»„
            import re
            json_match = re.search(r'\[.*\]', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                results = json.loads(json_str)

                # éªŒè¯å’Œä¿®å¤ç»“æœ
                valid_results = []
                for i, result in enumerate(results):
                    if isinstance(result, dict) and 'scores' in result:
                        result['segment_number'] = start_segment + i
                        result['success'] = True
                        valid_results.append(result)

                return valid_results

        except Exception as e:
            print(f"æ‰¹é‡å“åº”è§£æå¤±è´¥: {e}")

        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼Œè®©è°ƒç”¨æ–¹é™çº§å¤„ç†
        return []

    def _parse_segment_response(self, response: str, segment_num: int) -> Dict:
        """è§£æå•ä¸ªåˆ†æ®µå“åº”"""
        try:
            # ä½¿ç”¨åŸæœ‰çš„è§£æé€»è¾‘
            from enhanced_cloud_analyzer import EnhancedCloudAnalyzer
            analyzer = EnhancedCloudAnalyzer()
            return analyzer._parse_segment_response(response, segment_num)
        except Exception as e:
            return {
                'success': False,
                'segment_number': segment_num,
                'error': str(e)
            }

    def _extract_questions_smart(self, data: List[Dict]) -> List[Dict]:
        """æ™ºèƒ½æå–é—®é¢˜"""
        questions = []

        for item in data:
            if 'question' in item and 'answer' in item:
                questions.append({
                    'question': item['question'],
                    'answer': item['answer'],
                    'text_hash': hashlib.md5(
                        f"{item['question']}_{item['answer']}".encode()
                    ).hexdigest()
                })

        return questions

    def _deduplicate_questions(self, questions: List[Dict]) -> List[Dict]:
        """å»é™¤é‡å¤é—®é¢˜"""
        seen_hashes = set()
        unique_questions = []

        for q in questions:
            if q['text_hash'] not in seen_hashes:
                seen_hashes.add(q['text_hash'])
                unique_questions.append(q)

        print(f"ğŸ“Š é—®é¢˜å»é‡: {len(questions)} -> {len(unique_questions)}")
        return unique_questions

    async def analyze_file_ultra_optimized(self, file_path: Path, output_dir: Path) -> Dict:
        """è¶…çº§ä¼˜åŒ–æ–‡ä»¶åˆ†æ"""
        start_time = time.time()
        print(f"ğŸš€ è¶…çº§ä¼˜åŒ–åˆ†æ: {file_path.name}")

        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ™ºèƒ½æå–å’Œå»é‡é—®é¢˜
            questions = self._extract_questions_smart(data)
            unique_questions = self._deduplicate_questions(questions)

            if not unique_questions:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆé—®é¢˜',
                    'file': str(file_path)
                }

            # åˆ›å»ºåˆ†æ®µ
            segments = []
            for i in range(0, len(unique_questions), 2):
                q1 = unique_questions[i]
                q2 = unique_questions[i+1] if i+1 < len(unique_questions) else None

                segment_text = f"åˆ†æä»¥ä¸‹é—®é¢˜å’Œå›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ï¼š\n\n"
                segment_text += f"é—®é¢˜1: {q1['question']}\nå›ç­”: {q1['answer']}\n\n"

                if q2:
                    segment_text += f"é—®é¢˜2: {q2['question']}\nå›ç­”: {q2['answer']}\n\n"

                segments.append(segment_text)

            print(f"ğŸ“Š {len(unique_questions)} ä¸ªå»é‡é—®é¢˜ï¼Œåˆ†ä¸º {len(segments)} ä¸ªåˆ†æ®µ")

            # å¹¶å‘åˆ†ææ‰€æœ‰æ¨¡å‹
            all_results = {}

            for model in self.models:
                print(f"ğŸ¤– æ¨¡å‹ {model} æ‰¹é‡åˆ†æä¸­...")

                # æ‰¹é‡åˆ†æ
                model_results = await self._analyze_batch_segments(segments, model, batch_size=3)
                all_results[model] = model_results

                # æ˜¾ç¤ºè¿›åº¦
                success_count = len([r for r in model_results if r.get('success', False)])
                print(f"  âœ… {model}: {success_count}/{len(segments)} åˆ†æ®µæˆåŠŸ")

            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            final_scores = {}
            model_agreement = {}

            for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                trait_scores = []

                for model, results in all_results.items():
                    model_trait_scores = []
                    for result in results:
                        if result.get('success', False) and 'scores' in result:
                            score = result['scores'].get(trait, 3)
                            model_trait_scores.append(score)

                    if model_trait_scores:
                        avg_score = sum(model_trait_scores) / len(model_trait_scores)
                        trait_scores.append(avg_score)

                if trait_scores:
                    final_scores[trait] = round(sum(trait_scores) / len(trait_scores))
                    # è®¡ç®—æ¨¡å‹é—´ä¸€è‡´æ€§
                    consistency = 100 - (max(trait_scores) - min(trait_scores)) * 20
                    model_agreement[trait] = max(0, consistency)
                else:
                    final_scores[trait] = 3
                    model_agreement[trait] = 0

            # è®¡ç®—MBTI
            mbti_type = self._calculate_mbti(final_scores)

            # ç”Ÿæˆç»“æœ
            processing_time = time.time() - start_time
            self.stats['processing_time'] += processing_time
            self.stats['total_segments'] += len(segments) * len(self.models)

            result = {
                'success': True,
                'file': str(file_path),
                'processing_time': processing_time,
                'file_info': {
                    'filename': file_path.name,
                    'original_questions': len(questions),
                    'unique_questions': len(unique_questions),
                    'segments': len(segments),
                    'models': self.models
                },
                'final_scores': final_scores,
                'mbti_type': mbti_type,
                'model_agreement': model_agreement,
                'model_results': all_results,
                'performance_stats': {
                    'cache_hit_rate': self.stats['cache_hits'] / max(1, self.stats['total_segments']) * 100,
                    'api_calls_saved': self.stats['cache_hits'],
                    'total_api_calls': self.stats['api_calls'],
                    'avg_time_per_segment': processing_time / (len(segments) * len(self.models))
                }
            }

            # ä¿å­˜ç»“æœ
            output_file = output_dir / f"{file_path.stem}_ultra_optimized.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"âœ… è¶…çº§ä¼˜åŒ–å®Œæˆ: {output_file}")
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {processing_time:.1f} ç§’")
            print(f"ğŸ“Š ç¼“å­˜å‘½ä¸­ç‡: {result['performance_stats']['cache_hit_rate']:.1f}%")
            print(f"ğŸ¯ æœ€ç»ˆè¯„åˆ†: {final_scores}")
            print(f"ğŸ§  MBTI: {mbti_type}")

            return result

        except Exception as e:
            return {
                'success': False,
                'file': str(file_path),
                'error': str(e),
                'processing_time': time.time() - start_time
            }

    def _calculate_mbti(self, scores: Dict) -> str:
        """æ ¹æ®Big5è¯„åˆ†è®¡ç®—MBTI"""
        try:
            e_i = "E" if scores.get('extraversion', 3) > 3 else "I"
            s_n = "S" if scores.get('openness_to_experience', 3) < 4 else "N"
            t_f = "T" if scores.get('agreeableness', 3) < 3 else "F"
            j_p = "J" if scores.get('conscientiousness', 3) > 3 else "P"

            return f"{e_i}{s_n}{t_f}{j_p}"
        except:
            return "UNKNOWN"

async def test_ultra_optimized():
    print("ğŸš€ æµ‹è¯•è¶…çº§ä¼˜åŒ–åˆ†æå™¨...")

    try:
        analyzer = UltraOptimizedAnalyzer()

        # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
        results_dir = Path("results/results")
        json_files = list(results_dir.glob("*.json"))

        if not json_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
            return

        # é€‰æ‹©æµ‹è¯•æ–‡ä»¶
        test_file = json_files[10] if len(json_files) > 10 else json_files[0]
        print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file.name}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("ultra_optimized_results")
        output_dir.mkdir(exist_ok=True)

        # æ‰§è¡Œè¶…çº§ä¼˜åŒ–åˆ†æ
        result = await analyzer.analyze_file_ultra_optimized(test_file, output_dir)

        if result['success']:
            print(f"âœ… è¶…çº§ä¼˜åŒ–æµ‹è¯•æˆåŠŸ!")
            stats = result['performance_stats']
            print(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.1f} ç§’")
            print(f"   ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1f}%")
            print(f"   APIè°ƒç”¨èŠ‚çœ: {stats['api_calls_saved']} æ¬¡")
            print(f"   å¹³å‡æ¯æ®µ: {stats['avg_time_per_segment']:.1f} ç§’")
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {result['error']}")

        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        print(f"\nğŸ“Š æ€»ä½“æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æ€»ç¼“å­˜å‘½ä¸­: {analyzer.stats['cache_hits']}")
        print(f"   æ€»APIè°ƒç”¨: {analyzer.stats['api_calls']}")
        print(f"   æ€»åˆ†æ®µæ•°: {analyzer.stats['total_segments']}")

    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_ultra_optimized())