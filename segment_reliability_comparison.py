#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
2é¢˜åˆ†æ®µ vs 5é¢˜åˆ†æ®µä¿¡åº¦å¯¹æ¯”æµ‹è¯•
å¯¹åŒä¸€ä¸ªæµ‹è¯„æŠ¥å‘Šåˆ†åˆ«ç”¨ä¸¤ç§åˆ†æ®µæ–¹æ¡ˆåˆ†æï¼Œæ¯”è¾ƒè¯„åˆ†ä¸€è‡´æ€§
"""

import sys
import os
import json
import time
import glob
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
import statistics

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['DASHSCOPE_API_KEY'] = 'sk-ded837735b3c44599a9bc138da561c27'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class SegmentReliabilityComparator:
    def __init__(self, model: str = "qwen-long"):
        self.model = model
        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

    def _create_2segment_prompt(self, segment: List[Dict], segment_number: int, total_segments: int) -> str:
        """åˆ›å»º2é¢˜åˆ†æ®µåˆ†ææç¤º"""
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚åˆ†æä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ã€‚

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- 1åˆ†ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- 3åˆ†ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- 5åˆ†ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼**

ç¬¬{segment_number}æ®µé—®å·å†…å®¹ï¼ˆ{len(segment)}é¢˜/å…±{total_segments}æ®µï¼‰ï¼š
"""

        for i, item in enumerate(segment, 1):
            prompt += f"""
é—®é¢˜ {i}:
{item['question']}

å›ç­” {i}:
{item['answer']}

---
"""

        prompt += """
è¯·è¿”å›JSONæ ¼å¼ï¼š
{
  "success": true,
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  }
}
"""
        return prompt

    def _create_5segment_prompt(self, segment: List[Dict], segment_number: int, total_segments: int) -> str:
        """åˆ›å»º5é¢˜åˆ†æ®µåˆ†ææç¤º"""
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚åˆ†æä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ã€‚

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- 1åˆ†ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- 3åˆ†ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- 5åˆ†ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼**

ç¬¬{segment_number}æ®µé—®å·å†…å®¹ï¼ˆ{len(segment)}é¢˜/å…±{total_segments}æ®µï¼‰ï¼š
"""

        for i, item in enumerate(segment, 1):
            prompt += f"""
é—®é¢˜ {i}:
{item['question']}

å›ç­” {i}:
{item['answer']}

---
"""

        prompt += """
è¯·è¿”å›JSONæ ¼å¼ï¼š
{
  "success": true,
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  }
}
"""
        return prompt

    def _analyze_segment(self, segment: List[Dict], prompt_func: callable, segment_number: int, total_segments: int) -> Dict:
        """åˆ†æå•ä¸ªåˆ†æ®µ"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )

            prompt = prompt_func(segment, segment_number, total_segments)

            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.1
            )

            content = response.choices[0].message.content

            # è§£æJSON
            try:
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                    result = json.loads(json_str)
                else:
                    result = json.loads(content)
            except json.JSONDecodeError:
                return {
                    'success': False,
                    'error': 'JSONè§£æå¤±è´¥',
                    'raw_response': content[:200]
                }

            # éªŒè¯è¯„åˆ†æ ‡å‡†
            if 'scores' in result:
                invalid_scores = []
                for trait, score in result['scores'].items():
                    if score not in [1, 3, 5]:
                        invalid_scores.append(f"{trait}:{score}")
                        # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                        if score < 2:
                            result['scores'][trait] = 1
                        elif score > 4:
                            result['scores'][trait] = 5
                        else:
                            result['scores'][trait] = 3

                if invalid_scores:
                    print(f"    âš ï¸ å‘ç°å¹¶ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")

            result['segment_number'] = segment_number
            result['model'] = self.model

            return result

        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'segment_number': segment_number
            }

    def _analyze_with_segmentation(self, questions: List[Dict], segment_size: int) -> Dict:
        """ç”¨æŒ‡å®šåˆ†æ®µå¤§å°åˆ†æ"""
        print(f"  ğŸ” ä½¿ç”¨{segment_size}é¢˜åˆ†æ®µåˆ†æ...")

        # åˆ†æ®µå¤„ç†
        segments = []
        for i in range(0, len(questions), segment_size):
            segment = questions[i:i+segment_size]
            if len(segment) == segment_size:
                segments.append(segment)

        total_segments = len(segments)
        print(f"  ğŸ“Š åˆ†æˆ{total_segments}ä¸ª{segment_size}é¢˜åˆ†æ®µ")

        # é€‰æ‹©æç¤ºå‡½æ•°
        prompt_func = self._create_2segment_prompt if segment_size == 2 else self._create_5segment_prompt

        # åˆ†ææ¯ä¸ªåˆ†æ®µ
        segment_results = []
        for i, segment in enumerate(segments, 1):
            print(f"    åˆ†æåˆ†æ®µ{i}...")
            result = self._analyze_segment(segment, prompt_func, i, total_segments)

            if result['success']:
                segment_results.append(result)
                print(f"      âœ… è¯„åˆ†: {result['scores']}")
            else:
                print(f"      âŒ å¤±è´¥: {result.get('error', 'Unknown error')}")

            time.sleep(2)  # APIé™åˆ¶

        if segment_results:
            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            final_scores = {}
            for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                all_scores = [result['scores'][trait] for result in segment_results]
                final_scores[trait] = int(statistics.median(all_scores))

            return {
                'success': True,
                'segment_size': segment_size,
                'total_segments': total_segments,
                'segment_results': segment_results,
                'final_scores': final_scores
            }
        else:
            return {
                'success': False,
                'segment_size': segment_size,
                'error': 'æ²¡æœ‰æˆåŠŸçš„åˆ†æ®µç»“æœ'
            }

    def _calculate_score_consistency(self, scores_2segment: Dict, scores_5segment: Dict) -> Dict:
        """è®¡ç®—è¯„åˆ†ä¸€è‡´æ€§"""
        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']

        consistency_results = {}
        exact_matches = 0
        close_matches = 0  # ç›¸å·®ä¸è¶…è¿‡2åˆ†

        for trait in traits:
            score_2 = scores_2segment.get(trait, 0)
            score_5 = scores_5segment.get(trait, 0)

            exact_match = score_2 == score_5
            close_match = abs(score_2 - score_5) <= 2

            consistency_results[trait] = {
                'score_2segment': score_2,
                'score_5segment': score_5,
                'difference': abs(score_2 - score_5),
                'exact_match': exact_match,
                'close_match': close_match
            }

            if exact_match:
                exact_matches += 1
            if close_match:
                close_matches += 1

        # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
        total_traits = len(traits)
        exact_match_rate = (exact_matches / total_traits) * 100
        close_match_rate = (close_matches / total_traits) * 100

        return {
            'trait_details': consistency_results,
            'exact_matches': exact_matches,
            'close_matches': close_matches,
            'total_traits': total_traits,
            'exact_match_rate': exact_match_rate,
            'close_match_rate': close_match_rate,
            'consistency_score': (exact_match_rate * 0.7 + close_match_rate * 0.3)
        }

    def analyze_file_comparison(self, file_path: str) -> Dict:
        """å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡Œ2é¢˜vs5é¢˜åˆ†æ®µå¯¹æ¯”åˆ†æ"""
        print(f"\nğŸ“Š å¯¹æ¯”åˆ†ææ–‡ä»¶: {Path(file_path).name}")
        print("=" * 60)

        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–é—®é¢˜
            questions = []
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for item in data['assessment_results'][:20]:  # å–å‰20é¢˜ï¼Œä¿è¯å¤Ÿç”¨
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', '')
                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']

                            if question_text and answer_text:
                                questions.append({
                                    'question': question_text,
                                    'answer': answer_text
                                })

            if len(questions) < 10:
                print(f"âŒ é—®é¢˜æ•°é‡ä¸è¶³ï¼š{len(questions)}")
                return {'success': False, 'error': 'é—®é¢˜æ•°é‡ä¸è¶³'}

            print(f"ğŸ“‹ æå–äº† {len(questions)} ä¸ªé—®é¢˜")

            # 2é¢˜åˆ†æ®µåˆ†æ
            print(f"\nğŸ”¬ 2é¢˜åˆ†æ®µåˆ†æ:")
            result_2segment = self._analyze_with_segmentation(questions[:10], 2)  # ç”¨å‰10é¢˜

            # 5é¢˜åˆ†æ®µåˆ†æ
            print(f"\nğŸ”¬ 5é¢˜åˆ†æ®µåˆ†æ:")
            result_5segment = self._analyze_with_segmentation(questions[:10], 5)  # ç”¨å‰10é¢˜

            if result_2segment['success'] and result_5segment['success']:
                # è®¡ç®—ä¸€è‡´æ€§
                consistency = self._calculate_score_consistency(
                    result_2segment['final_scores'],
                    result_5segment['final_scores']
                )

                print(f"\nğŸ¯ ä¸€è‡´æ€§åˆ†æç»“æœ:")
                print(f"  âœ… å®Œå…¨åŒ¹é…: {consistency['exact_matches']}/{consistency['total_traits']} ({consistency['exact_match_rate']:.1f}%)")
                print(f"  âœ… æ¥è¿‘åŒ¹é…: {consistency['close_matches']}/{consistency['total_traits']} ({consistency['close_match_rate']:.1f}%)")
                print(f"  ğŸ“Š ä¸€è‡´æ€§åˆ†æ•°: {consistency['consistency_score']:.1f}/100")

                print(f"\nğŸ“ˆ è¯¦ç»†å¯¹æ¯”:")
                for trait, detail in consistency['trait_details'].items():
                    print(f"  {trait}: 2é¢˜={detail['score_2segment']}, 5é¢˜={detail['score_5segment']}, å·®å¼‚={detail['difference']}")

                # ä¿å­˜å¯¹æ¯”ç»“æœ
                comparison_result = {
                    "file_info": {
                        "filename": Path(file_path).name,
                        "total_questions": len(questions),
                        "analysis_date": datetime.now().isoformat(),
                        "model_used": self.model
                    },
                    "analysis_2segment": result_2segment,
                    "analysis_5segment": result_5segment,
                    "consistency_analysis": consistency,
                    "reliability_assessment": {
                        "high_reliability": consistency['consistency_score'] >= 80,
                        "medium_reliability": consistency['consistency_score'] >= 60,
                        "recommendation": "é«˜ä¿¡åº¦" if consistency['consistency_score'] >= 80 else "ä¸­ç­‰ä¿¡åº¦" if consistency['consistency_score'] >= 60 else "ä½ä¿¡åº¦"
                    }
                }

                output_filename = f"{Path(file_path).stem}_segment_comparison.json"
                output_path = os.path.join("segment_comparison_results", output_filename)

                os.makedirs("segment_comparison_results", exist_ok=True)
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(comparison_result, f, ensure_ascii=False, indent=2)

                print(f"\nğŸ’¾ å¯¹æ¯”ç»“æœå·²ä¿å­˜: {output_filename}")

                return {
                    'success': True,
                    'file_path': file_path,
                    'consistency_score': consistency['consistency_score'],
                    'exact_match_rate': consistency['exact_match_rate'],
                    'comparison_result': comparison_result
                }
            else:
                print(f"âŒ å…¶ä¸­ä¸€ç§åˆ†æ®µåˆ†æå¤±è´¥")
                return {
                    'success': False,
                    'error': 'åˆ†æ®µåˆ†æå¤±è´¥',
                    'result_2segment': result_2segment,
                    'result_5segment': result_5segment
                }

        except Exception as e:
            print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def batch_comparison_test(self, input_dir: str = "results/results", max_files: int = 5):
        """æ‰¹é‡å¯¹æ¯”æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹2é¢˜vs5é¢˜åˆ†æ®µä¿¡åº¦å¯¹æ¯”æµ‹è¯•")
        print("=" * 60)

        # æŸ¥æ‰¾æ–‡ä»¶
        files = glob.glob(os.path.join(input_dir, "*.json"))
        files = files[:max_files]

        print(f"ğŸ“Š é€‰æ‹© {len(files)} ä¸ªæ–‡ä»¶è¿›è¡Œå¯¹æ¯”æµ‹è¯•")

        if not files:
            print("âŒ æœªæ‰¾åˆ°æ–‡ä»¶")
            return

        # æ‰¹é‡å¤„ç†
        comparison_results = []
        consistency_scores = []

        for i, file_path in enumerate(files, 1):
            print(f"\n[{i}/{len(files)}] å¼€å§‹å¯¹æ¯”åˆ†æ...")
            result = self.analyze_file_comparison(file_path)

            if result['success']:
                comparison_results.append(result)
                consistency_scores.append(result['consistency_score'])
                print(f"   âœ… ä¸€è‡´æ€§åˆ†æ•°: {result['consistency_score']:.1f}")
            else:
                print(f"   âŒ åˆ†æå¤±è´¥: {result.get('error', 'Unknown error')}")

        # æ€»ä½“ç»Ÿè®¡
        if comparison_results:
            avg_consistency = sum(consistency_scores) / len(consistency_scores)
            high_reliability_count = sum(1 for score in consistency_scores if score >= 80)
            medium_reliability_count = sum(1 for score in consistency_scores if 60 <= score < 80)

            print(f"\nğŸ¯ æ€»ä½“ä¿¡åº¦è¯„ä¼°:")
            print(f"  ğŸ“Š å¹³å‡ä¸€è‡´æ€§åˆ†æ•°: {avg_consistency:.1f}/100")
            print(f"  âœ… é«˜ä¿¡åº¦æ–‡ä»¶: {high_reliability_count}/{len(comparison_results)}")
            print(f"  âš ï¸ ä¸­ç­‰ä¿¡åº¦æ–‡ä»¶: {medium_reliability_count}/{len(comparison_results)}")
            print(f"  âŒ ä½ä¿¡åº¦æ–‡ä»¶: {len(comparison_results) - high_reliability_count - medium_reliability_count}/{len(comparison_results)}")

            # æœ€ç»ˆè¯„ä¼°
            if avg_consistency >= 80:
                overall_rating = "ä¼˜ç§€"
                recommendation = "âœ… 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¿¡åº¦ä¼˜ç§€ï¼Œå¯ä»¥æ›¿ä»£2é¢˜åˆ†æ®µ"
            elif avg_consistency >= 70:
                overall_rating = "è‰¯å¥½"
                recommendation = "âš ï¸ 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¿¡åº¦è‰¯å¥½ï¼Œå»ºè®®ç»“åˆä½¿ç”¨"
            elif avg_consistency >= 60:
                overall_rating = "ä¸­ç­‰"
                recommendation = "âš ï¸ 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¿¡åº¦ä¸­ç­‰ï¼Œéœ€è¦ä¼˜åŒ–"
            else:
                overall_rating = "éœ€è¦æ”¹è¿›"
                recommendation = "âŒ 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¿¡åº¦ä¸è¶³ï¼Œå»ºè®®ç»§ç»­ä½¿ç”¨2é¢˜åˆ†æ®µ"

            print(f"\nğŸ† æ€»ä½“è¯„çº§: {overall_rating}")
            print(f"ğŸ’¡ å»ºè®®: {recommendation}")

            # ä¿å­˜æ€»ä½“æŠ¥å‘Š
            overall_report = {
                "test_info": {
                    "test_type": "2é¢˜vs5é¢˜åˆ†æ®µä¿¡åº¦å¯¹æ¯”",
                    "test_date": datetime.now().isoformat(),
                    "model_used": self.model,
                    "files_tested": len(files),
                    "files_successful": len(comparison_results)
                },
                "consistency_stats": {
                    "average_consistency": avg_consistency,
                    "high_reliability_count": high_reliability_count,
                    "medium_reliability_count": medium_reliability_count,
                    "low_reliability_count": len(comparison_results) - high_reliability_count - medium_reliability_count
                },
                "overall_assessment": {
                    "rating": overall_rating,
                    "recommendation": recommendation,
                    "reliable": avg_consistency >= 70
                },
                "individual_results": comparison_results
            }

            with open("segment_comparison_overall_report.json", 'w', encoding='utf-8') as f:
                json.dump(overall_report, f, ensure_ascii=False, indent=2)

            print(f"\nğŸ“„ æ€»ä½“æŠ¥å‘Šå·²ä¿å­˜: segment_comparison_overall_report.json")

            return overall_report
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„å¯¹æ¯”åˆ†æç»“æœ")
            return None

def main():
    """ä¸»å‡½æ•°"""
    comparator = SegmentReliabilityComparator(model="qwen-long")
    comparator.batch_comparison_test(max_files=3)  # æµ‹è¯•3ä¸ªæ–‡ä»¶

if __name__ == "__main__":
    main()