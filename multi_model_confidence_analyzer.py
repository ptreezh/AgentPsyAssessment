#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡å‹ç½®ä¿¡åº¦åˆ†æå™¨
é€šè¿‡æ¯”è¾ƒå¤šä¸ªç‹¬ç«‹æ¨¡å‹çš„è¯„ä¼°ç»“æœæ¥è®¡ç®—ç½®ä¿¡åº¦
"""

import json
import sys
import time
from typing import Dict, List, Any, Tuple
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from enhanced_cloud_analyzer import EnhancedCloudAnalyzer

class MultiModelConfidenceAnalyzer:
    """å¤šæ¨¡å‹ç½®ä¿¡åº¦åˆ†æå™¨"""

    def __init__(self, models: List[str] = None, api_key: str = None):
        # é»˜è®¤ä½¿ç”¨å®é™…å¯ç”¨çš„æ¨¡å‹ï¼ˆæš‚æ—¶ç§»é™¤æœ‰é—®é¢˜çš„Claude APIï¼‰
        self.models = models or ["qwen-max", "deepseek-v3.2-exp", "Moonshot-Kimi-K2-Instruct"]
        self.api_key = api_key
        self.results = {}

    def analyze_with_multiple_models(self, input_file: Path, output_dir: Path) -> Dict:
        """ä½¿ç”¨å¤šä¸ªæ¨¡å‹åˆ†æåŒä¸€ä»½æµ‹è¯„æŠ¥å‘Š"""
        print(f"ğŸ¤– å¼€å§‹å¤šæ¨¡å‹åˆ†æ: {input_file.name}")
        print(f"ğŸ“Š ä½¿ç”¨æ¨¡å‹: {', '.join(self.models)}")

        # åˆ›å»ºæ¨¡å‹ç‰¹å®šçš„è¾“å‡ºç›®å½•
        model_output_dir = output_dir / "multi_model_results"
        model_output_dir.mkdir(parents=True, exist_ok=True)

        multi_model_results = {}

        # ä½¿ç”¨å¤šä¸ªæ¨¡å‹è¿›è¡Œåˆ†æ
        for model in self.models:
            print(f"\nğŸ” æ­£åœ¨ä½¿ç”¨æ¨¡å‹ {model} åˆ†æ...")

            try:
                analyzer = EnhancedCloudAnalyzer(
                    model=model,
                    api_key=self.api_key
                )

                if not analyzer.api_available:
                    print(f"âŒ æ¨¡å‹ {model} APIä¸å¯ç”¨ï¼Œè·³è¿‡")
                    continue

                # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•
                model_dir = model_output_dir / model
                model_dir.mkdir(exist_ok=True)

                # æ‰§è¡Œåˆ†æ
                result = analyzer.analyze_full_assessment(str(input_file), str(model_dir))

                if result['success']:
                    # è·å–æœ€ç»ˆè¯„åˆ†å’ŒMBTIç»“æœ
                    final_scores = result.get('final_scores', {})
                    mbti_result = result.get('mbti_result', {})

                    multi_model_results[model] = {
                        'success': True,
                        'big5_scores': {trait: data.get('final_score', 3) for trait, data in final_scores.items()},
                        'mbti_type': mbti_result.get('type', 'Unknown'),
                        'final_scores_detailed': final_scores,
                        'mbti_detailed': mbti_result,
                        'summary_file': result.get('summary_file', 'N/A'),
                        'evidence_file': result.get('evidence_file', 'N/A')
                    }

                    big5_str = ", ".join([f"{trait[0].upper()}:{score}" for trait, score in multi_model_results[model]['big5_scores'].items()])
                    print(f"âœ… {model} - Big5: {big5_str} - MBTI: {mbti_result['type']}")
                else:
                    print(f"âŒ {model} åˆ†æå¤±è´¥: {result.get('error', 'Unknown error')}")
                    multi_model_results[model] = {
                        'success': False,
                        'error': result.get('error', 'Unknown error')
                    }

            except Exception as e:
                print(f"ğŸ’¥ {model} åˆ†æå¼‚å¸¸: {e}")
                multi_model_results[model] = {
                    'success': False,
                    'error': str(e)
                }

        # è®¡ç®—å¤šæ¨¡å‹ç½®ä¿¡åº¦
        confidence_analysis = self.calculate_multi_model_confidence(multi_model_results)

        # ç”Ÿæˆå¤šæ¨¡å‹æ±‡æ€»æŠ¥å‘Š
        self.save_multi_model_report(
            input_file,
            multi_model_results,
            confidence_analysis,
            model_output_dir
        )

        return {
            'file': str(input_file),
            'multi_model_results': multi_model_results,
            'confidence_analysis': confidence_analysis,
            'success': len(multi_model_results) > 0
        }

    def calculate_multi_model_confidence(self, multi_model_results: Dict) -> Dict:
        """åŸºäºå¤šæ¨¡å‹æ¯”è¾ƒè®¡ç®—ç½®ä¿¡åº¦"""
        successful_models = [model for model, result in multi_model_results.items() if result['success']]

        if len(successful_models) < 2:
            return {
                'overall_confidence': 0.0,
                'big5_confidence': {},
                'mbti_confidence': {},
                'note': f'åªæœ‰ {len(successful_models)} ä¸ªæ¨¡å‹æˆåŠŸåˆ†æï¼Œæ— æ³•è®¡ç®—å¤šæ¨¡å‹ç½®ä¿¡åº¦'
            }

        print(f"\nğŸ“ˆ è®¡ç®— {len(successful_models)} ä¸ªæ¨¡å‹ä¹‹é—´çš„ç½®ä¿¡åº¦...")

        # æ”¶é›†æ‰€æœ‰æˆåŠŸæ¨¡å‹çš„è¯„åˆ†
        big5_scores_by_model = {}
        mbti_types_by_model = {}

        for model in successful_models:
            result = multi_model_results[model]
            big5_scores_by_model[model] = result['big5_scores']
            mbti_types_by_model[model] = result['mbti_type']

        # è®¡ç®—Big5æ¯ä¸ªç»´åº¦çš„ç½®ä¿¡åº¦
        big5_confidence = {}
        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']

        for trait in traits:
            scores = [big5_scores_by_model[model][trait] for model in successful_models]
            confidence = self.calculate_score_agreement(scores)
            big5_confidence[trait] = {
                'confidence_percent': confidence,
                'scores_by_model': {model: big5_scores_by_model[model][trait] for model in successful_models},
                'agreement_level': self.get_agreement_level(confidence)
            }

        # è®¡ç®—MBTIç½®ä¿¡åº¦
        mbti_confidence = self.calculate_mbti_agreement(mbti_types_by_model)

        # è®¡ç®—æ€»ä½“ç½®ä¿¡åº¦
        big5_confidences = [conf['confidence_percent'] for conf in big5_confidence.values()]
        overall_confidence = sum(big5_confidences) / len(big5_confidences)

        confidence_analysis = {
            'overall_confidence': round(overall_confidence, 1),
            'big5_confidence': big5_confidence,
            'mbti_confidence': mbti_confidence,
            'successful_models': successful_models,
            'total_models_attempted': len(self.models),
            'analysis_timestamp': datetime.now().isoformat()
        }

        # æ‰“å°ç½®ä¿¡åº¦åˆ†æç»“æœ
        print(f"ğŸ¯ æ€»ä½“ç½®ä¿¡åº¦: {confidence_analysis.get('overall_confidence', 0)}%")
        print(f"ğŸ“Š Big5å„ç»´åº¦ç½®ä¿¡åº¦:")
        for trait, conf in big5_confidence.items():
            trait_name = trait.replace('_', ' ').title()
            print(f"  {trait_name}: {conf.get('confidence_percent', 0)}% ({conf.get('agreement_level', 'Unknown')})")
        print(f"ğŸ§  MBTIç½®ä¿¡åº¦: {mbti_confidence.get('confidence_percent', 0)}% ({mbti_confidence.get('agreement_level', 'Unknown')})")

        return confidence_analysis

    def calculate_score_agreement(self, scores: List[int]) -> float:
        """è®¡ç®—è¯„åˆ†é—´çš„ä¸€è‡´æ€§ï¼ˆåŸºäºå®Œå…¨åŒ¹é…çš„æ¯”ä¾‹ï¼‰"""
        if len(scores) < 2:
            return 0.0

        # è®¡ç®—è¯„åˆ†åˆ†å¸ƒï¼Œæ”¯æŒå„ç§å¯èƒ½çš„è¯„åˆ†å€¼
        score_counts = {}
        for score in scores:
            if score not in score_counts:
                score_counts[score] = 0
            score_counts[score] += 1

        # æœ€å¸¸è§çš„è¯„åˆ†åŠå…¶å‡ºç°æ¬¡æ•°
        most_common_score = max(score_counts, key=score_counts.get)
        agreement_count = score_counts[most_common_score]

        # ç½®ä¿¡åº¦ = æœ€å¤šè¯„åˆ†çš„æ•°é‡ / æ€»è¯„åˆ†æ•°é‡
        confidence = (agreement_count / len(scores)) * 100

        return round(confidence, 1)

    def calculate_mbti_agreement(self, mbti_types: Dict[str, str]) -> Dict:
        """è®¡ç®—MBTIç±»å‹é—´çš„ä¸€è‡´æ€§"""
        if len(mbti_types) < 2:
            return {
                'confidence_percent': 0.0,
                'agreement_level': 'æ— æ³•è®¡ç®—',
                'types_by_model': mbti_types
            }

        # ç»Ÿè®¡MBTIç±»å‹åˆ†å¸ƒ
        type_counts = {}
        for model, mbti_type in mbti_types.items():
            if mbti_type not in type_counts:
                type_counts[mbti_type] = []
            type_counts[mbti_type].append(model)

        # æœ€å¸¸è§çš„MBTIç±»å‹
        most_common_type = max(type_counts, key=lambda x: len(type_counts[x]))
        agreement_count = len(type_counts[most_common_type])

        # ç½®ä¿¡åº¦è®¡ç®—
        confidence = (agreement_count / len(mbti_types)) * 100

        return {
            'confidence_percent': round(confidence, 1),
            'agreement_level': self.get_agreement_level(confidence),
            'most_common_type': most_common_type,
            'types_by_model': mbti_types,
            'type_distribution': {mbti_type: len(models) for mbti_type, models in type_counts.items()}
        }

    def get_agreement_level(self, confidence_percent: float) -> str:
        """æ ¹æ®ç½®ä¿¡åº¦ç™¾åˆ†æ¯”è¿”å›ä¸€è‡´æ€§çº§åˆ«"""
        if confidence_percent >= 80:
            return "é«˜åº¦ä¸€è‡´"
        elif confidence_percent >= 60:
            return "ä¸­ç­‰ä¸€è‡´"
        elif confidence_percent >= 40:
            return "ä½åº¦ä¸€è‡´"
        else:
            return "ä¸ä¸€è‡´"

    def save_multi_model_report(self, input_file: Path, multi_model_results: Dict,
                               confidence_analysis: Dict, output_dir: Path):
        """ä¿å­˜å¤šæ¨¡å‹åˆ†ææŠ¥å‘Š"""
        filename_base = input_file.stem

        # ä¿å­˜è¯¦ç»†çš„å¤šæ¨¡å‹åˆ†æç»“æœ
        report_data = {
            'analysis_info': {
                'file_analyzed': str(input_file),
                'filename': input_file.name,
                'analysis_timestamp': datetime.now().isoformat(),
                'models_used': self.models,
                'algorithm': 'multi_model_confidence_v1.0'
            },
            'multi_model_results': multi_model_results,
            'confidence_analysis': confidence_analysis
        }

        # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†æŠ¥å‘Š
        json_report = output_dir / f"{filename_base}_multi_model_confidence.json"
        with open(json_report, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        # ç”ŸæˆMarkdownæ‘˜è¦æŠ¥å‘Š
        self.generate_markdown_summary(report_data, output_dir, filename_base)

        print(f"ğŸ“‹ å¤šæ¨¡å‹ç½®ä¿¡åº¦æŠ¥å‘Šå·²ä¿å­˜:")
        print(f"   è¯¦ç»†æŠ¥å‘Š: {json_report}")
        print(f"   æ‘˜è¦æŠ¥å‘Š: {output_dir / f'{filename_base}_multi_model_summary.md'}")

    def generate_markdown_summary(self, report_data: Dict, output_dir: Path, filename_base: str):
        """ç”ŸæˆMarkdownæ ¼å¼çš„å¤šæ¨¡å‹åˆ†ææ‘˜è¦"""
        confidence = report_data['confidence_analysis']
        successful_models = confidence.get('successful_models', [])

        md_content = f"""# å¤šæ¨¡å‹ç½®ä¿¡åº¦åˆ†ææŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **åˆ†ææ–‡ä»¶:** {report_data['analysis_info']['filename']}
- **åˆ†ææ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ä½¿ç”¨æ¨¡å‹:** {', '.join(report_data['analysis_info']['models_used'])}
- **æˆåŠŸæ¨¡å‹:** {', '.join(successful_models)} ({len(successful_models)}/{len(self.models)})
- **ç®—æ³•ç‰ˆæœ¬:** multi_model_confidence_v1.0

## æ€»ä½“ç½®ä¿¡åº¦

**{confidence['overall_confidence']}%** - {self.get_agreement_level(confidence['overall_confidence'])}

## Big5å„ç»´åº¦ç½®ä¿¡åº¦

| ç»´åº¦ | ç½®ä¿¡åº¦ | ä¸€è‡´æ€§çº§åˆ« | å„æ¨¡å‹è¯„åˆ† |
|------|--------|------------|------------|
"""

        traits_display = {
            'openness_to_experience': 'å¼€æ”¾æ€§ (O)',
            'conscientiousness': 'å°½è´£æ€§ (C)',
            'extraversion': 'å¤–å‘æ€§ (E)',
            'agreeableness': 'å®œäººæ€§ (A)',
            'neuroticism': 'ç¥ç»è´¨ (N)'
        }

        for trait, conf in confidence['big5_confidence'].items():
            trait_display = traits_display.get(trait, trait)
            scores_str = ", ".join([f"{model}:{score}" for model, score in conf['scores_by_model'].items()])
            md_content += f"| {trait_display} | {conf['confidence_percent']}% | {conf['agreement_level']} | {scores_str} |\n"

        md_content += f"""
## MBTIç½®ä¿¡åº¦

**{confidence['mbti_confidence']['confidence_percent']}%** - {confidence['mbti_confidence']['agreement_level']}

- **æœ€å¸¸è§çš„MBTIç±»å‹:** {confidence['mbti_confidence']['most_common_type']}
- **å„æ¨¡å‹ç»“æœ:** {', '.join([f"{model}:{mbti}" for model, mbti in confidence['mbti_confidence']['types_by_model'].items()])}

## è¯¦ç»†è¯„åˆ†å¯¹æ¯”

| æ¨¡å‹ | O | C | E | A | N | MBTI |
|------|---|---|---|---|---|------|
"""

        for model in successful_models:
            if model in report_data['multi_model_results'] and report_data['multi_model_results'][model]['success']:
                result = report_data['multi_model_results'][model]
                scores = result['big5_scores']
                mbti = result['mbti_type']
                md_content += f"| {model} | {scores['openness_to_experience']} | {scores['conscientiousness']} | {scores['extraversion']} | {scores['agreeableness']} | {scores['neuroticism']} | {mbti} |\n"

        # ä¿å­˜Markdownæ–‡ä»¶
        md_file = output_dir / f"{filename_base}_multi_model_summary.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)

def main():
    """æµ‹è¯•å¤šæ¨¡å‹ç½®ä¿¡åº¦åˆ†æ"""
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python multi_model_confidence_analyzer.py <input_file> [output_dir]")
        return

    input_file = Path(sys.argv[1])
    if not input_file.exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return

    output_dir = Path(sys.argv[2]) if len(sys.argv) > 2 else Path("multi_model_confidence_results")

    # åˆ›å»ºå¤šæ¨¡å‹åˆ†æå™¨
    analyzer = MultiModelConfidenceAnalyzer()

    # æ‰§è¡Œåˆ†æ
    result = analyzer.analyze_with_multiple_models(input_file, output_dir)

    if result['success']:
        print(f"\nğŸ‰ å¤šæ¨¡å‹ç½®ä¿¡åº¦åˆ†æå®Œæˆ!")
        confidence = result['confidence_analysis']['overall_confidence']
        print(f"ğŸ“Š æ€»ä½“ç½®ä¿¡åº¦: {confidence}%")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    else:
        print(f"\nâŒ å¤šæ¨¡å‹åˆ†æå¤±è´¥")

if __name__ == "__main__":
    main()