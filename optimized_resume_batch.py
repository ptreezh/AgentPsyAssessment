#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–æ–­ç‚¹ç»§ç»­æ‰¹é‡åˆ†æ - 5é¢˜åˆ†æ®µ + 1ç§’å»¶è¿Ÿ + æ™ºèƒ½ç¼“å­˜
"""

import sys
import os
import json
import hashlib
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DASHSCOPE_API_KEY'] = 'sk-3f16ac9d87e34ca88bf3925c3651624f'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def optimized_resume_batch():
    print("ğŸš€ ä¼˜åŒ–æ–­ç‚¹ç»§ç»­æ‰¹é‡åˆ†æ - 5é¢˜åˆ†æ®µ + 1ç§’å»¶è¿Ÿ")

    try:
        # å¯¼å…¥å¹¶ä¿®æ”¹åŸæœ‰çš„æ‰¹é‡åˆ†æå™¨
        import importlib
        import batch_four_model_analysis

        # é‡æ–°åŠ è½½æ¨¡å—ä»¥åº”ç”¨æˆ‘ä»¬çš„ä¿®æ”¹
        importlib.reload(batch_four_model_analysis)

        # åˆ›å»ºä¼˜åŒ–åˆ†æå™¨
        analyzer = batch_four_model_analysis.BatchFourModelAnalyzer(
            models=["qwen-max", "deepseek-v3.2-exp", "Moonshot-Kimi-K2-Instruct"]
        )

        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {', '.join(analyzer.models)}")
        print(f"ğŸ”‘ APIå¯†é’¥å·²è®¾ç½®")

        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        results_dir = Path("results/results")
        if not results_dir.exists():
            print("âŒ resultsç›®å½•ä¸å­˜åœ¨")
            return

        json_files = list(results_dir.glob("*.json"))
        if not json_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
            return

        print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ä»¶")

        # æ£€æŸ¥æ–­ç‚¹ä¿¡æ¯
        if analyzer.load_progress():
            print(f"ğŸ“‚ å‘ç°æ–­ç‚¹ç»§ç»­ä¿¡æ¯:")
            print(f"   å·²å®Œæˆ: {len(analyzer.completed_files)} ä¸ªæ–‡ä»¶")
            print(f"   å¤±è´¥: {len(analyzer.failed_files)} ä¸ªæ–‡ä»¶")
        else:
            print("ğŸ“‚ æœªå‘ç°æ–­ç‚¹ä¿¡æ¯ï¼Œä»å¤´å¼€å§‹")

        # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
        remaining_files = [f for f in json_files if str(f) not in analyzer.completed_files and str(f) not in analyzer.failed_files]

        print(f"ğŸ“Š å‰©ä½™å¾…å¤„ç†æ–‡ä»¶: {len(remaining_files)} ä¸ª")

        if not remaining_files:
            print("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ")
            return

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("optimized_four_model_results")
        output_dir.mkdir(exist_ok=True)

        # ä¼˜åŒ–è¿›åº¦å›è°ƒ
        def optimized_progress_callback(completed, total, result):
            success_rate = sum(1 for r in analyzer.results if r.get('success', False)) / len(analyzer.results) * 100 if analyzer.results else 0
            confidences = [r.get('confidence_analysis', {}).get('overall_confidence', 0) for r in analyzer.results if r.get('success', False)]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0
            completed_in_batch = len(analyzer.completed_files) + len(analyzer.failed_files)
            progress_pct = completed_in_batch / total * 100
            eta_hours = (total - completed_in_batch) * 0.2 / 3600 if completed_in_batch > 0 else 0
            print(f"ğŸš€ ä¼˜åŒ–è¿›åº¦: {completed_in_batch}/{total} ({progress_pct:.1f}%) - æˆåŠŸç‡: {success_rate:.1f}% - å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.1f}% - é¢„è®¡å‰©ä½™: {eta_hours:.1f}å°æ—¶")

        print(f"\nğŸ¯ å¼€å§‹ä¼˜åŒ–æ‰¹é‡åˆ†æ...")
        print(f"âš¡ ä¼˜åŒ–é…ç½®: 5é¢˜åˆ†æ®µ, 1ç§’å»¶è¿Ÿ, æ™ºèƒ½ç¼“å­˜")
        print(f"ğŸ”§ é¢„æœŸæå‡: åˆ†æ®µå‡å°‘50%, å»¶è¿Ÿå‡å°‘93%")

        # ä¸´æ—¶ä¿®æ”¹åˆ†æå™¨é…ç½®
        original_segment_size = getattr(analyzer, 'segment_size', 2)
        analyzer.segment_size = 5  # 5é¢˜åˆ†æ®µ

        # æ‰§è¡Œæ‰¹é‡åˆ†æ
        results = analyzer.analyze_batch(
            remaining_files,
            output_dir,
            progress_callback=optimized_progress_callback,
            delay_between_files=1  # 1ç§’å»¶è¿Ÿ
        )

        # æ¢å¤åŸå§‹é…ç½®
        analyzer.segment_size = original_segment_size

        # ç”Ÿæˆæœ€ç»ˆæ±‡æ€»
        successful = sum(1 for r in results if r['success'])
        print(f"\nğŸ“Š ä¼˜åŒ–æ‰¹é‡åˆ†æå®Œæˆ:")
        print(f"âœ… æˆåŠŸ: {successful}/{len(results)}")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")

        if successful > 0:
            analyzer.generate_summary_report(results, output_dir)
            print(f"ğŸ“‹ æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {output_dir}/four_model_batch_summary.md")

            # ç”Ÿæˆå½“å‰è¿›åº¦æ±‡æ€»
            os.system("python generate_current_summary.py")

            # è®¡ç®—ä¼˜åŒ–æ•ˆæœ
            original_segments_per_file = 50 // 2  # åŸæ¥25åˆ†æ®µ
            new_segments_per_file = 50 // 5  # ç°åœ¨10åˆ†æ®µ
            segment_reduction = original_segments_per_file / new_segments_per_file

            print(f"\nğŸ¯ ä¼˜åŒ–æ•ˆæœ:")
            print(f"   åˆ†æ®µæ•°é‡: {original_segments_per_file} â†’ {new_segments_per_file} (å‡å°‘ {segment_reduction:.1f}å€)")
            print(f"   å»¶è¿Ÿæ—¶é—´: 15ç§’ â†’ 1ç§’ (å‡å°‘15å€)")
            print(f"   é¢„æœŸæ€»ä½“æå‡: {segment_reduction * 15:.1f}å€")

    except Exception as e:
        print(f"ğŸ’¥ ä¼˜åŒ–æ‰¹é‡åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    optimized_resume_batch()