#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆäº‘è¯„ä¼°å™¨åˆ†æ®µå¼å¿ƒç†è¯„ä¼°åˆ†æå™¨
æ”¯æŒå¤šä¸ªäº‘æœåŠ¡æä¾›å•†ï¼šQwenã€DeepSeekã€GLMã€Moonshotç­‰
"""

import json
import sys
import time
from typing import Dict, List, Any, Tuple
from pathlib import Path
from datetime import datetime
import openai
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class EnhancedCloudAnalyzer:
    """å¢å¼ºç‰ˆäº‘è¯„ä¼°å™¨ï¼Œæ”¯æŒå¤šä¸ªäº‘æœåŠ¡æä¾›å•†"""

    def __init__(self, model: str = "qwen-long", api_key: str = None, base_url: str = None, max_questions_per_segment: int = 2):
        self.model = model
        self.api_key = api_key
        self.base_url = base_url
        self.max_questions_per_segment = max_questions_per_segment

        # æ ¹æ®æ¨¡å‹ç¡®å®šAPIé…ç½®
        self._configure_api()

        # åˆå§‹åŒ–Big5è¯„åˆ†ç´¯ç§¯å™¨
        self.big_five_traits = {
            'openness_to_experience': {'scores': [], 'evidence': [], 'weight': 0},
            'conscientiousness': {'scores': [], 'evidence': [], 'weight': 0},
            'extraversion': {'scores': [], 'evidence': [], 'weight': 0},
            'agreeableness': {'scores': [], 'evidence': [], 'weight': 0},
            'neuroticism': {'scores': [], 'evidence': [], 'weight': 0}
        }

        self.analysis_log = []
        self.per_question_scores = []
        self.segment_results = []

        # åˆå§‹åŒ–äº‘è¯„ä¼°å™¨å®¢æˆ·ç«¯
        if self.api_key and self.base_url:
            if self.api_type == 'anthropic':
                # Anthropic APIé€šè¿‡OpenAIå…¼å®¹æ¥å£è®¿é—®
                self.client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url
                )
            else:
                # OpenAIå…¼å®¹æ¥å£
                self.client = openai.OpenAI(
                    api_key=self.api_key,
                    base_url=self.base_url
                )
        else:
            self.client = None

        # æ£€æŸ¥APIè¿æ¥
        self.api_available = self._check_api_connection()

    def _configure_api(self):
        """æ ¹æ®æ¨¡å‹é…ç½®APIå‚æ•°"""
        model_configs = {
            # DashScope APIä¸­å®é™…å¯ç”¨çš„æ¨¡å‹
            'qwen-long': {
                'api_key_env': 'DASHSCOPE_API_KEY',
                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
            },
            'qwen-max': {
                'api_key_env': 'DASHSCOPE_API_KEY',
                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
            },

            # DeepSeek Models (é€šè¿‡DashScope APIï¼Œå®é™…å¯ç”¨)
            'deepseek-v3.2-exp': {
                'api_key_env': 'DASHSCOPE_API_KEY',
                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
            },
            'deepseek-chat': {
                'api_key_env': 'DASHSCOPE_API_KEY',
                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
            },

            # Moonshot Models (é€šè¿‡DashScope APIï¼Œå®é™…å¯ç”¨)
            'Moonshot-Kimi-K2-Instruct': {
                'api_key_env': 'DASHSCOPE_API_KEY',
                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1'
            },

            # Anthropic Models (via BigModel)
            'claude-3.5-sonnet': {
                'api_key_env': 'ANTHROPIC_AUTH_TOKEN',
                'base_url_env': 'ANTHROPIC_BASE_URL',
                'api_type': 'anthropic'
            },
            'claude-3-opus': {
                'api_key_env': 'ANTHROPIC_AUTH_TOKEN',
                'base_url_env': 'ANTHROPIC_BASE_URL',
                'api_type': 'anthropic'
            },

            # GLM Models (åœ¨DashScope APIä¸­ä¸å¯ç”¨ï¼Œä¿ç•™é…ç½®)
            'GLM-4.5': {
                'api_key_env': 'DASHSCOPE_API_KEY',
                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1',
                'note': 'åœ¨DashScope APIä¸­ä¸å¯ç”¨'
            },
            'GLM-4.5-AIR': {
                'api_key_env': 'DASHSCOPE_API_KEY',
                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1',
                'note': 'åœ¨DashScope APIä¸­ä¸å¯ç”¨'
            },
            'glm4.5': {
                'api_key_env': 'DASHSCOPE_API_KEY',
                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1',
                'note': 'åœ¨DashScope APIä¸­ä¸å¯ç”¨'
            },
            'glm-4.5': {
                'api_key_env': 'DASHSCOPE_API_KEY',
                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1',
                'note': 'åœ¨DashScope APIä¸­ä¸å¯ç”¨'
            },
            'glm4': {
                'api_key_env': 'DASHSCOPE_API_KEY',
                'base_url': 'https://dashscope.aliyuncs.com/compatible-mode/v1',
                'note': 'åœ¨DashScope APIä¸­ä¸å¯ç”¨'
            }
        }

        config = model_configs.get(self.model, {})

        if not self.api_key:
            import os
            self.api_key = os.getenv(config.get('api_key_env', ''))

        if not self.base_url:
            if 'base_url_env' in config:
                self.base_url = os.getenv(config['base_url_env'], '')
            else:
                self.base_url = config.get('base_url', '')

        # è®¾ç½®APIç±»å‹
        self.api_type = config.get('api_type', 'openai')

        # è®¾ç½®é»˜è®¤å€¼ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if not self.api_key and self.api_type == 'openai':
            self.api_key = "sk-ffd03518254b495b8d27e723cd413fc1"

    def _check_api_connection(self) -> bool:
        """æ£€æŸ¥APIè¿æ¥æ˜¯å¦å¯ç”¨"""
        if not self.client:
            print(f"âŒ æœªé…ç½®å®¢æˆ·ç«¯: {self.model}")
            return False

        try:
            test_response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=10
            )
            print(f"âœ… APIè¿æ¥æˆåŠŸ: {self.model}")
            return True
        except Exception as e:
            print(f"âŒ APIè¿æ¥å¤±è´¥: {self.model} - {e}")
            return False

    def extract_questions(self, assessment_data: Dict) -> List[Dict]:
        """ä»è¯„ä¼°æ•°æ®ä¸­æå–é—®é¢˜åˆ—è¡¨"""
        if 'assessment_results' in assessment_data:
            assessment_results = assessment_data['assessment_results']

            if isinstance(assessment_results, list):
                if len(assessment_results) > 0 and isinstance(assessment_results[0], dict):
                    if 'question_data' in assessment_results[0]:
                        questions = []
                        for result in assessment_results:
                            if 'question_data' in result:
                                question_data = result['question_data'].copy()
                                if 'conversation_log' in result:
                                    for msg in result['conversation_log']:
                                        if msg.get('role') == 'assistant':
                                            question_data['agent_response'] = msg['content']
                                            break
                                questions.append(question_data)
                        return questions

        if 'questions' in assessment_data:
            return assessment_data['questions']

        print(f"è­¦å‘Š: æ— æ³•ä»æ•°æ®ä¸­æå–é—®é¢˜ï¼Œå¯ç”¨é”®: {list(assessment_data.keys())}")
        return []

    def create_segments(self, questions: List[Dict]) -> List[List[Dict]]:
        """åˆ›å»ºåˆ†æ®µï¼Œæ¯æ®µåŒ…å«æŒ‡å®šæ•°é‡çš„é—®é¢˜"""
        segments = []
        for i in range(0, len(questions), self.max_questions_per_segment):
            segment = questions[i:i + self.max_questions_per_segment]
            if segment:
                segments.append(segment)

        print(f"åˆ›å»º {len(segments)} ä¸ªåˆ†æ®µï¼Œæ¯æ®µæœ€å¤š {self.max_questions_per_segment} ä¸ªé—®é¢˜")
        return segments

    def validate_and_fix_score(self, score: Any) -> int:
        """éªŒè¯å¹¶ä¿®å¤è¯„åˆ†ï¼Œç¡®ä¿åªèƒ½æ˜¯1ã€3ã€5"""
        try:
            score = int(score)
            if score in [1, 3, 5]:
                return score
            elif score <= 2:
                return 1
            elif score <= 4:
                return 3
            else:
                return 5
        except (ValueError, TypeError):
            return 3  # é»˜è®¤å€¼

    def analyze_segment(self, segment: List[Dict], segment_number: int) -> Dict:
        """åˆ†æå•ä¸ªåˆ†æ®µï¼Œä½¿ç”¨ä¸¥æ ¼çš„1-3-5è¯„åˆ†æ ‡å‡†"""
        try:
            # æ„å»ºåˆ†æ®µåˆ†ææç¤º
            segment_prompt = self._build_segment_prompt(segment, segment_number)

            # è°ƒç”¨äº‘APIè¿›è¡Œåˆ†æ
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æBig5äººæ ¼ç‰¹è´¨ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§1-3-5è¯„åˆ†æ ‡å‡†è¿›è¡Œè¯„ä¼°ã€‚"},
                    {"role": "user", "content": segment_prompt}
                ],
                max_tokens=2000,
                temperature=0.1
            )

            analysis_content = response.choices[0].message.content

            # è§£æåˆ†æç»“æœ
            segment_result = self._parse_segment_response(analysis_content, segment_number)

            if segment_result['success']:
                # ç´¯ç§¯è¯„åˆ†åˆ°å¯¹åº”ç»´åº¦
                self._accumulate_scores(segment_result['scores'], segment_result['evidence'])
                print(f"âœ… æ®µ {segment_number} åˆ†ææˆåŠŸ")
                return segment_result
            else:
                print(f"âŒ æ®µ {segment_number} åˆ†æå¤±è´¥: {segment_result.get('error', 'Unknown error')}")
                return {'success': False, 'segment_number': segment_number, 'error': segment_result.get('error', 'Unknown error')}

        except Exception as e:
            print(f"ğŸ’¥ æ®µ {segment_number} åˆ†æå¼‚å¸¸: {e}")
            return {'success': False, 'segment_number': segment_number, 'error': str(e)}

    def _build_segment_prompt(self, segment: List[Dict], segment_number: int) -> str:
        """æ„å»ºåˆ†æ®µåˆ†ææç¤º"""
        prompt = f"""è¯·åˆ†æä»¥ä¸‹ç¬¬{segment_number}æ®µé—®å·å›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ã€‚

**è¯„åˆ†æ ‡å‡†ï¼ˆé‡è¦ï¼‰ï¼š**
- 1åˆ† = ä½è¡¨ç°
- 3åˆ† = ä¸­ç­‰è¡¨ç°
- 5åˆ† = é«˜è¡¨ç°
- åªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªåˆ†æ•°å€¼

**é—®å·å†…å®¹ï¼š**
"""

        for i, question in enumerate(segment, 1):
            prompt += f"\né—®é¢˜{i}: {question.get('question_text', 'N/A')}\n"
            prompt += f"å›ç­”: {question.get('user_response', question.get('response', 'N/A'))}\n"

            if 'agent_response' in question:
                prompt += f"æ™ºèƒ½ä½“å›å¤: {question['agent_response'][:200]}...\n"
            prompt += "---\n"

        prompt += """
**åˆ†æè¦æ±‚ï¼š**
1. é€é¢˜åˆ†æå›ç­”å†…å®¹
2. åŸºäºå›ç­”è¯„ä¼°æ¯ä¸ªBig5ç»´åº¦ï¼šå¼€æ”¾æ€§(O)ã€å°½è´£æ€§(C)ã€å¤–å‘æ€§(E)ã€å®œäººæ€§(A)ã€ç¥ç»è´¨(N)
3. ä¸¥æ ¼æŒ‰ç…§1-3-5è¯„åˆ†ï¼Œä¸ä½¿ç”¨å…¶ä»–æ•°å€¼
4. ä¸ºæ¯ä¸ªè¯„åˆ†æä¾›å…·ä½“çš„è¯„ä¼°ä¾æ®

**è¾“å‡ºæ ¼å¼ï¼ˆJSONï¼‰ï¼š**
```json
{
  "success": true,
  "segment_number": åˆ†æ®µç¼–å·,
  "scores": {
    "openness_to_experience": è¯„åˆ†(1/3/5),
    "conscientiousness": è¯„åˆ†(1/3/5),
    "extraversion": è¯„åˆ†(1/3/5),
    "agreeableness": è¯„åˆ†(1/3/5),
    "neuroticism": è¯„åˆ†(1/3/5)
  },
  "evidence": {
    "openness_to_experience": "è¯„ä¼°ä¾æ®",
    "conscientiousness": "è¯„ä¼°ä¾æ®",
    "extraversion": "è¯„ä¼°ä¾æ®",
    "agreeableness": "è¯„ä¼°ä¾æ®",
    "neuroticism": "è¯„ä¼°ä¾æ®"
  },
  "analysis": "è¯¦ç»†åˆ†æè¿‡ç¨‹"
}
```
"""
        return prompt

    def _clean_json_string(self, json_str: str) -> str:
        """æ¸…ç†JSONå­—ç¬¦ä¸²ä¸­çš„æ§åˆ¶å­—ç¬¦å’Œæ— æ•ˆå­—ç¬¦"""
        import re

        # ç§»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆé™¤äº†æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ã€å›è½¦ç¬¦ï¼‰
        json_str = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', json_str)

        # ä¿®å¤å¸¸è§çš„JSONæ ¼å¼é—®é¢˜
        # ç§»é™¤å¤šä½™çš„é€—å·ï¼ˆåœ¨}æˆ–]ä¹‹å‰ï¼‰
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)

        # ç¡®ä¿å­—ç¬¦ä¸²å€¼è¢«æ­£ç¡®å¼•å·åŒ…å›´
        json_str = re.sub(r':\s*([^",\[\]\{\}\s][^",\[\]\{\}]*[^",\[\]\{\}\s])(\s*[,}\]])', r': "\1"\2', json_str)

        # ä¿®å¤è½¬ä¹‰å­—ç¬¦é—®é¢˜
        json_str = json_str.replace('\\"', '"')
        json_str = json_str.replace('\\/', '/')

        return json_str

    def _extract_and_fix_json(self, response_content: str) -> str:
        """æ›´æ¿€è¿›çš„JSONæå–å’Œä¿®å¤"""
        import re

        # å°è¯•å¤šç§JSONæå–æ¨¡å¼
        patterns = [
            # æ ‡å‡†JSONå¯¹è±¡
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',
            # å¸¦æœ‰æ¢è¡Œçš„JSON
            r'\{[\s\S]*?\}',
            # ç®€åŒ–çš„JSONç»“æ„
            r'\{[^}]*"scores"[^}]*\}',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, response_content)
            for match in matches:
                try:
                    # æ¸…ç†å’Œä¿®å¤
                    cleaned = self._clean_json_string(match)
                    # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
                    json.loads(cleaned)
                    return cleaned
                except:
                    continue

        # å¦‚æœä»¥ä¸Šéƒ½å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨æ„å»ºJSON
        return self._build_fallback_json(response_content)

    def _build_fallback_json(self, response_content: str) -> str:
        """æ„å»ºå¤‡ç”¨çš„JSONç»“æ„"""
        import re

        # å°è¯•æå–åˆ†æ•°ä¿¡æ¯
        scores = {}
        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']

        for trait in traits:
            # æŸ¥æ‰¾å½¢å¦‚ "openness_to_experience": 3 çš„æ¨¡å¼
            pattern = rf'"{trait}"\s*:\s*([1-5])'
            match = re.search(pattern, response_content)
            if match:
                scores[trait] = int(match.group(1))
            else:
                scores[trait] = 3  # é»˜è®¤å€¼

        # æ„å»ºæ ‡å‡†JSON
        return json.dumps({
            'scores': scores,
            'analysis': 'Auto-fixed from malformed response',
            'confidence': 'medium'
        })

    def _parse_segment_response(self, response_content: str, segment_number: int) -> Dict:
        """è§£æåˆ†æ®µåˆ†æå“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_start = response_content.find('{')
            json_end = response_content.rfind('}') + 1

            if json_start != -1 and json_end > json_start:
                json_str = response_content[json_start:json_end]

                # æ¸…ç†JSONå­—ç¬¦ä¸²ä¸­çš„æ§åˆ¶å­—ç¬¦å’Œæ— æ•ˆå­—ç¬¦
                json_str = self._clean_json_string(json_str)

                result = json.loads(json_str)

                # éªŒè¯å¹¶ä¿®å¤è¯„åˆ†
                if 'scores' in result:
                    for trait in result['scores']:
                        result['scores'][trait] = self.validate_and_fix_score(result['scores'][trait])

                result['segment_number'] = segment_number
                result['raw_response'] = response_content
                return result
            else:
                return {
                    'success': False,
                    'segment_number': segment_number,
                    'error': 'æ— æ³•æå–JSONç»“æœ',
                    'raw_response': response_content
                }

        except json.JSONDecodeError as e:
            # å°è¯•æ›´æ¿€è¿›çš„JSONä¿®å¤
            try:
                # å°è¯•æå–å¹¶ä¿®å¤JSON
                fixed_json = self._extract_and_fix_json(response_content)
                if fixed_json:
                    result = json.loads(fixed_json)

                    # éªŒè¯å¹¶ä¿®å¤è¯„åˆ†
                    if 'scores' in result:
                        for trait in result['scores']:
                            result['scores'][trait] = self.validate_and_fix_score(result['scores'][trait])

                    result['segment_number'] = segment_number
                    result['raw_response'] = response_content
                    result['json_fixed'] = True
                    return result
            except:
                pass  # å¦‚æœä¿®å¤å¤±è´¥ï¼Œç»§ç»­åˆ°åŸå§‹é”™è¯¯å¤„ç†

            return {
                'success': False,
                'segment_number': segment_number,
                'error': f'JSONè§£æé”™è¯¯: {e}',
                'raw_response': response_content
            }
        except Exception as e:
            # å¤„ç†å…¶ä»–å¼‚å¸¸ï¼Œç‰¹åˆ«æ˜¯'success'é”®é”™è¯¯
            try:
                # å°è¯•æå–å¹¶ä¿®å¤JSON
                fixed_json = self._extract_and_fix_json(response_content)
                if fixed_json:
                    result = json.loads(fixed_json)

                    # éªŒè¯å¹¶ä¿®å¤è¯„åˆ†
                    if 'scores' in result:
                        for trait in result['scores']:
                            result['scores'][trait] = self.validate_and_fix_score(result['scores'][trait])

                    result['segment_number'] = segment_number
                    result['raw_response'] = response_content
                    result['json_fixed'] = True
                    return result
            except:
                pass  # å¦‚æœä¿®å¤å¤±è´¥ï¼Œç»§ç»­åˆ°åŸå§‹é”™è¯¯å¤„ç†

            return {
                'success': False,
                'segment_number': segment_number,
                'error': f'è§£æé”™è¯¯: {e}',
                'raw_response': response_content
            }

    def _accumulate_scores(self, scores: Dict, evidence: Dict):
        """ç´¯ç§¯åˆ†æ®µè¯„åˆ†åˆ°æ€»è¯„åˆ†"""
        for trait, score in scores.items():
            if trait in self.big_five_traits:
                self.big_five_traits[trait]['scores'].append(score)
                self.big_five_traits[trait]['weight'] += 1

        for trait, ev in evidence.items():
            if trait in self.big_five_traits:
                self.big_five_traits[trait]['evidence'].append(ev)

    def analyze_full_assessment(self, input_file: str, output_dir: str) -> Dict:
        """åˆ†æå®Œæ•´çš„æµ‹è¯„è¯„ä¼°"""
        print(f"ğŸ” å¼€å§‹åˆ†æ: {input_file}")

        try:
            # è¯»å–è¯„ä¼°æ•°æ®
            with open(input_file, 'r', encoding='utf-8') as f:
                assessment_data = json.load(f)

            # æå–é—®é¢˜
            questions = self.extract_questions(assessment_data)
            if not questions:
                return {
                    'success': False,
                    'error': 'æ— æ³•æå–é—®é¢˜æ•°æ®',
                    'file': input_file
                }

            print(f"ğŸ“Š æå–åˆ° {len(questions)} ä¸ªé—®é¢˜")

            # åˆ›å»ºåˆ†æ®µ
            segments = self.create_segments(questions)

            # åˆ†ææ¯ä¸ªåˆ†æ®µ
            successful_segments = 0
            for i, segment in enumerate(segments, 1):
                print(f"ğŸ“ åˆ†æåˆ†æ®µ {i}/{len(segments)}...")
                print(f"  ä½¿ç”¨ {self.model} åˆ†ææ®µ {i} ({len(segment)} é¢˜)")

                segment_result = self.analyze_segment(segment, i)
                self.segment_results.append(segment_result)

                if segment_result['success']:
                    successful_segments += 1
                    print(f"  æˆåŠŸç´¯ç§¯åˆ†æ®µ {i} çš„è¯„åˆ†")
                else:
                    print(f"  æ®µ {i} åˆ†æå¤±è´¥: {segment_result.get('error', 'Unknown error')}")

            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            final_scores = self.calculate_final_scores()
            mbti_result = self.generate_mbti_type(final_scores)

            # ä¿å­˜ç»“æœ
            self.save_analysis_results(input_file, output_dir, final_scores, mbti_result)

            success_rate = successful_segments / len(segments) * 100 if segments else 0
            print(f"âœ… åˆ†æå®Œæˆ: {Path(input_file).name}")
            print(f"ğŸ“Š æˆåŠŸç‡: {success_rate:.1f}% ({successful_segments}/{len(segments)})")

            return {
                'success': True,
                'file': input_file,
                'success_rate': success_rate,
                'final_scores': final_scores,
                'mbti_result': mbti_result
            }

        except Exception as e:
            print(f"ğŸ’¥ åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'error': str(e),
                'file': input_file
            }

    def calculate_final_scores(self) -> Dict:
        """è®¡ç®—æœ€ç»ˆBig5è¯„åˆ†"""
        final_scores = {}

        for trait, data in self.big_five_traits.items():
            scores = data['scores']

            if scores:
                # è®¡ç®—å¹³å‡åˆ†
                avg_score = sum(scores) / len(scores)

                # å››èˆäº”å…¥åˆ°æœ€æ¥è¿‘çš„1ã€3ã€5
                if avg_score <= 2:
                    final_score = 1
                elif avg_score <= 4:
                    final_score = 3
                else:
                    final_score = 5

                # è®¡ç®—ç½®ä¿¡åº¦
                score_distribution = {1: 0, 3: 0, 5: 0}
                for score in scores:
                    score_distribution[score] += 1

                # ç½®ä¿¡åº¦è®¡ç®—ï¼šåŸºäºè¯„åˆ†ä¸€è‡´æ€§
                max_count = max(score_distribution.values())
                consistency = max_count / len(scores)
                confidence = round(consistency * 100, 1)

                final_scores[trait] = {
                    'final_score': final_score,
                    'average_score': round(avg_score, 2),
                    'raw_scores': scores.copy(),
                    'score_distribution': score_distribution,
                    'confidence_percent': confidence,
                    'evidence_count': len(data['evidence']),
                    'weight': len(scores),
                    'evidence_samples': data['evidence'][:3]
                }
            else:
                final_scores[trait] = {
                    'final_score': 3,  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
                    'average_score': 3.0,
                    'raw_scores': [],
                    'score_distribution': {1: 0, 3: 0, 5: 0},
                    'confidence_percent': 0.0,
                    'evidence_count': 0,
                    'weight': 0,
                    'evidence_samples': [],
                    'warning': 'No successful segment analysis'
                }

        return final_scores

    def generate_mbti_type(self, final_scores: Dict) -> Dict:
        """åŸºäºBig5è¯„åˆ†ç”ŸæˆMBTIç±»å‹"""
        O = final_scores.get('openness_to_experience', {}).get('final_score', 3)
        C = final_scores.get('conscientiousness', {}).get('final_score', 3)
        E = final_scores.get('extraversion', {}).get('final_score', 3)
        A = final_scores.get('agreeableness', {}).get('final_score', 3)
        N = final_scores.get('neuroticism', {}).get('final_score', 3)

        # MBTIè®¡ç®—é€»è¾‘
        e_score = E + (5 - N)
        i_score = (5 - E) + N
        E_preference = 'E' if e_score > i_score else 'I'
        E_confidence = abs(e_score - i_score) / 8

        S_confidence = abs(O - 3) / 2
        S_preference = 'N' if O > 3 else 'S'

        T_confidence = abs(A - 3) / 2
        T_preference = 'T' if A < 3 else 'F'

        J_confidence = abs(C - 3) / 2
        J_preference = 'J' if C > 3 else 'P'

        overall_confidence = (E_confidence + S_confidence + T_confidence + J_confidence) / 4

        mbti_type = f"{E_preference}{S_preference}{T_preference}{J_preference}"

        return {
            'type': mbti_type,
            'preferences': {
                'E/I': {'score': e_score, 'preference': E_preference, 'confidence': round(E_confidence * 100, 1)},
                'S/N': {'score': O, 'preference': S_preference, 'confidence': round(S_confidence * 100, 1)},
                'T/F': {'score': A, 'preference': T_preference, 'confidence': round(T_confidence * 100, 1)},
                'J/P': {'score': C, 'preference': J_preference, 'confidence': round(J_confidence * 100, 1)}
            },
            'overall_confidence': round(overall_confidence * 100, 1)
        }

    def save_analysis_results(self, input_file: str, output_dir: str, final_scores: Dict, mbti_result: Dict):
        """ä¿å­˜åˆ†æç»“æœåˆ°åˆ†ç¦»çš„æ–‡ä»¶"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        filename_base = Path(input_file).stem

        # ä¿å­˜æ‘˜è¦æ–‡ä»¶ï¼ˆåªåŒ…å«è¯„åˆ†å’ŒMBTIï¼‰
        summary_data = {
            'analysis_info': {
                'file_analyzed': input_file,
                'filename': Path(input_file).name,
                'analysis_timestamp': datetime.now().isoformat(),
                'model_used': self.model,
                'analyzer_type': 'enhanced_cloud_segmented_v1.0'
            },
            'big5_final_scores': {
                trait: {
                    'final_score': data['final_score'],
                    'confidence_percent': data['confidence_percent'],
                    'evidence_count': data['evidence_count']
                } for trait, data in final_scores.items()
            },
            'mbti_type': mbti_result['type'],
            'mbti_confidence': mbti_result['overall_confidence'],
            'analysis_quality': {
                'successful_segments': sum(1 for r in self.segment_results if r['success']),
                'total_segments': len(self.segment_results),
                'success_rate': sum(1 for r in self.segment_results if r['success']) / len(self.segment_results) * 100 if self.segment_results else 0
            }
        }

        summary_file = output_path / f"{filename_base}_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)

        # ä¿å­˜è¯¦ç»†è¯æ®æ–‡ä»¶ï¼ˆåŒ…å«æ‰€æœ‰è¯„åˆ†ä¾æ®ï¼‰
        evidence_data = {
            'analysis_info': summary_data['analysis_info'],
            'detailed_scores': final_scores,
            'mbti_detailed': mbti_result,
            'segment_results': self.segment_results,
            'raw_traits_data': self.big_five_traits
        }

        evidence_file = output_path / f"{filename_base}_detailed_evidence.json"
        with open(evidence_file, 'w', encoding='utf-8') as f:
            json.dump(evidence_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ æ‘˜è¦æ–‡ä»¶: {summary_file.name}")
        print(f"ğŸ“‹ è¯¦ç»†è¯æ®æ–‡ä»¶: {evidence_file.name}")

        # æ‰“å°æœ€ç»ˆç»“æœæ‘˜è¦
        print(f"ğŸ¯ Big5æœ€ç»ˆè¯„åˆ†:")
        for trait, data in final_scores.items():
            confidence = data['confidence_percent']
            print(f"  {trait}: {data['final_score']}/5 (ç½®ä¿¡åº¦: {confidence}%, åŸºäº{data['weight']}ä¸ªè¯æ®)")
        print(f"ğŸ§  MBTIç±»å‹: {mbti_result['type']} (ç½®ä¿¡åº¦: {mbti_result['overall_confidence']}%)")