#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤šæ¨¡å‹ç½®ä¿¡åº¦å¿ƒç†è¯„ä¼°åˆ†æå™¨
ä½¿ç”¨å¤šä¸ªäº‘æ¨¡å‹è¿›è¡Œæ¯”è¾ƒåˆ†æï¼ŒåŸºäºæ¨¡å‹é—´ä¸€è‡´æ€§è®¡ç®—ç½®ä¿¡åº¦
"""

import json
import sys
import time
import argparse
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from multi_model_confidence_analyzer import MultiModelConfidenceAnalyzer

class BatchMultiModelAnalyzer:
    """æ‰¹é‡å¤šæ¨¡å‹ç½®ä¿¡åº¦åˆ†æå™¨"""

    def __init__(self, models: list = None, api_key: str = None, max_workers: int = 1):
        self.models = models or ["qwen-long", "qwen-max"]
        self.api_key = api_key or "sk-ffd03518254b495b8d27e723cd413fc1"
        self.max_workers = max_workers
        self.results = []

    def analyze_single_file(self, input_file: Path, output_dir: Path) -> dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„å¤šæ¨¡å‹ç½®ä¿¡åº¦"""
        try:
            # åˆ›å»ºå¤šæ¨¡å‹åˆ†æå™¨
            analyzer = MultiModelConfidenceAnalyzer(
                models=self.models,
                api_key=self.api_key
            )

            # æ‰§è¡Œå¤šæ¨¡å‹åˆ†æ
            result = analyzer.analyze_with_multiple_models(input_file, output_dir)

            if result['success']:
                confidence = result['confidence_analysis']['overall_confidence']
                successful_models = result['confidence_analysis']['successful_models']

                # ç”Ÿæˆç®€åŒ–æ‘˜è¦
                summary = {
                    'file': str(input_file),
                    'success': True,
                    'overall_confidence': confidence,
                    'successful_models': successful_models,
                    'total_models_attempted': len(self.models),
                    'big5_confidence': result['confidence_analysis']['big5_confidence'],
                    'mbti_confidence': result['confidence_analysis']['mbti_confidence'],
                    'most_common_mbti': result['confidence_analysis']['mbti_confidence'].get('most_common_type', 'N/A'),
                    'agreement_level': result['confidence_analysis']['mbti_confidence'].get('agreement_level', 'N/A')
                }

                # è·å–ç¬¬ä¸€ä¸ªæˆåŠŸæ¨¡å‹çš„ç»“æœä½œä¸ºä»£è¡¨æ€§è¯„åˆ†
                if successful_models:
                    first_success_model = successful_models[0]
                    model_result = result['multi_model_results'][first_success_model]
                    summary['representative_big5_scores'] = model_result['big5_scores']
                    summary['representative_mbti'] = model_result['mbti_type']

                big5_str = ""
                if 'representative_big5_scores' in summary:
                    big5_str = ", ".join([f"{trait[0].upper()}:{score}" for trait, score in summary['representative_big5_scores'].items()])

                print(f"âœ… {input_file.name} - ç½®ä¿¡åº¦: {confidence}% - MBTI: {summary.get('representative_mbti', 'N/A')} ({summary['agreement_level']})")
                return summary
            else:
                print(f"âŒ {input_file.name} - å¤šæ¨¡å‹åˆ†æå¤±è´¥")
                return {
                    'file': str(input_file),
                    'success': False,
                    'error': 'No successful model analysis',
                    'models_attempted': self.models
                }

        except Exception as e:
            print(f"ğŸ’¥ {input_file.name} - å¼‚å¸¸: {e}")
            return {
                'file': str(input_file),
                'success': False,
                'error': str(e),
                'models_attempted': self.models
            }

    def analyze_batch(self, input_files: list[Path], output_dir: Path,
                     progress_callback=None, delay_between_files: int = 10) -> list[dict]:
        """æ‰¹é‡åˆ†ææ–‡ä»¶çš„å¤šæ¨¡å‹ç½®ä¿¡åº¦"""
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤šæ¨¡å‹ç½®ä¿¡åº¦åˆ†æ")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {len(input_files)} ä¸ª")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {', '.join(self.models)}")
        print(f"âš¡ å¹¶å‘æ•°: {self.max_workers}")
        print(f"â±ï¸  æ–‡ä»¶é—´å»¶è¿Ÿ: {delay_between_files} ç§’")

        output_dir.mkdir(parents=True, exist_ok=True)

        results = []
        completed = 0

        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(self.analyze_single_file, file, output_dir): file
                for file in input_files
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_file):
                file = future_to_file[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed += 1

                    if progress_callback:
                        progress_callback(completed, len(input_files), result)

                    # æ˜¾ç¤ºè¿›åº¦
                    status = "âœ…" if result['success'] else "âŒ"
                    confidence_str = f" (ç½®ä¿¡åº¦: {result['overall_confidence']}%)" if result['success'] else ""
                    print(f"[{completed}/{len(input_files)}] {status} {file.name}{confidence_str}")

                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    if completed < len(input_files):
                        print(f"â³ ç­‰å¾… {delay_between_files} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶...")
                        time.sleep(delay_between_files)

                except Exception as e:
                    print(f"ğŸ’¥ å¤„ç† {file.name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    results.append({
                        'file': str(file),
                        'success': False,
                        'error': str(e),
                        'models_attempted': self.models
                    })
                    completed += 1

        return results

    def generate_summary_report(self, results: list[dict], output_dir: Path):
        """ç”Ÿæˆå¤šæ¨¡å‹æ‰¹é‡åˆ†ææ±‡æ€»æŠ¥å‘Š"""
        successful_results = [r for r in results if r['success']]
        failed_results = [r for r in results if not r['success']]

        # ç»Ÿè®¡æ•°æ®
        overall_confidences = [r['overall_confidence'] for r in successful_results]
        avg_confidence = sum(overall_confidences) / len(overall_confidences) if overall_confidences else 0

        # ç»Ÿè®¡Big5è¯„åˆ†åˆ†å¸ƒï¼ˆä½¿ç”¨ä»£è¡¨æ€§è¯„åˆ†ï¼‰
        big5_stats = {}
        mbti_stats = {}
        confidence_distribution = {'é«˜åº¦ä¸€è‡´': 0, 'ä¸­ç­‰ä¸€è‡´': 0, 'ä½åº¦ä¸€è‡´': 0, 'ä¸ä¸€è‡´': 0}

        for result in successful_results:
            # Big5ç»Ÿè®¡
            if 'representative_big5_scores' in result:
                for trait, score in result['representative_big5_scores'].items():
                    if trait not in big5_stats:
                        big5_stats[trait] = {1: 0, 3: 0, 5: 0}
                    big5_stats[trait][score] += 1

            # MBTIç»Ÿè®¡
            if 'representative_mbti' in result:
                mbti_type = result['representative_mbti']
                if mbti_type not in mbti_stats:
                    mbti_stats[mbti_type] = 0
                mbti_stats[mbti_type] += 1

            # ç½®ä¿¡åº¦åˆ†å¸ƒç»Ÿè®¡
            agreement_level = result.get('agreement_level', 'ä¸ä¸€è‡´')
            if agreement_level in confidence_distribution:
                confidence_distribution[agreement_level] += 1

        # ç”Ÿæˆæ±‡æ€»æ•°æ®
        summary = {
            'summary': {
                'total_files': len(results),
                'successful': len(successful_results),
                'failed': len(failed_results),
                'success_rate': len(successful_results) / len(results) * 100 if results else 0,
                'models_used': self.models,
                'analysis_timestamp': datetime.now().isoformat(),
                'algorithm_version': 'multi_model_confidence_v1.0'
            },
            'confidence_statistics': {
                'average_confidence': round(avg_confidence, 1),
                'confidence_distribution': confidence_distribution,
                'high_confidence_count': confidence_distribution['é«˜åº¦ä¸€è‡´'],
                'medium_confidence_count': confidence_distribution['ä¸­ç­‰ä¸€è‡´'],
                'low_confidence_count': confidence_distribution['ä½åº¦ä¸€è‡´'],
                'inconsistent_count': confidence_distribution['ä¸ä¸€è‡´']
            },
            'big5_distribution': big5_stats,
            'mbti_distribution': mbti_stats,
            'detailed_results': results
        }

        # ä¿å­˜JSONæ±‡æ€»
        json_summary = output_dir / f"batch_multi_model_summary.json"
        with open(json_summary, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(summary, output_dir)

        print(f"\nğŸ“Š å¤šæ¨¡å‹æ‰¹é‡åˆ†ææ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"   JSON: {json_summary}")
        print(f"   Markdown: {output_dir / 'batch_multi_model_report.md'}")

    def generate_markdown_report(self, summary: dict, output_dir: Path):
        """ç”ŸæˆMarkdownæ ¼å¼çš„å¤šæ¨¡å‹æ‰¹é‡åˆ†ææŠ¥å‘Š"""
        summary_stats = summary['summary']
        confidence_stats = summary['confidence_statistics']
        big5_stats = summary['big5_distribution']
        mbti_stats = summary['mbti_distribution']

        md_content = f"""# æ‰¹é‡å¤šæ¨¡å‹ç½®ä¿¡åº¦åˆ†ææŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **åˆ†ææ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ä½¿ç”¨æ¨¡å‹:** {', '.join(summary_stats['models_used'])}
- **ç®—æ³•ç‰ˆæœ¬:** multi_model_confidence_v1.0
- **è¯„åˆ†æ ‡å‡†:** ä¸¥æ ¼1-3-5è¯„åˆ† (1=ä½, 3=ä¸­, 5=é«˜)
- **ç½®ä¿¡åº¦è®¡ç®—:** åŸºäºå¤šæ¨¡å‹é—´ä¸€è‡´æ€§

## æ±‡æ€»ç»Ÿè®¡

- **æ€»æ–‡ä»¶æ•°:** {summary_stats['total_files']}
- **æˆåŠŸåˆ†æ:** {summary_stats['successful']}
- **å¤±è´¥åˆ†æ:** {summary_stats['failed']}
- **æˆåŠŸç‡:** {summary_stats['success_rate']:.1f}%

## ç½®ä¿¡åº¦ç»Ÿè®¡

- **å¹³å‡ç½®ä¿¡åº¦:** {confidence_stats['average_confidence']}%
- **é«˜åº¦ä¸€è‡´ (â‰¥80%):** {confidence_stats['high_confidence_count']} ä¸ªæ–‡ä»¶
- **ä¸­ç­‰ä¸€è‡´ (60-79%):** {confidence_stats['medium_confidence_count']} ä¸ªæ–‡ä»¶
- **ä½åº¦ä¸€è‡´ (40-59%):** {confidence_stats['low_confidence_count']} ä¸ªæ–‡ä»¶
- **ä¸ä¸€è‡´ (<40%):** {confidence_stats['inconsistent_count']} ä¸ªæ–‡ä»¶

## Big5è¯„åˆ†åˆ†å¸ƒ

"""

        for trait, scores in big5_stats.items():
            total = sum(scores.values())
            md_content += f"### {trait.replace('_', ' ').title()}\n"
            md_content += f"- 1åˆ† (ä½): {scores[1]} ({scores[1]/total*100:.1f}%)\n"
            md_content += f"- 3åˆ† (ä¸­): {scores[3]} ({scores[3]/total*100:.1f}%)\n"
            md_content += f"- 5åˆ† (é«˜): {scores[5]} ({scores[5]/total*100:.1f}%)\n\n"

        md_content += "## MBTIç±»å‹åˆ†å¸ƒ\n\n"

        for mbti_type, count in sorted(mbti_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(mbti_stats) * 100 if mbti_stats else 0
            md_content += f"- **{mbti_type}:** {count} ({percentage:.1f}%)\n"

        md_content += "\n## è¯¦ç»†ç»“æœ\n\n"

        for result in summary['detailed_results']:
            if result['success']:
                filename = Path(result['file']).name
                confidence = result['overall_confidence']
                mbti = result.get('representative_mbti', 'N/A')
                agreement = result.get('agreement_level', 'N/A')
                models = f"{len(result['successful_models'])}/{result['total_models_attempted']}"

                big5_str = ""
                if 'representative_big5_scores' in result:
                    big5_str = " - Big5: " + ", ".join([f"{trait[0].upper()}:{score}" for trait, score in result['representative_big5_scores'].items()])

                md_content += f"- **{filename}** - ç½®ä¿¡åº¦: {confidence}% - MBTI: {mbti} ({agreement}) - æ¨¡å‹: {models}{big5_str}\n"

        if failed_results:
            md_content += "\n## å¤±è´¥çš„æ–‡ä»¶\n\n"
            for result in failed_results:
                filename = Path(result['file']).name
                md_content += f"- **{filename}** - é”™è¯¯: {result.get('error', 'Unknown error')}\n"

        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_file = output_dir / "batch_multi_model_report.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡å¤šæ¨¡å‹ç½®ä¿¡åº¦Big5åˆ†æ')
    parser.add_argument('input_path', help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--models', nargs='+', default=['qwen-long', 'qwen-max'],
                       help='ä½¿ç”¨çš„äº‘æ¨¡å‹åˆ—è¡¨')
    parser.add_argument('--output', default='multi_model_confidence_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--sample', type=int, help='é‡‡æ ·æ–‡ä»¶æ•°é‡')
    parser.add_argument('--filter', help='æ–‡ä»¶åè¿‡æ»¤æ¨¡å¼')
    parser.add_argument('--workers', type=int, default=1, help='å¹¶å‘å·¥ä½œæ•°ï¼ˆå»ºè®®1é¿å…APIé™åˆ¶ï¼‰')
    parser.add_argument('--delay', type=int, default=15, help='æ–‡ä»¶é—´å»¶è¿Ÿç§’æ•°ï¼ˆå¤šæ¨¡å‹åˆ†æéœ€è¦æ›´é•¿å»¶è¿Ÿï¼‰')

    args = parser.parse_args()

    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    input_path = Path(args.input_path)
    if input_path.is_file():
        input_files = [input_path]
    elif input_path.is_dir():
        input_files = list(input_path.glob("*.json"))
    else:
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return

    # åº”ç”¨è¿‡æ»¤å™¨
    if args.filter:
        input_files = [f for f in input_files if args.filter.lower() in f.name.lower()]

    # é‡‡æ ·
    if args.sample:
        input_files = input_files[:args.sample]

    if not input_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
        return

    print(f"ğŸ” æ‰¾åˆ° {len(input_files)} ä¸ªæ–‡ä»¶è¿›è¡Œå¤šæ¨¡å‹åˆ†æ")

    # åˆ›å»ºæ‰¹é‡å¤šæ¨¡å‹åˆ†æå™¨
    analyzer = BatchMultiModelAnalyzer(
        models=args.models,
        max_workers=args.workers
    )

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # æ‰§è¡Œæ‰¹é‡åˆ†æ
    def progress_callback(completed, total, result):
        success_rate = sum(1 for r in analyzer.results if r.get('success', False)) / len(analyzer.results) * 100 if analyzer.results else 0
        avg_confidence = sum(r.get('overall_confidence', 0) for r in analyzer.results if r.get('success', False)) / len([r for r in analyzer.results if r.get('success', False)]) if analyzer.results and any(r.get('success', False) for r in analyzer.results) else 0
        print(f"ğŸ“ˆ è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) - æˆåŠŸç‡: {success_rate:.1f}% - å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.1f}%")

    results = analyzer.analyze_batch(
        input_files,
        output_dir,
        progress_callback=progress_callback,
        delay_between_files=args.delay
    )

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print(f"\nğŸ“‹ ç”Ÿæˆå¤šæ¨¡å‹ç½®ä¿¡åº¦æ±‡æ€»æŠ¥å‘Š...")
    analyzer.generate_summary_report(results, output_dir)

    # æœ€ç»ˆç»Ÿè®¡
    successful = sum(1 for r in results if r['success'])
    avg_confidence = sum(r.get('overall_confidence', 0) for r in results if r['success']) / len([r for r in results if r['success']]) if successful > 0 else 0
    high_confidence = sum(1 for r in results if r['success'] and r.get('overall_confidence', 0) >= 80)

    print(f"\nğŸ‰ å¤šæ¨¡å‹æ‰¹é‡åˆ†æå®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {successful}/{len(results)} ({successful/len(results)*100:.1f}%)")
    print(f"ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.1f}%")
    print(f"ğŸ¯ é«˜ç½®ä¿¡åº¦æ–‡ä»¶: {high_confidence} ä¸ª")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ”§ ä½¿ç”¨å¤šæ¨¡å‹ç½®ä¿¡åº¦ç®—æ³•: æ¨¡å‹é—´ä¸€è‡´æ€§æ¯”è¾ƒ")

if __name__ == "__main__":
    main()