#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¹è¿›ç‰ˆäº‘è¯„ä¼°å™¨ - è§£å†³è§’è‰²æ··æ·†é—®é¢˜
"""

import json
import re
import time
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
import statistics

# å¼ºåˆ¶æ— ç¼“å†²è¾“å‡º
import sys
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)

# è®¾ç½®ç¯å¢ƒå˜é‡
import os
os.environ['PYTHONUNBUFFERED'] = '1'

class ImprovedCloudAnalyzer:
    def __init__(self, model: str = "qwen-max"):
        self.model = model
        self.api_key = os.getenv('DASHSCOPE_API_KEY', 'sk-ded837735b3c44599a9bc138da561c27')
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_segments': 0,
            'successful_segments': 0,
            'failed_segments': 0,
            'total_api_calls': 0,
            'score_distribution': {1: 0, 3: 0, 5: 0},
            'unique_score_patterns': set()
        }

    def _build_improved_prompt(self, segment: List[Dict], segment_number: int) -> str:
        """æ„å»ºæ”¹è¿›çš„åˆ†æ®µåˆ†ææç¤º"""

        prompt = f"""ã€é‡è¦ï¼šä½ çš„äººæ ¼è¯„ä¼°åˆ†æå¸ˆè§’è‰²ã€‘

ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨è¯„ä¼°AIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**ä»¥ä¸‹é—®å·å›ç­”ï¼Œ**ä¸æ˜¯**å›ç­”é—®é¢˜ã€‚

**å…³é”®åŒºåˆ«ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…
- âŒ ä½ ä¸è¦å›ç­”é—®å·é—®é¢˜
- âœ… ä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä½ è¦åˆ†æå›ç­”è€…å±•ç°çš„äººæ ¼ç‰¹å¾

**è¯„ä¼°ä»»åŠ¡ï¼š**
åˆ†æä»¥ä¸‹{len(segment)}ä¸ªé—®é¢˜å’Œå›ç­”ï¼Œè¯„ä¼°å›ç­”è€…çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ç¨‹åº¦
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æº
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘

**è¯„åˆ†æ ‡å‡†ï¼ˆä¸¥æ ¼ä½¿ç”¨ï¼‰ï¼š**
- 1åˆ†ï¼šæä½è¡¨ç°ï¼ˆæ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨ï¼‰
- 2åˆ†ï¼šè¾ƒä½è¡¨ç°ï¼ˆå€¾å‘ç¼ºä¹ï¼‰
- 3åˆ†ï¼šä¸­ç­‰è¡¨ç°ï¼ˆå¹³è¡¡æˆ–ä¸ç¡®å®šï¼‰
- 4åˆ†ï¼šè¾ƒé«˜è¡¨ç°ï¼ˆå€¾å‘å…·å¤‡ï¼‰
- 5åˆ†ï¼šæé«˜è¡¨ç°ï¼ˆæ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨ï¼‰

**åˆ†ææ–¹æ³•ï¼š**
1. å¿½ç•¥è§’è‰²æ‰®æ¼”è®¾å®šï¼ˆå¦‚"æˆ‘æ˜¯XXè§’è‰²"ï¼‰
2. ä¸“æ³¨å®é™…å›ç­”å†…å®¹å’Œè¡Œä¸ºå€¾å‘
3. å¯»æ‰¾å…·ä½“çš„è¡Œä¸ºè¯æ®
4. é¿å…é»˜è®¤ç»™ä¸­ç­‰åˆ†æ•°

---

**é—®å·å†…å®¹ï¼ˆç¬¬{segment_number}æ®µï¼‰ï¼š**
"""

        for i, item in enumerate(segment, 1):
            prompt += f"""
**é—®é¢˜ {i}:**
{item['question']}

**å›ç­” {i}:**
{item['answer']}

---
"""

        prompt += """
**è¾“å‡ºè¦æ±‚ï¼š**
è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š

```json
{
  "success": true,
  "segment_number": åˆ†æ®µç¼–å·,
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {
    "openness_to_experience": 1-5æ•´æ•°,
    "conscientiousness": 1-5æ•´æ•°,
    "extraversion": 1-5æ•´æ•°,
    "agreeableness": 1-5æ•´æ•°,
    "neuroticism": 1-5æ•´æ•°
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“è¯æ®å¼•ç”¨",
    "conscientiousness": "å…·ä½“è¯æ®å¼•ç”¨",
    "extraversion": "å…·ä½“è¯æ®å¼•ç”¨",
    "agreeableness": "å…·ä½“è¯æ®å¼•ç”¨",
    "neuroticism": "å…·ä½“è¯æ®å¼•ç”¨"
  },
  "confidence": "high/medium/low"
}
```

**é‡è¦æé†’ï¼š**
- ä½ æ˜¯åˆ†æå¸ˆï¼Œä¸æ˜¯å›ç­”è€…
- åŸºäº**å®é™…å›ç­”å†…å®¹**è¯„ä¼°
- ç»™å‡ºå·®å¼‚åŒ–è¯„åˆ†ï¼Œé¿å…å…¨3åˆ†
- æä¾›å…·ä½“çš„æ–‡å­—è¯æ®
"""

        return prompt

    def analyze_segment_improved(self, segment: List[Dict], segment_number: int) -> Dict:
        """æ”¹è¿›çš„åˆ†æ®µåˆ†ææ–¹æ³•"""
        self.stats['total_segments'] += 1
        self.stats['total_api_calls'] += 1

        try:
            # æ„å»ºæ”¹è¿›çš„æç¤º
            prompt = self._build_improved_prompt(segment, segment_number)

            # è°ƒç”¨API
            import openai
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )

            print(f"  ğŸ” åˆ†ææ®µ{segment_number}: {self.model} ({len(segment)}é¢˜)")

            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚ä¸“æ³¨äºåˆ†æä»–äººçš„äººæ ¼ç‰¹å¾ï¼Œä¸è¦æ··æ·†è‡ªå·±çš„è§’è‰²ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.1
            )

            analysis_content = response.choices[0].message.content
            print(f"  ğŸ“ APIå“åº”é•¿åº¦: {len(analysis_content)} å­—ç¬¦")

            # è§£æç»“æœ
            result = self._parse_improved_response(analysis_content, segment_number)

            if result['success']:
                self.stats['successful_segments'] += 1

                # ç»Ÿè®¡è¯„åˆ†åˆ†å¸ƒ
                for score in result['scores'].values():
                    if score in self.stats['score_distribution']:
                        self.stats['score_distribution'][score] += 1

                # è®°å½•è¯„åˆ†æ¨¡å¼
                score_pattern = tuple(sorted(result['scores'].values()))
                self.stats['unique_score_patterns'].add(score_pattern)

                # æ£€æŸ¥æ˜¯å¦å…¨3åˆ†
                all_three = all(score == 3 for score in result['scores'].values())
                if all_three:
                    print(f"  âš ï¸ è­¦å‘Š: æ‰€æœ‰è¯„åˆ†å‡ä¸º3åˆ†")
                else:
                    print(f"  âœ… è¯„åˆ†æœ‰å·®å¼‚åŒ–: {set(result['scores'].values())}")

                print(f"  âœ… æ®µ{segment_number} åˆ†ææˆåŠŸ")
                return result
            else:
                self.stats['failed_segments'] += 1
                print(f"  âŒ æ®µ{segment_number} åˆ†æå¤±è´¥: {result.get('error', 'Unknown error')}")
                return result

        except Exception as e:
            self.stats['failed_segments'] += 1
            print(f"  ğŸ’¥ æ®µ{segment_number} åˆ†æå¼‚å¸¸: {e}")
            return {'success': False, 'segment_number': segment_number, 'error': str(e)}

    def _parse_improved_response(self, response_content: str, segment_number: int) -> Dict:
        """è§£ææ”¹è¿›çš„å“åº”æ ¼å¼"""
        try:
            # æå–JSONéƒ¨åˆ†
            json_match = re.search(r'```json\s*(.*?)\s*```', response_content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”
                json_str = response_content

            # æ¸…ç†JSONå­—ç¬¦ä¸²
            json_str = self._clean_json_string(json_str)

            # è§£æJSON
            result = json.loads(json_str)

            # éªŒè¯å¿…éœ€å­—æ®µ
            required_fields = ['success', 'scores', 'evidence']
            for field in required_fields:
                if field not in result:
                    result[field] = {} if field in ['scores', 'evidence'] else False

            # éªŒè¯è¯„åˆ†
            if 'scores' in result:
                for trait, score in result['scores'].items():
                    if not isinstance(score, int) or score < 1 or score > 5:
                        result['scores'][trait] = 3  # é»˜è®¤å€¼

            result['segment_number'] = segment_number
            result['raw_response'] = response_content[:500]  # ä¿å­˜éƒ¨åˆ†åŸå§‹å“åº”ç”¨äºè°ƒè¯•

            return result

        except json.JSONDecodeError as e:
            return {
                'success': False,
                'segment_number': segment_number,
                'error': f'JSONè§£æå¤±è´¥: {str(e)}',
                'raw_response': response_content[:200]
            }
        except Exception as e:
            return {
                'success': False,
                'segment_number': segment_number,
                'error': f'è§£æå¤±è´¥: {str(e)}',
                'raw_response': response_content[:200]
            }

    def _clean_json_string(self, json_str: str) -> str:
        """æ¸…ç†JSONå­—ç¬¦ä¸²"""
        import re

        # ç§»é™¤æ§åˆ¶å­—ç¬¦
        json_str = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', json_str)

        # ç§»é™¤å¤šä½™çš„é€—å·
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)

        # ä¿®å¤å¼•å·é—®é¢˜
        json_str = re.sub(r':\s*([^",\[\]\{\}\s][^",\[\]\{\}]*[^",\[\]\{\}\s])(\s*[,}\]])', r': "\1"\2', json_str)

        return json_str

    def get_stats_summary(self) -> Dict:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        success_rate = (self.stats['successful_segments'] / max(1, self.stats['total_segments'])) * 100

        # è®¡ç®—è¯„åˆ†å¤šæ ·æ€§
        total_scores = sum(self.stats['score_distribution'].values())
        score_diversity = len([s for s, c in self.stats['score_distribution'].items() if c > 0])

        return {
            'success_rate': success_rate,
            'total_segments': self.stats['total_segments'],
            'successful_segments': self.stats['successful_segments'],
            'failed_segments': self.stats['failed_segments'],
            'score_distribution': self.stats['score_distribution'],
            'score_diversity': score_diversity,
            'unique_patterns': len(self.stats['unique_score_patterns']),
            'most_common_pattern': self._get_most_common_pattern()
        }

    def _get_most_common_pattern(self) -> str:
        """è·å–æœ€å¸¸è§çš„è¯„åˆ†æ¨¡å¼"""
        if not self.stats['unique_score_patterns']:
            return "N/A"

        # ç®€å•è¿”å›æœ€å¸¸è§çš„æ¨¡å¼
        patterns = list(self.stats['unique_score_patterns'])
        if (3, 3, 3, 3, 3) in patterns:
            return "å…¨3åˆ†æ¨¡å¼"
        else:
            return str(patterns[0]) if patterns else "N/A"

def test_improved_analyzer():
    """æµ‹è¯•æ”¹è¿›çš„åˆ†æå™¨"""
    print("ğŸ§ª æµ‹è¯•æ”¹è¿›ç‰ˆåˆ†æå™¨...")

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_file = "results/results/asses_deepseek_r1_70b_agent_big_five_50_complete2_a10_e0_t0_0_09271.json"

    try:
        with open(test_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        questions = []
        if 'assessment_results' in data and isinstance(data['assessment_results'], list):
            for item in data['assessment_results'][:10]:  # åªæµ‹è¯•å‰10é¢˜
                if isinstance(item, dict) and 'question_data' in item:
                    question_data = item['question_data']
                    if isinstance(question_data, dict):
                        question_text = question_data.get('prompt_for_agent', question_data.get('mapped_ipip_concept', ''))

                        answer_text = ''
                        if 'extracted_response' in item and item['extracted_response']:
                            answer_text = item['extracted_response']
                        elif 'conversation_log' in item and isinstance(item['conversation_log'], list):
                            for msg in item['conversation_log']:
                                if isinstance(msg, dict) and msg.get('role') == 'assistant':
                                    answer_text = msg.get('content', '')
                                    break

                        if question_text and answer_text:
                            questions.append({
                                'question': question_text,
                                'answer': answer_text
                            })

        if len(questions) < 5:
            print("âŒ æµ‹è¯•æ•°æ®ä¸è¶³")
            return

        # åˆ›å»ºæ”¹è¿›ç‰ˆåˆ†æå™¨
        analyzer = ImprovedCloudAnalyzer(model="qwen-max")

        # æµ‹è¯•ä¸åŒåˆ†æ®µå¤§å°
        test_segments = [2, 5]

        for segment_size in test_segments:
            print(f"\nğŸ¯ æµ‹è¯•{segment_size}é¢˜åˆ†æ®µ...")

            segment_results = []
            for i in range(0, min(segment_size * 2, len(questions)), segment_size):
                segment = questions[i:i+segment_size]
                if len(segment) == segment_size:
                    result = analyzer.analyze_segment_improved(segment, i//segment_size + 1)
                    segment_results.append(result)
                    time.sleep(1)  # é¿å…APIé™åˆ¶

            # è¾“å‡ºç»Ÿè®¡
            stats = analyzer.get_stats_summary()
            print(f"ğŸ“Š {segment_size}é¢˜åˆ†æ®µç»Ÿè®¡:")
            print(f"   æˆåŠŸç‡: {stats['success_rate']:.1f}%")
            print(f"   è¯„åˆ†å¤šæ ·æ€§: {stats['score_diversity']}/3")
            print(f"   è¯„åˆ†åˆ†å¸ƒ: {stats['score_distribution']}")
            print(f"   å¸¸è§æ¨¡å¼: {stats['most_common_pattern']}")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_improved_analyzer()