#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
5é¢˜åˆ†æ®µæ‰¹é‡åˆ†æè„šæœ¬ - é‡æ–°åˆ†ææ‰€æœ‰æµ‹è¯„æŠ¥å‘Š
ä½¿ç”¨å·²éªŒè¯çš„1-3-5è¯„åˆ†æ ‡å‡†
"""

import sys
import os
import json
import time
import glob
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import statistics

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['DASHSCOPE_API_KEY'] = 'sk-ded837735b3c44599a9bc138da561c27'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class FiveSegmentAnalyzer:
    def __init__(self, model: str = "qwen-long"):
        self.model = model
        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_segments': 0,
            'successful_segments': 0,
            'failed_segments': 0,
            'score_distribution': {1: 0, 3: 0, 5: 0},
            'processing_start': None,
            'processing_end': None
        }

    def _create_5segment_prompt(self, segment: List[Dict], segment_number: int, total_segments: int) -> str:
        """åˆ›å»º5é¢˜åˆ†æ®µåˆ†ææç¤º"""
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**ä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°å›ç­”è€…å±•ç°çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**å…³é”®æé†’ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…ï¼Œä¸è¦å›ç­”é—®å·é—®é¢˜
- âŒ ä¸è¦æ··æ·†è§’è‰²ï¼Œä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä¸“æ³¨äºåˆ†æå›ç­”ä¸­çš„äººæ ¼ç‰¹å¾
- âœ… å¿½ç•¥è§’è‰²æ‰®æ¼”å†…å®¹ï¼Œä¸“æ³¨å®é™…è¡Œä¸ºå€¾å‘

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ç¨‹åº¦
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æº
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼ˆå¿…é¡»ä½¿ç”¨ï¼‰ï¼š**
- **1åˆ†**ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- **3åˆ†**ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- **5åˆ†**ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼Œç¦æ­¢ä½¿ç”¨2ã€4ç­‰å…¶ä»–æ•°å€¼ï¼**

**ç¬¬{segment_number}æ®µé—®å·å†…å®¹ï¼ˆ{len(segment)}é¢˜/å…±{total_segments}æ®µï¼‰ï¼š**
"""

        for i, item in enumerate(segment, 1):
            prompt += f"""
**é—®é¢˜ {i}:**
{item['question']}

**å›ç­” {i}:**
{item['answer']}

---
"""

        prompt += """
**è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š**
```json
{
  "success": true,
  "segment_number": {segment_number},
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“è¯æ®å¼•ç”¨",
    "conscientiousness": "å…·ä½“è¯æ®å¼•ç”¨",
    "extraversion": "å…·ä½“è¯æ®å¼•ç”¨",
    "agreeableness": "å…·ä½“è¯æ®å¼•ç”¨",
    "neuroticism": "å…·ä½“è¯æ®å¼•ç”¨"
  },
  "confidence": "high/medium/low"
}
```

**å†æ¬¡æé†’ï¼šæ¯ä¸ªè¯„åˆ†å¿…é¡»æ˜¯1ã€3æˆ–5ï¼Œä¸èƒ½ä½¿ç”¨å…¶ä»–æ•°å€¼ï¼**
"""

        return prompt

    def _analyze_segment(self, segment: List[Dict], segment_number: int, total_segments: int) -> Dict:
        """åˆ†æå•ä¸ªåˆ†æ®µ"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )

            prompt = self._create_5segment_prompt(segment, segment_number, total_segments)

            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.1
            )

            content = response.choices[0].message.content
            print(f"  ğŸ“ APIå“åº”é•¿åº¦: {len(content) if content else 0} å­—ç¬¦")

            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not content or content.strip() == "":
                print(f"  âŒ APIå“åº”ä¸ºç©º")
                return {
                    'success': False,
                    'segment_number': segment_number,
                    'error': 'APIå“åº”ä¸ºç©º',
                    'raw_response': 'No content'
                }

            # è§£æJSON - æå–```json```åŒ…è£¹çš„å†…å®¹
            try:
                import re
                print(f"  ğŸ” å°è¯•è§£æJSONå“åº”...")

                # å…ˆå°è¯•åŒ¹é…```json```åŒ…è£¹çš„å†…å®¹
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                    print(f"  âœ… æ‰¾åˆ°```json```åŒ…è£¹çš„å†…å®¹")
                    result = json.loads(json_str)
                else:
                    # å°è¯•åŒ¹é…å•ç‹¬çš„JSONå¯¹è±¡
                    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                        print(f"  âœ… æ‰¾åˆ°JSONå¯¹è±¡")
                        result = json.loads(json_str)
                    else:
                        # å°è¯•ç›´æ¥è§£æ
                        print(f"  âš ï¸ å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”...")
                        result = json.loads(content)

                print(f"  âœ… JSONè§£ææˆåŠŸ")

            except json.JSONDecodeError as e:
                print(f"  âŒ JSONè§£æå¤±è´¥: {str(e)[:100]}")
                print(f"  ğŸ“„ å“åº”å†…å®¹é¢„è§ˆ: {content[:200] if content else 'No content'}...")
                return {
                    'success': False,
                    'segment_number': segment_number,
                    'error': f'JSONè§£æå¤±è´¥: {str(e)[:100]}',
                    'raw_response': content[:500] if content else 'No content'
                }

            # éªŒè¯è¯„åˆ†æ ‡å‡†
            if 'scores' in result:
                invalid_scores = []
                for trait, score in result['scores'].items():
                    if score not in [1, 3, 5]:
                        invalid_scores.append(f"{trait}:{score}")
                        # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                        if score < 2:
                            result['scores'][trait] = 1
                        elif score > 4:
                            result['scores'][trait] = 5
                        else:
                            result['scores'][trait] = 3

                if invalid_scores:
                    print(f"  âš ï¸ å‘ç°å¹¶ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")

            result['model'] = self.model
            result['segment_number'] = segment_number
            result['processing_time'] = time.time()

            return result

        except Exception as e:
            return {
                'success': False,
                'segment_number': segment_number,
                'error': f'åˆ†æå¤±è´¥: {str(e)}',
                'raw_response': str(e)
            }

    def analyze_file(self, file_path: str, output_dir: str) -> Dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        self.stats['total_files'] += 1

        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–é—®é¢˜
            questions = []
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for item in data['assessment_results']:
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', question_data.get('mapped_ipip_concept', ''))

                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']
                            elif 'conversation_log' in item and isinstance(item['conversation_log'], list):
                                for msg in item['conversation_log']:
                                    if isinstance(msg, dict) and msg.get('role') == 'assistant':
                                        answer_text = msg.get('content', '')
                                        break

                            if question_text and answer_text:
                                questions.append({
                                    'question': question_text,
                                    'answer': answer_text
                                })

            if len(questions) < 5:
                raise Exception(f"é—®é¢˜æ•°é‡ä¸è¶³ï¼š{len(questions)}")

            # åˆ†æ®µå¤„ç†ï¼ˆæ¯æ®µ5é¢˜ï¼‰
            segment_size = 5
            segments = []
            for i in range(0, len(questions), segment_size):
                segment = questions[i:i+segment_size]
                if len(segment) == segment_size:
                    segments.append(segment)

            total_segments = len(segments)
            segment_results = []

            print(f"  ğŸ“Š {Path(file_path).name}: {len(questions)}é¢˜ -> {total_segments}ä¸ªåˆ†æ®µ")

            # åˆ†ææ¯ä¸ªåˆ†æ®µ
            for i, segment in enumerate(segments, 1):
                self.stats['total_segments'] += 1

                result = self._analyze_segment(segment, i, total_segments)
                segment_results.append(result)

                if result['success']:
                    self.stats['successful_segments'] += 1
                    # ç»Ÿè®¡è¯„åˆ†åˆ†å¸ƒ
                    for score in result['scores'].values():
                        self.stats['score_distribution'][score] += 1

                    print(f"    âœ… æ®µ{i} ({len(segment)}é¢˜): {list(result['scores'].values())}")
                else:
                    self.stats['failed_segments'] += 1
                    print(f"    âŒ æ®µ{i}: {result.get('error', 'Unknown error')}")

                time.sleep(3)  # APIé™åˆ¶

            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            if segment_results:
                final_scores = {}
                for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                    all_scores = []
                    for result in segment_results:
                        if result['success'] and 'scores' in result:
                            all_scores.append(result['scores'][trait])

                    if all_scores:
                        final_scores[trait] = statistics.median(all_scores)
                        final_scores[trait] = int(final_scores[trait])  # è½¬æ¢ä¸ºæ•´æ•°

                # ç”ŸæˆMBTIç±»å‹
                mbti_type = self._calculate_mbti_type(final_scores)

                # ä¿å­˜ç»“æœ
                output_filename = f"{Path(file_path).stem}_5segment_analysis.json"
                output_path = os.path.join(output_dir, output_filename)

                analysis_result = {
                    "file_info": {
                        "filename": Path(file_path).name,
                        "total_questions": len(questions),
                        "segments_count": total_segments,
                        "questions_per_segment": segment_size,
                        "model_used": self.model,
                        "analysis_date": datetime.now().isoformat()
                    },
                    "segment_results": segment_results,
                    "final_scores": final_scores,
                    "mbti_type": mbti_type,
                    "validation_stats": self._calculate_validation_stats(segment_results),
                    "stats": self._get_stats_summary()
                }

                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(analysis_result, f, ensure_ascii=False, indent=2)

                self.stats['processed_files'] += 1
                print(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_filename}")

                return {
                    'success': True,
                    'file_path': file_path,
                    'output_path': output_path,
                    'total_segments': total_segments,
                    'successful_segments': len([r for r in segment_results if r['success']]),
                    'final_scores': final_scores,
                    'mbti_type': mbti_type
                }
            else:
                raise Exception("æ²¡æœ‰åˆ†æ®µç»“æœ")

        except Exception as e:
            self.stats['failed_files'] += 1
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e)
            }

    def _calculate_mbti_type(self, scores: Dict) -> str:
        """æ ¹æ®Big5è¯„åˆ†è®¡ç®—MBTIç±»å‹"""
        try:
            # Big5åˆ°MBTIçš„æ˜ å°„
            openness = scores.get('openness_to_experience', 3)
            conscientiousness = scores.get('conscientiousness', 3)
            extraversion = scores.get('extraversion', 3)
            agreeableness = scores.get('agreeableness', 3)
            neuroticism = scores.get('neuroticism', 3)

            # I/Eç»´åº¦
            I_E = 'I' if extraversion <= 3 else 'E'

            # S/Nç»´åº¦
            S_N = 'S' if neuroticism <= 3 else 'N'

            # T/Fç»´åº¦ - åŸºäºæ€ç»´ä¸æƒ…æ„Ÿçš„å¹³è¡¡
            T_F = 'T' if conscientiousness >= openness_to_experience else 'F'

            # J/Pç»´åº¦ - åŸºäºç»„ç»‡ä¸é€‚åº”æ€§çš„å¹³è¡¡
            J_P = 'J' if conscientiousness >= 4 and openness_to_experience <= 3 else 'P'

            return f"{I_E}{S_N}{T_F}{J_P}"

        except Exception:
            return "UNKNOWN"

    def _calculate_validation_stats(self, segment_results: List[Dict]) -> Dict:
        """è®¡ç®—éªŒè¯ç»Ÿè®¡"""
        successful_results = [r for r in segment_results if r.get('success', False)]

        if not successful_results:
            return {
                'total_segments': len(segment_results),
                'successful_segments': 0,
                'success_rate': 0.0,
                'score_diversity': 0,
                'all_three_count': 0,
                'avg_score': 0,
                'valid_scores_ratio': 0
            }

        success_rate = len(successful_results) / len(segment_results) * 100

        # è¯„åˆ†å¤šæ ·æ€§
        all_scores = []
        for result in successful_results:
            if 'scores' in result:
                all_scores.extend(result['scores'].values())

        score_diversity = len(set(all_scores))

        # å…¨3åˆ†åˆ†æ®µæ•°é‡
        all_three_count = 0
        for result in successful_results:
            if 'scores' in result:
                scores = result['scores'].values()
                if all(score == 3 for score in scores):
                    all_three_count += 1

        avg_score = sum(all_scores) / len(all_scores) if all_scores else 0

        # 1-3-5è¯„åˆ†æ ‡å‡†ç¬¦åˆç‡
        valid_scores = sum(1 for score in all_scores if score in [1, 3, 5])
        valid_scores_ratio = (valid_scores / len(all_scores) * 100) if all_scores else 0

        return {
            'total_segments': len(segment_results),
            'successful_segments': len(successful_results),
            'success_rate': success_rate,
            'score_diversity': score_diversity,
            'unique_scores': sorted(list(set(all_scores))),
            'all_three_count': all_three_count,
            'avg_score': avg_score,
            'valid_scores_ratio': valid_scores_ratio,
            'credibility_score': self._calculate_credibility_score(success_rate, score_diversity, all_three_count, len(segment_results))
        }

    def _calculate_credibility_score(self, success_rate: float, score_diversity: int, all_three_count: int, total_segments: int) -> int:
        """è®¡ç®—å¯ä¿¡åº¦åˆ†æ•°"""
        if total_segments == 0:
            return 0

        base_score = success_rate
        diversity_bonus = min(score_diversity * 10, 40)
        all_three_penalty = (all_three_count / total_segments) * 50

        final_score = min(100, int(base_score + diversity_bonus - all_three_penalty))
        return max(0, final_score)

    def _get_stats_summary(self) -> Dict:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        success_rate = (self.stats['successful_segments'] / max(1, self.stats['total_segments'])) * 100
        return {
            'total_files': self.stats['total_files'],
            'processed_files': self.stats['processed_files'],
            'failed_files': self.stats['failed_files'],
            'total_segments': self.stats['total_segments'],
            'successful_segments': self.stats['successful_segments'],
            'failed_segments': self.stats['failed_segments'],
            'success_rate': success_rate,
            'score_distribution': self.stats['score_distribution'],
            'processing_time': None
        }

    def batch_analyze(self, input_dir: str, output_dir: str = "5segment_results", file_pattern: str = "*.json", max_files: int = None):
        """æ‰¹é‡åˆ†æ"""
        print(f"ğŸš€ å¼€å§‹5é¢˜åˆ†æ®µæ‰¹é‡åˆ†æ")
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"ğŸ“Š æ¯æ®µå¤§å°: 5é¢˜")
        print(f"âš¡ åˆ†æ®µé—´éš”: 3ç§’")
        print()

        self.stats['processing_start'] = datetime.now()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        file_pattern = os.path.join(input_dir, file_pattern)
        files = glob.glob(file_pattern)

        if max_files:
            files = files[:max_files]

        print(f"ğŸ“Š æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")

        if not files:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
            return

        # æ‰¹é‡å¤„ç†
        batch_results = []

        for i, file_path in enumerate(files, 1):
            print(f"ğŸ“ˆ [{i}/{len(files)}] å¤„ç†: {Path(file_path).name}")

            result = self.analyze_file(file_path, output_dir)
            batch_results.append(result)

            # æ˜¾ç¤ºè¿›åº¦
            successful = len([r for r in batch_results if r.get('success', False)])
            print(f"   æˆåŠŸ: {successful}/{len(batch_results)}")

        # å®Œæˆç»Ÿè®¡
        self.stats['processing_end'] = datetime.now()
        if self.stats['processing_start'] and self.stats['processing_end']:
            self.stats['processing_time'] = (self.stats['processing_end'] - self.stats['processing_start']).total_seconds()

        print()
        print("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ")
        print("=" * 50)
        stats = self._get_stats_summary()
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {stats['total_files']}")
        print(f"âœ… å¤„ç†æˆåŠŸ: {stats['processed_files']}")
        print(f"âŒ å¤„ç†å¤±è´¥: {stats['failed_files']}")
        print(f"ğŸ“Š æ€»åˆ†æ®µæ•°: {stats['total_segments']}")
        print(f"âœ… æˆåŠŸåˆ†æ®µ: {stats['successful_segments']}")
        print(f"âŒ å¤±è´¥åˆ†æ®µ: {stats['failed_segments']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {stats['success_rate']:.1f}%")

        if stats['score_distribution']:
            total_scores = sum(stats['score_distribution'].values())
            print(f"ğŸ“Š è¯„åˆ†åˆ†å¸ƒ: {stats['score_distribution']}")
            print(f"   1åˆ†: {stats['score_distribution'][1]/total_scores:.1f}%")
            print(f"   3åˆ†: {stats['score_distribution'][3]/total_scores:.1f}%")
            print(f"   5åˆ†: {stats['score_distribution'][5]/total_scores:.1f}%")

        # ä¿å­˜æ‰¹é‡å¤„ç†æŠ¥å‘Š
        batch_report = {
            "batch_info": {
                "model": self.model,
                "segment_size": 5,
                "processing_date": datetime.now().isoformat(),
                "processing_time": stats.get('processing_time')
            },
            "input_files": files,
            "results": batch_results,
            "stats": stats
        }

        with open(os.path.join(output_dir, "batch_5segment_report.json"), 'w', encoding='utf-8') as f:
            json.dump(batch_report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ æ‰¹é‡æŠ¥å‘Šå·²ä¿å­˜: batch_5segment_report.json")

        return batch_report

def main():
    """ä¸»å‡½æ•°"""
    analyzer = FiveSegmentAnalyzer(model="qwen-long")

    # è¾“å…¥è¾“å‡ºç›®å½•
    input_dir = "results/results"
    output_dir = "5segment_results"

    # æ‰¹é‡åˆ†æ
    analyzer.batch_analyze(input_dir, output_dir, max_files=10)  # å…ˆå¤„ç†10ä¸ªæ–‡ä»¶æµ‹è¯•

if __name__ == "__main__":
    main()