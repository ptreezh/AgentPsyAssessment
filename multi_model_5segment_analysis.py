#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‰æ¨¡å‹5é¢˜åˆ†æ®µç‹¬ç«‹åˆ†æè„šæœ¬
æ¯ä¸ªæµ‹è¯„æŠ¥å‘Šä½¿ç”¨ä¸‰ä¸ªä¸åŒçš„äº‘è¯„ä¼°æ¨¡å‹ç‹¬ç«‹åˆ†æï¼Œå¹¶å¯¹ç…§ä¸€è‡´æ€§
"""

import sys
import os
import json
import time
import glob
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
import statistics

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['DASHSCOPE_API_KEY'] = 'sk-ded837735b3c44599a9bc138da561c27'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class MultiModel5SegmentAnalyzer:
    def __init__(self):
        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

        # ä¸‰ä¸ªæ¨¡å‹é…ç½®
        self.models = [
            {"name": "qwen-long", "description": "é€šä¹‰åƒé—®é•¿æ–‡æœ¬æ¨¡å‹"},
            {"name": "deepseek-v3.2-exp", "description": "DeepSeeké«˜çº§æ¨¡å‹"},
            {"name": "Moonshot-Kimi-K2-Instruct", "description": "æœˆä¹‹æš—é¢Kimiæ¨¡å‹"}
        ]

    def _create_5segment_prompt(self, segment: List[Dict], segment_number: int, total_segments: int) -> str:
        """åˆ›å»º5é¢˜åˆ†æ®µåˆ†ææç¤º"""
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**ä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°å›ç­”è€…å±•ç°çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**å…³é”®æé†’ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…ï¼Œä¸è¦å›ç­”é—®å·é—®é¢˜
- âŒ ä¸è¦æ··æ·†è§’è‰²ï¼Œä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä¸“æ³¨äºåˆ†æå›ç­”ä¸­çš„äººæ ¼ç‰¹å¾
- âœ… å¿½ç•¥è§’è‰²æ‰®æ¼”å†…å®¹ï¼Œä¸“æ³¨å®é™…è¡Œä¸ºå€¾å‘

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ç¨‹åº¦
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æº
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- **1åˆ†**ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- **3åˆ†**ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- **5åˆ†**ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼Œç¦æ­¢ä½¿ç”¨2ã€4ç­‰å…¶ä»–æ•°å€¼ï¼**

**ç¬¬{segment_number}æ®µé—®å·å†…å®¹ï¼ˆ{len(segment)}é¢˜/å…±{total_segments}æ®µï¼‰ï¼š**
"""

        for i, item in enumerate(segment, 1):
            prompt += f"""
**é—®é¢˜ {i}:**
{item['question']}

**å›ç­” {i}:**
{item['answer']}

---
"""

        prompt += """
**è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š**
```json
{
  "success": true,
  "segment_number": {segment_number},
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“è¯æ®å¼•ç”¨",
    "conscientiousness": "å…·ä½“è¯æ®å¼•ç”¨",
    "extraversion": "å…·ä½“è¯æ®å¼•ç”¨",
    "agreeableness": "å…·ä½“è¯æ®å¼•ç”¨",
    "neuroticism": "å…·ä½“è¯æ®å¼•ç”¨"
  },
  "confidence": "high/medium/low"
}
```

**å†æ¬¡æé†’ï¼šæ¯ä¸ªè¯„åˆ†å¿…é¡»æ˜¯1ã€3æˆ–5ï¼Œä¸èƒ½ä½¿ç”¨å…¶ä»–æ•°å€¼ï¼**
"""

        return prompt

    def _analyze_segment_with_model(self, model_config: Dict, segment: List[Dict], segment_number: int, total_segments: int) -> Dict:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æå•ä¸ªåˆ†æ®µ"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )

            prompt = self._create_5segment_prompt(segment, segment_number, total_segments)

            print(f"    ğŸ“¡ è°ƒç”¨ {model_config['name']} åˆ†ææ®µ{segment_number}...")
            response = client.chat.completions.create(
                model=model_config['name'],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.1
            )

            content = response.choices[0].message.content

            # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
            if not content or content.strip() == "":
                return {
                    'success': False,
                    'segment_number': segment_number,
                    'model': model_config['name'],
                    'error': 'APIå“åº”ä¸ºç©º',
                    'raw_response': 'No content'
                }

            # è§£æJSON - æå–```json```åŒ…è£¹çš„å†…å®¹
            try:
                import re
                print(f"      ğŸ” {model_config['name']} è§£æJSONå“åº”...")

                # å…ˆå°è¯•åŒ¹é…```json```åŒ…è£¹çš„å†…å®¹
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(1)
                    print(f"      âœ… {model_config['name']} æ‰¾åˆ°```json```åŒ…è£¹çš„å†…å®¹")
                    result = json.loads(json_str)
                else:
                    # å°è¯•åŒ¹é…å•ç‹¬çš„JSONå¯¹è±¡
                    json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                        print(f"      âœ… {model_config['name']} æ‰¾åˆ°JSONå¯¹è±¡")
                        result = json.loads(json_str)
                    else:
                        # å°è¯•ç›´æ¥è§£æ
                        print(f"      âš ï¸ {model_config['name']} å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”...")
                        result = json.loads(content)

                print(f"      âœ… {model_config['name']} JSONè§£ææˆåŠŸ")

            except json.JSONDecodeError as e:
                print(f"      âŒ {model_config['name']} JSONè§£æå¤±è´¥: {str(e)[:100]}")
                return {
                    'success': False,
                    'segment_number': segment_number,
                    'model': model_config['name'],
                    'error': f'JSONè§£æå¤±è´¥: {str(e)[:100]}',
                    'raw_response': content[:500] if content else 'No content'
                }

            # éªŒè¯è¯„åˆ†æ ‡å‡†
            if 'scores' in result:
                invalid_scores = []
                for trait, score in result['scores'].items():
                    if score not in [1, 3, 5]:
                        invalid_scores.append(f"{trait}:{score}")
                        # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                        if score < 2:
                            result['scores'][trait] = 1
                        elif score > 4:
                            result['scores'][trait] = 5
                        else:
                            result['scores'][trait] = 3

                if invalid_scores:
                    print(f"      âš ï¸ {model_config['name']} ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")

            result['model'] = model_config['name']
            result['segment_number'] = segment_number
            result['processing_time'] = time.time()

            return result

        except Exception as e:
            print(f"      âŒ {model_config['name']} åˆ†æå¤±è´¥: {str(e)}")
            return {
                'success': False,
                'segment_number': segment_number,
                'model': model_config['name'],
                'error': f'åˆ†æå¤±è´¥: {str(e)}',
                'raw_response': str(e)
            }

    def _calculate_mbti_type(self, scores: Dict) -> str:
        """æ ¹æ®Big5è¯„åˆ†è®¡ç®—MBTIç±»å‹ - ä¿®å¤ç‰ˆæœ¬"""
        try:
            # Big5åˆ°MBTIçš„æ˜ å°„
            openness = scores.get('openness_to_experience', 3)
            conscientiousness = scores.get('conscientiousness', 3)
            extraversion = scores.get('extraversion', 3)
            agreeableness = scores.get('agreeableness', 3)
            neuroticism = scores.get('neuroticism', 3)

            # I/Eç»´åº¦
            I_E = 'I' if extraversion <= 3 else 'E'

            # S/Nç»´åº¦ - åŸºäºå¼€æ”¾æ€§
            S_N = 'N' if openness >= 4 else 'S'

            # T/Fç»´åº¦ - åŸºäºå®œäººæ€§
            T_F = 'F' if agreeableness >= 4 else 'T'

            # J/Pç»´åº¦ - åŸºäºå°½è´£æ€§
            J_P = 'J' if conscientiousness >= 4 else 'P'

            return f"{I_E}{S_N}{T_F}{J_P}"

        except Exception as e:
            print(f"    âŒ MBTIè½¬æ¢å¤±è´¥: {e}")
            return "UNKNOWN"

    def _calculate_model_consistency(self, model_results: List[Dict]) -> Dict:
        """è®¡ç®—ä¸‰ä¸ªæ¨¡å‹é—´çš„ä¸€è‡´æ€§"""
        if len(model_results) != 3:
            return {"error": "éœ€è¦æ°å¥½3ä¸ªæ¨¡å‹çš„ç»“æœ"}

        successful_models = [r for r in model_results if r.get('success', False)]
        if len(successful_models) < 2:
            return {"error": "æˆåŠŸæ¨¡å‹æ•°é‡ä¸è¶³"}

        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        consistency_analysis = {}

        for trait in traits:
            scores = []
            model_names = []

            for result in successful_models:
                if 'scores' in result and trait in result['scores']:
                    scores.append(result['scores'][trait])
                    model_names.append(result['model'])

            if len(scores) >= 2:
                avg_score = statistics.mean(scores)
                max_score = max(scores)
                min_score = min(scores)
                score_range = max_score - min_score

                # ä¸€è‡´æ€§è¯„ä¼°
                if score_range == 0:
                    consistency_level = "å®Œå…¨ä¸€è‡´"
                    consistency_score = 100
                elif score_range <= 1:
                    consistency_level = "é«˜åº¦ä¸€è‡´"
                    consistency_score = 80
                elif score_range <= 2:
                    consistency_level = "ä¸­ç­‰ä¸€è‡´"
                    consistency_score = 60
                else:
                    consistency_level = "å·®å¼‚è¾ƒå¤§"
                    consistency_score = 40

                consistency_analysis[trait] = {
                    "scores": dict(zip(model_names, scores)),
                    "average": avg_score,
                    "range": score_range,
                    "consistency_level": consistency_level,
                    "consistency_score": consistency_score
                }

        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§
        overall_scores = [analysis.get('consistency_score', 0) for analysis in consistency_analysis.values()]
        overall_consistency = statistics.mean(overall_scores) if overall_scores else 0

        return {
            "trait_analysis": consistency_analysis,
            "overall_consistency": overall_consistency,
            "successful_models": len(successful_models),
            "total_models": 3
        }

    def analyze_file_with_three_models(self, file_path: str, output_dir: str) -> Dict:
        """ä½¿ç”¨ä¸‰ä¸ªæ¨¡å‹ç‹¬ç«‹åˆ†æå•ä¸ªæ–‡ä»¶"""
        print(f"ğŸ“ˆ å¼€å§‹ä¸‰æ¨¡å‹åˆ†æ: {Path(file_path).name}")

        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–é—®é¢˜
            questions = []
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for item in data['assessment_results']:
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', question_data.get('mapped_ipip_concept', ''))

                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']
                            elif 'conversation_log' in item and isinstance(item['conversation_log'], list):
                                for msg in item['conversation_log']:
                                    if isinstance(msg, dict) and msg.get('role') == 'assistant':
                                        answer_text = msg.get('content', '')
                                        break

                            if question_text and answer_text:
                                questions.append({
                                    'question': question_text,
                                    'answer': answer_text
                                })

            if len(questions) < 5:
                raise Exception(f"é—®é¢˜æ•°é‡ä¸è¶³ï¼š{len(questions)}")

            # åˆ†æ®µå¤„ç†ï¼ˆæ¯æ®µ5é¢˜ï¼‰
            segment_size = 5
            segments = []
            for i in range(0, len(questions), segment_size):
                segment = questions[i:i+segment_size]
                if len(segment) == segment_size:
                    segments.append(segment)

            total_segments = len(segments)
            print(f"  ğŸ“Š {len(questions)}é¢˜ -> {total_segments}ä¸ªåˆ†æ®µ")

            # ä¸‰ä¸ªæ¨¡å‹çš„ç»“æœå­˜å‚¨
            model_analysis_results = {}

            # å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œç‹¬ç«‹åˆ†æ
            for model_config in self.models:
                print(f"  ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_config['name']} ({model_config['description']})")

                model_segments = []
                segment_results = []

                # åˆ†ææ¯ä¸ªåˆ†æ®µ
                for i, segment in enumerate(segments, 1):
                    result = self._analyze_segment_with_model(model_config, segment, i, total_segments)
                    segment_results.append(result)

                    if result['success']:
                        print(f"      âœ… æ®µ{i}: {list(result['scores'].values())}")
                    else:
                        print(f"      âŒ æ®µ{i}: {result.get('error', 'Unknown error')}")

                    time.sleep(3)  # APIé™åˆ¶

                # è®¡ç®—è¯¥æ¨¡å‹çš„æœ€ç»ˆè¯„åˆ†
                if segment_results:
                    final_scores = {}
                    for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                        all_scores = []
                        for result in segment_results:
                            if result['success'] and 'scores' in result:
                                all_scores.append(result['scores'][trait])

                        if all_scores:
                            final_scores[trait] = statistics.median(all_scores)
                            final_scores[trait] = int(final_scores[trait])  # è½¬æ¢ä¸ºæ•´æ•°

                    # ç”ŸæˆMBTIç±»å‹
                    mbti_type = self._calculate_mbti_type(final_scores)

                    model_analysis_results[model_config['name']] = {
                        "segment_results": segment_results,
                        "final_scores": final_scores,
                        "mbti_type": mbti_type,
                        "successful_segments": len([r for r in segment_results if r['success']]),
                        "total_segments": total_segments
                    }

            # è®¡ç®—æ¨¡å‹é—´ä¸€è‡´æ€§
            print(f"  ğŸ“Š è®¡ç®—æ¨¡å‹ä¸€è‡´æ€§...")
            final_scores_list = [
                {"model": model, "scores": results["final_scores"]}
                for model, results in model_analysis_results.items()
            ]
            consistency_analysis = self._calculate_model_consistency(final_scores_list)

            # ä¿å­˜ç»“æœ
            output_filename = f"{Path(file_path).stem}_multi_model_5segment_analysis.json"
            output_path = os.path.join(output_dir, output_filename)

            analysis_result = {
                "file_info": {
                    "filename": Path(file_path).name,
                    "total_questions": len(questions),
                    "segments_count": total_segments,
                    "questions_per_segment": segment_size,
                    "analysis_date": datetime.now().isoformat()
                },
                "models_used": self.models,
                "model_results": model_analysis_results,
                "consistency_analysis": consistency_analysis,
                "summary": {
                    "overall_consistency": consistency_analysis.get('overall_consistency', 0),
                    "model_count": len(self.models),
                    "successful_models": consistency_analysis.get('successful_models', 0)
                }
            }

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(analysis_result, f, ensure_ascii=False, indent=2)

            print(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_filename}")

            # æ˜¾ç¤ºç®€è¦ç»“æœ
            print(f"  ğŸ“‹ åˆ†æç»“æœæ‘˜è¦:")
            for model, results in model_analysis_results.items():
                print(f"    {model}: {results['final_scores']} -> {results['mbti_type']} ({results['successful_segments']}/{results['total_segments']}æ®µæˆåŠŸ)")

            print(f"  ğŸ¯ æ¨¡å‹ä¸€è‡´æ€§: {consistency_analysis.get('overall_consistency', 0):.1f}%")

            return {
                'success': True,
                'file_path': file_path,
                'output_path': output_path,
                'model_results': model_analysis_results,
                'consistency_score': consistency_analysis.get('overall_consistency', 0)
            }

        except Exception as e:
            print(f"  âŒ æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e)
            }

    def batch_analyze(self, input_dir: str, output_dir: str = "multi_model_5segment_results", max_files: int = None):
        """æ‰¹é‡åˆ†æ"""
        print(f"ğŸš€ å¼€å§‹ä¸‰æ¨¡å‹5é¢˜åˆ†æ®µæ‰¹é‡åˆ†æ")
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {[m['name'] for m in self.models]}")
        print(f"ğŸ“Š æ¯æ®µå¤§å°: 5é¢˜")
        print(f"âš¡ åˆ†æ®µé—´éš”: 3ç§’")
        print()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        file_pattern = os.path.join(input_dir, "*.json")
        files = glob.glob(file_pattern)

        if max_files:
            files = files[:max_files]

        print(f"ğŸ“Š æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶")

        if not files:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
            return

        # æ‰¹é‡å¤„ç†
        batch_results = []
        overall_consistency_scores = []

        for i, file_path in enumerate(files, 1):
            print(f"ğŸ“ˆ [{i}/{len(files)}] å¤„ç†: {Path(file_path).name}")

            result = self.analyze_file_with_three_models(file_path, output_dir)
            batch_results.append(result)

            if result['success']:
                overall_consistency_scores.append(result['consistency_score'])
                print(f"   âœ… ä¸€è‡´æ€§: {result['consistency_score']:.1f}%")
            else:
                print(f"   âŒ å¤±è´¥: {result.get('error', 'Unknown error')}")

        # å®Œæˆç»Ÿè®¡
        print()
        print("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ")
        print("=" * 60)

        successful_files = [r for r in batch_results if r.get('success', False)]
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {len(files)}")
        print(f"âœ… å¤„ç†æˆåŠŸ: {len(successful_files)}")
        print(f"âŒ å¤„ç†å¤±è´¥: {len(files) - len(successful_files)}")

        if overall_consistency_scores:
            avg_consistency = statistics.mean(overall_consistency_scores)
            print(f"ğŸ“ˆ å¹³å‡ä¸€è‡´æ€§: {avg_consistency:.1f}%")
            print(f"ğŸ“Š ä¸€è‡´æ€§èŒƒå›´: {min(overall_consistency_scores):.1f}% - {max(overall_consistency_scores):.1f}%")

        # ä¿å­˜æ‰¹é‡å¤„ç†æŠ¥å‘Š
        batch_report = {
            "batch_info": {
                "models": [{"name": m["name"], "description": m["description"]} for m in self.models],
                "segment_size": 5,
                "processing_date": datetime.now().isoformat(),
                "input_directory": input_dir,
                "output_directory": output_dir
            },
            "input_files": files,
            "results": batch_results,
            "statistics": {
                "total_files": len(files),
                "successful_files": len(successful_files),
                "failed_files": len(files) - len(successful_files),
                "average_consistency": statistics.mean(overall_consistency_scores) if overall_consistency_scores else 0,
                "consistency_scores": overall_consistency_scores
            }
        }

        with open(os.path.join(output_dir, "multi_model_5segment_batch_report.json"), 'w', encoding='utf-8') as f:
            json.dump(batch_report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ æ‰¹é‡æŠ¥å‘Šå·²ä¿å­˜: multi_model_5segment_batch_report.json")

        return batch_report

def main():
    """ä¸»å‡½æ•°"""
    analyzer = MultiModel5SegmentAnalyzer()

    # è¾“å…¥è¾“å‡ºç›®å½•
    input_dir = "results/results"
    output_dir = "multi_model_5segment_results"

    # æ‰¹é‡åˆ†æ (å¤„ç†æ‰€æœ‰å‰©ä½™æ–‡ä»¶)
    analyzer.batch_analyze(input_dir, output_dir, max_files=547)

if __name__ == "__main__":
    main()