#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å››æ¨¡å‹æ‰¹é‡äº‘è¯„ä¼°å™¨åˆ†æå™¨
ä½¿ç”¨ qwen-max, deepseek-v3.2-exp, Moonshot-Kimi-K2-Instruct, claude-3.5-sonnet
æ”¯æŒæ–­ç‚¹ç»§ç»­åŠŸèƒ½
"""

import json
import sys
import time
import argparse
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from multi_model_confidence_analyzer import MultiModelConfidenceAnalyzer
import hashlib

class BatchFourModelAnalyzer:
    """å››æ¨¡å‹æ‰¹é‡åˆ†æå™¨ - æ”¯æŒæ–­ç‚¹ç»§ç»­"""

    def __init__(self, models: list = None, api_key: str = None, max_workers: int = 1):
        # æš‚æ—¶ç§»é™¤Claude APIï¼Œä½¿ç”¨ä¸‰ä¸ªå¯ç”¨çš„DashScopeæ¨¡å‹
        self.models = models or ["qwen-max", "deepseek-v3.2-exp", "Moonshot-Kimi-K2-Instruct"]
        # å¼ºåˆ¶è®¾ç½®æ–°çš„APIå¯†é’¥
        self.api_key = api_key or "sk-3f16ac9d87e34ca88bf3925c3651624f"
        self.max_workers = max_workers
        self.results = []

        # æ–­ç‚¹ç»§ç»­ç›¸å…³
        self.progress_file = Path("batch_four_model_progress.json")
        self.completed_files = set()
        self.failed_files = set()
        self.start_time = None

    def load_progress(self):
        """åŠ è½½æ–­ç‚¹ç»§ç»­ä¿¡æ¯"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
                    self.completed_files = set(progress_data.get('completed_files', []))
                    self.failed_files = set(progress_data.get('failed_files', []))
                    self.start_time = progress_data.get('start_time')

                print(f"ğŸ“‚ å‘ç°æ–­ç‚¹ç»§ç»­ä¿¡æ¯:")
                print(f"   å·²å®Œæˆ: {len(self.completed_files)} ä¸ªæ–‡ä»¶")
                print(f"   å¤±è´¥: {len(self.failed_files)} ä¸ªæ–‡ä»¶")
                if self.start_time:
                    start_dt = datetime.fromisoformat(self.start_time)
                    print(f"   å¼€å§‹æ—¶é—´: {start_dt.strftime('%Y-%m-%d %H:%M:%S')}")
                return True
            except Exception as e:
                print(f"âš ï¸  æ— æ³•åŠ è½½æ–­ç‚¹ä¿¡æ¯: {e}")
                return False
        return False

    def save_progress(self):
        """ä¿å­˜æ–­ç‚¹ç»§ç»­ä¿¡æ¯"""
        progress_data = {
            'models': self.models,
            'completed_files': list(self.completed_files),
            'failed_files': list(self.failed_files),
            'start_time': self.start_time or datetime.now().isoformat(),
            'last_update': datetime.now().isoformat(),
            'total_processed': len(self.completed_files) + len(self.failed_files)
        }

        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜æ–­ç‚¹ä¿¡æ¯å¤±è´¥: {e}")

    def analyze_single_file(self, input_file: Path, output_dir: Path) -> dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„å››æ¨¡å‹ç½®ä¿¡åº¦"""
        file_hash = hashlib.md5(str(input_file).encode('utf-8')).hexdigest()[:8]

        try:
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
            if str(input_file) in self.completed_files:
                # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
                model_output_dir = output_dir / "multi_model_results"
                summary_file = model_output_dir / f"{input_file.stem}_multi_model_confidence.json"

                if summary_file.exists():
                    print(f"â­ï¸  è·³è¿‡å·²å®Œæˆæ–‡ä»¶: {input_file.name}")
                    try:
                        with open(summary_file, 'r', encoding='utf-8') as f:
                            existing_result = json.load(f)
                        return {
                            'file': str(input_file),
                            'success': True,
                            'skipped': True,
                            'existing_result': existing_result
                        }
                    except:
                        print(f"âš ï¸  å·²å®Œæˆæ–‡ä»¶ç»“æœæŸåï¼Œé‡æ–°åˆ†æ: {input_file.name}")
                else:
                    print(f"âš ï¸  æ ‡è®°å®Œæˆä½†ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œé‡æ–°åˆ†æ: {input_file.name}")

            print(f"ğŸ” å¼€å§‹åˆ†æ: {input_file.name} [{file_hash}]")

            # åˆ›å»ºå¤šæ¨¡å‹åˆ†æå™¨
            analyzer = MultiModelConfidenceAnalyzer(
                models=self.models,
                api_key=self.api_key
            )

            # æ‰§è¡Œå››æ¨¡å‹åˆ†æ
            result = analyzer.analyze_with_multiple_models(input_file, output_dir)

            if result['success']:
                confidence = result['confidence_analysis']['overall_confidence']
                successful_models = result['confidence_analysis']['successful_models']

                # ç”Ÿæˆç®€åŒ–æ‘˜è¦
                summary = {
                    'file': str(input_file),
                    'file_hash': file_hash,
                    'success': True,
                    'overall_confidence': confidence,
                    'successful_models': successful_models,
                    'total_models_attempted': len(self.models),
                    'big5_confidence': result['confidence_analysis']['big5_confidence'],
                    'mbti_confidence': result['confidence_analysis']['mbti_confidence'],
                    'most_common_mbti': result['confidence_analysis']['mbti_confidence'].get('most_common_type', 'N/A'),
                    'agreement_level': result['confidence_analysis']['mbti_confidence'].get('agreement_level', 'N/A'),
                    'analysis_timestamp': datetime.now().isoformat()
                }

                # è·å–ä»£è¡¨æ€§è¯„åˆ†ï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸæ¨¡å‹ï¼‰
                if successful_models:
                    first_success_model = successful_models[0]
                    model_result = result['multi_model_results'][first_success_model]
                    summary['representative_big5_scores'] = model_result['big5_scores']
                    summary['representative_mbti'] = model_result['mbti_type']

                big5_str = ""
                if 'representative_big5_scores' in summary:
                    big5_str = ", ".join([f"{trait[0].upper()}:{score}" for trait, score in summary['representative_big5_scores'].items()])

                print(f"âœ… {input_file.name} - ç½®ä¿¡åº¦: {confidence}% - MBTI: {summary.get('representative_mbti', 'N/A')} ({summary['agreement_level']})")

                # æ ‡è®°ä¸ºå·²å®Œæˆ
                self.completed_files.add(str(input_file))

                return summary
            else:
                print(f"âŒ {input_file.name} - å››æ¨¡å‹åˆ†æå¤±è´¥")

                # æ ‡è®°ä¸ºå¤±è´¥
                self.failed_files.add(str(input_file))

                return {
                    'file': str(input_file),
                    'success': False,
                    'error': 'Multi-model analysis failed',
                    'models_attempted': self.models
                }

        except Exception as e:
            print(f"ğŸ’¥ {input_file.name} - å¼‚å¸¸: {e}")

            # æ ‡è®°ä¸ºå¤±è´¥
            self.failed_files.add(str(input_file))

            return {
                'file': str(input_file),
                'success': False,
                'error': str(e),
                'models_attempted': self.models
            }

    def analyze_batch(self, input_files: list[Path], output_dir: Path,
                     progress_callback=None, delay_between_files: int = 15) -> list[dict]:
        """æ‰¹é‡åˆ†ææ–‡ä»¶çš„å››æ¨¡å‹ç½®ä¿¡åº¦"""
        print(f"ğŸš€ å¼€å§‹å››æ¨¡å‹æ‰¹é‡äº‘è¯„ä¼°åˆ†æ")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {len(input_files)} ä¸ª")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {', '.join(self.models)}")
        print(f"âš¡ å¹¶å‘æ•°: {self.max_workers}")
        print(f"â±ï¸  æ–‡ä»¶é—´å»¶è¿Ÿ: {delay_between_files} ç§’")
        print(f"ğŸ”„ æ”¯æŒæ–­ç‚¹ç»§ç»­: âœ…")

        output_dir.mkdir(parents=True, exist_ok=True)

        # åŠ è½½æ–­ç‚¹ç»§ç»­ä¿¡æ¯
        has_progress = self.load_progress()
        if has_progress:
            # è¿‡æ»¤å·²å®Œæˆçš„æ–‡ä»¶
            remaining_files = [f for f in input_files if str(f) not in self.completed_files]
            print(f"ğŸ“Š è¿‡æ»¤åå‰©ä½™æ–‡ä»¶: {len(remaining_files)} ä¸ª")
        else:
            remaining_files = input_files
            self.start_time = datetime.now().isoformat()

        if not remaining_files:
            print("âœ… æ‰€æœ‰æ–‡ä»¶å·²å®Œæˆåˆ†æ")
            return []

        results = []
        completed = 0

        # ä½¿ç”¨å•çº¿ç¨‹å¤„ç†ä»¥é¿å…APIé™åˆ¶
        for i, file in enumerate(remaining_files, 1):
            print(f"\nğŸ“ˆ è¿›åº¦: [{i}/{len(remaining_files)}] å‰©ä½™: {len(remaining_files) - i}")

            try:
                result = self.analyze_single_file(file, output_dir)
                results.append(result)
                completed += 1

                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress_callback(len(self.completed_files) + len(self.failed_files), len(input_files), result)

                # ä¿å­˜æ–­ç‚¹ä¿¡æ¯
                self.save_progress()

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i < len(remaining_files):
                    print(f"â³ ç­‰å¾… {delay_between_files} ç§’åå¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶...")
                    time.sleep(delay_between_files)

            except KeyboardInterrupt:
                print(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
                self.save_progress()
                print("ğŸ’¾ è¿›åº¦å·²ä¿å­˜ï¼Œå¯ä½¿ç”¨ç›¸åŒå‘½ä»¤ç»§ç»­")
                break
            except Exception as e:
                print(f"ğŸ’¥ å¤„ç† {file.name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                self.failed_files.add(str(file))
                self.save_progress()

        return results

    def generate_summary_report(self, results: list[dict], output_dir: Path):
        """ç”Ÿæˆå››æ¨¡å‹æ‰¹é‡åˆ†ææ±‡æ€»æŠ¥å‘Š"""
        successful_results = [r for r in results if r['success'] and not r.get('skipped', False)]
        failed_results = [r for r in results if not r['success']]
        skipped_results = [r for r in results if r.get('skipped', False)]

        # ç»Ÿè®¡æ•°æ®
        overall_confidences = [r['overall_confidence'] for r in successful_results if 'overall_confidence' in r]
        avg_confidence = sum(overall_confidences) / len(overall_confidences) if overall_confidences else 0

        # ç»Ÿè®¡Big5è¯„åˆ†åˆ†å¸ƒ
        big5_stats = {}
        mbti_stats = {}
        confidence_distribution = {'é«˜åº¦ä¸€è‡´': 0, 'ä¸­ç­‰ä¸€è‡´': 0, 'ä½åº¦ä¸€è‡´': 0, 'ä¸ä¸€è‡´': 0}

        for result in successful_results:
            # Big5ç»Ÿè®¡
            if 'representative_big5_scores' in result:
                for trait, score in result['representative_big5_scores'].items():
                    if trait not in big5_stats:
                        big5_stats[trait] = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
                    big5_stats[trait][score] += 1

            # MBTIç»Ÿè®¡
            if 'representative_mbti' in result:
                mbti_type = result['representative_mbti']
                if mbti_type not in mbti_stats:
                    mbti_stats[mbti_type] = 0
                mbti_stats[mbti_type] += 1

            # ç½®ä¿¡åº¦åˆ†å¸ƒç»Ÿè®¡
            agreement_level = result.get('agreement_level', 'ä¸ä¸€è‡´')
            if agreement_level in confidence_distribution:
                confidence_distribution[agreement_level] += 1

        # ç”Ÿæˆæ±‡æ€»æ•°æ®
        summary = {
            'summary': {
                'total_files': len(results),
                'successful': len(successful_results),
                'failed': len(failed_results),
                'skipped': len(skipped_results),
                'success_rate': len(successful_results) / len(results) * 100 if results else 0,
                'models_used': self.models,
                'analysis_timestamp': datetime.now().isoformat(),
                'algorithm_version': 'four_model_confidence_v1.0',
                'start_time': self.start_time,
                'completion_time': datetime.now().isoformat()
            },
            'confidence_statistics': {
                'average_confidence': round(avg_confidence, 1),
                'confidence_distribution': confidence_distribution,
                'high_confidence_count': confidence_distribution['é«˜åº¦ä¸€è‡´'],
                'medium_confidence_count': confidence_distribution['ä¸­ç­‰ä¸€è‡´'],
                'low_confidence_count': confidence_distribution['ä½åº¦ä¸€è‡´'],
                'inconsistent_count': confidence_distribution['ä¸ä¸€è‡´']
            },
            'big5_distribution': big5_stats,
            'mbti_distribution': mbti_stats,
            'detailed_results': results
        }

        # ä¿å­˜JSONæ±‡æ€»
        json_summary = output_dir / f"batch_four_model_summary.json"
        with open(json_summary, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self.generate_markdown_report(summary, output_dir)

        print(f"\nğŸ“Š å››æ¨¡å‹æ‰¹é‡åˆ†ææ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"   JSON: {json_summary}")
        print(f"   Markdown: {output_dir / 'batch_four_model_report.md'}")

    def generate_markdown_report(self, summary: dict, output_dir: Path):
        """ç”ŸæˆMarkdownæ ¼å¼çš„å››æ¨¡å‹æ‰¹é‡åˆ†ææŠ¥å‘Š"""
        summary_stats = summary['summary']
        confidence_stats = summary['confidence_statistics']
        big5_stats = summary['big5_distribution']
        mbti_stats = summary['mbti_distribution']

        md_content = f"""# å››æ¨¡å‹æ‰¹é‡äº‘è¯„ä¼°åˆ†ææŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **åˆ†ææ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ä½¿ç”¨æ¨¡å‹:** {', '.join(summary_stats['models_used'])}
- **ç®—æ³•ç‰ˆæœ¬:** four_model_confidence_v1.0
- **è¯„åˆ†æ ‡å‡†:** ä¸¥æ ¼1-3-5è¯„åˆ† (1=ä½, 3=ä¸­, 5=é«˜)
- **ç½®ä¿¡åº¦è®¡ç®—:** åŸºäºå››æ¨¡å‹é—´ä¸€è‡´æ€§
- **æ–­ç‚¹ç»§ç»­:** æ”¯æŒ

## æ±‡æ€»ç»Ÿè®¡

- **æ€»æ–‡ä»¶æ•°:** {summary_stats['total_files']}
- **æˆåŠŸåˆ†æ:** {summary_stats['successful']}
- **å¤±è´¥åˆ†æ:** {summary_stats['failed']}
- **è·³è¿‡æ–‡ä»¶:** {summary_stats['skipped']}
- **æˆåŠŸç‡:** {summary_stats['success_rate']:.1f}%

## ç½®ä¿¡åº¦ç»Ÿè®¡

- **å¹³å‡ç½®ä¿¡åº¦:** {confidence_stats['average_confidence']}%
- **é«˜åº¦ä¸€è‡´ (â‰¥80%):** {confidence_stats['high_confidence_count']} ä¸ªæ–‡ä»¶
- **ä¸­ç­‰ä¸€è‡´ (60-79%):** {confidence_stats['medium_confidence_count']} ä¸ªæ–‡ä»¶
- **ä½åº¦ä¸€è‡´ (40-59%):** {confidence_stats['low_confidence_count']} ä¸ªæ–‡ä»¶
- **ä¸ä¸€è‡´ (<40%):** {confidence_stats['inconsistent_count']} ä¸ªæ–‡ä»¶

## Big5è¯„åˆ†åˆ†å¸ƒ

"""

        for trait, scores in big5_stats.items():
            total = sum(scores.values())
            md_content += f"### {trait.replace('_', ' ').title()}\n"
            for score in sorted(scores.keys()):
                count = scores[score]
                if count > 0:
                    percentage = count / total * 100
                    md_content += f"- {score}åˆ†: {count} ({percentage:.1f}%)\n"
            md_content += "\n"

        md_content += "## MBTIç±»å‹åˆ†å¸ƒ\n\n"

        for mbti_type, count in sorted(mbti_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(mbti_stats) * 100 if mbti_stats else 0
            md_content += f"- **{mbti_type}:** {count} ({percentage:.1f}%)\n"

        md_content += "\n## è¯¦ç»†ç»“æœ\n\n"

        for result in summary['detailed_results']:
            if result['success'] and not result.get('skipped', False):
                filename = Path(result['file']).name
                confidence = result.get('overall_confidence', 0)
                mbti = result.get('representative_mbti', 'N/A')
                agreement = result.get('agreement_level', 'N/A')
                models = f"{len(result.get('successful_models', []))}/{result.get('total_models_attempted', 0)}"

                big5_str = ""
                if 'representative_big5_scores' in result:
                    big5_str = " - Big5: " + ", ".join([f"{trait[0].upper()}:{score}" for trait, score in result['representative_big5_scores'].items()])

                md_content += f"- **{filename}** - ç½®ä¿¡åº¦: {confidence}% - MBTI: {mbti} ({agreement}) - æ¨¡å‹: {models}{big5_str}\n"

        if failed_results:
            md_content += "\n## å¤±è´¥çš„æ–‡ä»¶\n\n"
            for result in failed_results:
                filename = Path(result['file']).name
                md_content += f"- **{filename}** - é”™è¯¯: {result.get('error', 'Unknown error')}\n"

        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_file = output_dir / "batch_four_model_report.md"
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å››æ¨¡å‹æ‰¹é‡äº‘è¯„ä¼°Big5åˆ†æ')
    parser.add_argument('input_path', help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--output', default='four_model_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--sample', type=int, help='é‡‡æ ·æ–‡ä»¶æ•°é‡')
    parser.add_argument('--filter', help='æ–‡ä»¶åè¿‡æ»¤æ¨¡å¼')
    parser.add_argument('--delay', type=int, default=20, help='æ–‡ä»¶é—´å»¶è¿Ÿç§’æ•°ï¼ˆå››æ¨¡å‹åˆ†æéœ€è¦æ›´é•¿å»¶è¿Ÿï¼‰')
    parser.add_argument('--resume', action='store_true', help='å¼ºåˆ¶ä»æ–­ç‚¹ç»§ç»­ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰')

    args = parser.parse_args()

    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    input_path = Path(args.input_path)
    if input_path.is_file():
        input_files = [input_path]
    elif input_path.is_dir():
        input_files = list(input_path.glob("*.json"))
    else:
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return

    # åº”ç”¨è¿‡æ»¤å™¨
    if args.filter:
        input_files = [f for f in input_files if args.filter.lower() in f.name.lower()]

    # é‡‡æ ·
    if args.sample:
        input_files = input_files[:args.sample]

    if not input_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
        return

    print(f"ğŸ” æ‰¾åˆ° {len(input_files)} ä¸ªæ–‡ä»¶è¿›è¡Œå››æ¨¡å‹åˆ†æ")

    # åˆ›å»ºä¸‰æ¨¡å‹æ‰¹é‡åˆ†æå™¨ï¼ˆæš‚æ—¶ç§»é™¤æœ‰é—®é¢˜çš„Claude APIï¼‰
    analyzer = BatchFourModelAnalyzer(
        models=["qwen-max", "deepseek-v3.2-exp", "Moonshot-Kimi-K2-Instruct"]
    )

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # æ‰§è¡Œæ‰¹é‡åˆ†æ
    def progress_callback(completed, total, result):
        success_rate = sum(1 for r in analyzer.results if r.get('success', False)) / len(analyzer.results) * 100 if analyzer.results else 0
        confidences = [r.get('confidence_analysis', {}).get('overall_confidence', 0) for r in analyzer.results if r.get('success', False)]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0
        completed_in_batch = len(analyzer.completed_files) + len(analyzer.failed_files)
        print(f"ğŸ“ˆ æ€»è¿›åº¦: {completed_in_batch}/{total} ({completed_in_batch/total*100:.1f}%) - æˆåŠŸç‡: {success_rate:.1f}% - å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.1f}%")

    results = analyzer.analyze_batch(
        input_files,
        output_dir,
        progress_callback=progress_callback,
        delay_between_files=args.delay
    )

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print(f"\nğŸ“‹ ç”Ÿæˆå››æ¨¡å‹æ±‡æ€»æŠ¥å‘Š...")
    analyzer.generate_summary_report(results, output_dir)

    # æœ€ç»ˆç»Ÿè®¡
    successful = sum(1 for r in results if r['success'] and not r.get('skipped', False))
    skipped = sum(1 for r in results if r.get('skipped', False))
    avg_confidence = sum(r.get('overall_confidence', 0) for r in results if r['success'] and not r.get('skipped', False)) / len([r for r in results if r['success'] and not r.get('skipped', False)]) if successful > 0 else 0
    high_confidence = sum(1 for r in results if r['success'] and not r.get('skipped', False) and r.get('overall_confidence', 0) >= 80)

    print(f"\nğŸ‰ å››æ¨¡å‹æ‰¹é‡åˆ†æå®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {successful}/{len(results)} ({successful/len(results)*100:.1f}%)")
    if skipped > 0:
        print(f"â­ï¸  è·³è¿‡: {skipped} ä¸ªæ–‡ä»¶ï¼ˆæ–­ç‚¹ç»§ç»­ï¼‰")
    print(f"ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.1f}%")
    print(f"ğŸ¯ é«˜ç½®ä¿¡åº¦æ–‡ä»¶: {high_confidence} ä¸ª")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {', '.join(analyzer.models)}")
    print(f"ğŸ”„ æ–­ç‚¹æ–‡ä»¶: {analyzer.progress_file}")

if __name__ == "__main__":
    main()