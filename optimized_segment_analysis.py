#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„åˆ†æ®µåˆ†æ - æ”¯æŒä¸´æ—¶æ–‡ä»¶ç¼“å­˜
"""

import sys
import os
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DASHSCOPE_API_KEY'] = 'sk-3f16ac9d87e34ca88bf3925c3651624f'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class OptimizedSegmentAnalyzer:
    def __init__(self, model: str = "qwen-max", cache_dir: str = "segment_cache"):
        from enhanced_cloud_analyzer import EnhancedCloudAnalyzer
        self.analyzer = EnhancedCloudAnalyzer(model=model)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def _get_cache_key(self, questions_text: str, segment_num: int) -> str:
        """ç”Ÿæˆåˆ†æ®µç¼“å­˜é”®"""
        content = f"{questions_text}_{segment_num}_{self.analyzer.model}"
        return hashlib.md5(content.encode()).hexdigest()

    def _get_cache_file(self, cache_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"{cache_key}.json"

    def analyze_segment_with_cache(self, questions_text: str, segment_num: int) -> Dict:
        """å¸¦ç¼“å­˜çš„åˆ†æåˆ†æ®µ"""
        cache_key = self._get_cache_key(questions_text, segment_num)
        cache_file = self._get_cache_file(cache_key)

        # æ£€æŸ¥ç¼“å­˜
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_result = json.load(f)
                print(f"  ğŸ“¦ ä½¿ç”¨ç¼“å­˜: æ®µ {segment_num}")
                return cached_result
            except:
                pass  # ç¼“å­˜æŸåï¼Œé‡æ–°åˆ†æ

        # æ‰§è¡Œåˆ†æ
        print(f"  ğŸ” åˆ†ææ®µ {segment_num} (2 é¢˜)")
        result = self.analyzer._analyze_segment(questions_text, segment_num)

        # ä¿å­˜åˆ°ç¼“å­˜
        if result.get('success', False):
            try:
                with open(cache_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"  ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: æ®µ {segment_num}")
            except Exception as e:
                print(f"  âš ï¸  ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

        return result

    def analyze_file_optimized(self, file_path: Path, output_dir: Path) -> Dict:
        """ä¼˜åŒ–çš„æ–‡ä»¶åˆ†æ"""
        print(f"ğŸš€ ä¼˜åŒ–åˆ†æ: {file_path.name}")

        try:
            # è¯»å–å¹¶é¢„å¤„ç†æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–é—®é¢˜
            questions = []
            for item in data:
                if 'question' in item and 'answer' in item:
                    questions.append({
                        'question': item['question'],
                        'answer': item['answer']
                    })

            if not questions:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é—®é¢˜',
                    'file': str(file_path)
                }

            # åˆ›å»ºåˆ†æ®µï¼ˆæ¯æ®µ2ä¸ªé—®é¢˜ï¼‰
            segments = []
            for i in range(0, len(questions), 2):
                segment_questions = questions[i:i+2]
                segment_text = f"åˆ†æä»¥ä¸‹é—®é¢˜å’Œå›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ï¼š\n\n"
                for j, q in enumerate(segment_questions, 1):
                    segment_text += f"é—®é¢˜{j}: {q['question']}\nå›ç­”: {q['answer']}\n\n"
                segments.append(segment_text)

            print(f"ğŸ“Š æ€»å…± {len(questions)} ä¸ªé—®é¢˜ï¼Œåˆ†ä¸º {len(segments)} ä¸ªåˆ†æ®µ")

            # åˆ†æåˆ†æ®µï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            segment_results = []
            accumulated_scores = {}
            accumulated_evidence = {}

            for i, segment_text in enumerate(segments, 1):
                result = self.analyze_segment_with_cache(segment_text, i)
                segment_results.append(result)

                if result.get('success', False):
                    # ç´¯ç§¯è¯„åˆ†
                    if 'scores' in result:
                        for trait, score in result['scores'].items():
                            if trait not in accumulated_scores:
                                accumulated_scores[trait] = []
                            accumulated_scores[trait].append(score)

                    # ç´¯ç§¯è¯æ®
                    if 'evidence' in result:
                        for trait, evidence_list in result['evidence'].items():
                            if trait not in accumulated_evidence:
                                accumulated_evidence[trait] = []
                            accumulated_evidence[trait].extend(evidence_list)

                    print(f"  âœ… æ®µ {i} åˆ†ææˆåŠŸ")
                else:
                    print(f"  âŒ æ®µ {i} åˆ†æå¤±è´¥: {result.get('error', 'Unknown')}")

            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            final_scores = {}
            for trait, scores in accumulated_scores.items():
                if scores:
                    final_scores[trait] = round(sum(scores) / len(scores))
                else:
                    final_scores[trait] = 3

            # ç”Ÿæˆæ‘˜è¦
            summary = {
                'file_info': {
                    'filename': file_path.name,
                    'total_questions': len(questions),
                    'total_segments': len(segments),
                    'successful_segments': len([r for r in segment_results if r.get('success', False)]),
                    'model': self.analyzer.model
                },
                'final_scores': final_scores,
                'accumulated_evidence': accumulated_evidence,
                'segment_results': segment_results,
                'cache_stats': {
                    'cache_dir': str(self.cache_dir),
                    'cached_files': len(list(self.cache_dir.glob("*.json")))
                }
            }

            # ä¿å­˜ç»“æœ
            output_file = output_dir / f"{file_path.stem}_optimized_summary.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)

            print(f"âœ… ä¼˜åŒ–åˆ†æå®Œæˆ: {output_file}")
            print(f"ğŸ“Š æˆåŠŸç‡: {summary['file_info']['successful_segments']}/{len(segments)} ({summary['file_info']['successful_segments']/len(segments)*100:.1f}%)")
            print(f"ğŸ“¦ ç¼“å­˜æ–‡ä»¶æ•°: {summary['cache_stats']['cached_files']}")

            return {
                'success': True,
                'file': str(file_path),
                'summary': summary,
                'output_file': str(output_file)
            }

        except Exception as e:
            return {
                'success': False,
                'file': str(file_path),
                'error': str(e)
            }

def test_optimized_analysis():
    print("ğŸš€ æµ‹è¯•ä¼˜åŒ–åˆ†æ®µåˆ†æ...")

    try:
        # åˆ›å»ºä¼˜åŒ–åˆ†æå™¨
        analyzer = OptimizedSegmentAnalyzer(model="qwen-max")

        # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
        results_dir = Path("results/results")
        json_files = list(results_dir.glob("*.json"))

        if not json_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
            return

        # é€‰æ‹©ä¸€ä¸ªæœªå¤„ç†çš„æ–‡ä»¶æµ‹è¯•
        test_file = json_files[5] if len(json_files) > 5 else json_files[0]
        print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file.name}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("optimized_test_results")
        output_dir.mkdir(exist_ok=True)

        # è®°å½•æ—¶é—´
        import time
        start_time = time.time()

        # æ‰§è¡Œä¼˜åŒ–åˆ†æ
        result = analyzer.analyze_file_optimized(test_file, output_dir)

        elapsed_time = time.time() - start_time
        print(f"â±ï¸  ä¼˜åŒ–åˆ†æè€—æ—¶: {elapsed_time:.1f} ç§’")

        if result['success']:
            print(f"âœ… ä¼˜åŒ–åˆ†ææˆåŠŸ!")
            success_rate = result['summary']['file_info']['successful_segments'] / result['summary']['file_info']['total_segments'] * 100
            print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"   ç¼“å­˜æ–‡ä»¶æ•°: {result['summary']['cache_stats']['cached_files']}")
        else:
            print(f"âŒ ä¼˜åŒ–åˆ†æå¤±è´¥: {result['error']}")

    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_optimized_analysis()