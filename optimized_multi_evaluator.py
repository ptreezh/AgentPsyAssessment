#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–çš„å¤šè¯„ä¼°å™¨ç³»ç»Ÿ
ä¼˜å…ˆä½¿ç”¨ä¸‰ä¸ªæ ¸å¿ƒè¯„ä¼°å™¨ï¼Œä»…åœ¨éœ€è¦æ—¶å¢åŠ æ›´å¤šè¯„ä¼°å™¨
"""

import argparse
import json
import sys
import time
from typing import Dict, List, Any, Optional
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from enhanced_cloud_analyzer import EnhancedCloudAnalyzer

class OptimizedMultiEvaluator:
    """ä¼˜åŒ–çš„å¤šè¯„ä¼°å™¨ç³»ç»Ÿ"""

    def __init__(self, api_key: str = None):
        # æ ¸å¿ƒè¯„ä¼°å™¨ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
        self.core_evaluators = ["ollama_mistral", "phi3_mini", "qwen3_4b"]
        
        # å¤‡ç”¨è¯„ä¼°å™¨ï¼ˆä»…åœ¨æ ¸å¿ƒè¯„ä¼°å™¨å¤±è´¥æˆ–ä¸ä¸€è‡´æ—¶ä½¿ç”¨ï¼‰
        self.backup_evaluators = ["qwen-long", "deepseek-v3.2-exp", "Moonshot-Kimi-K2-Instruct"]
        
        self.api_key = api_key
        self.results = {}
        self.consensus_threshold = 0.7  # ä¸€è‡´æ€§é˜ˆå€¼

    def evaluate_with_core_models(self, input_file: Path, output_dir: Path) -> Dict:
        """ä½¿ç”¨æ ¸å¿ƒè¯„ä¼°å™¨è¿›è¡Œä¼˜å…ˆè¯„ä¼°"""
        print("ğŸ¯ å¼€å§‹æ ¸å¿ƒè¯„ä¼°å™¨åˆ†æ")
        print(f"ğŸ“Š æ ¸å¿ƒè¯„ä¼°å™¨: {', '.join(self.core_evaluators)}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir.mkdir(parents=True, exist_ok=True)
        
        core_results = {}
        
        # é¦–å…ˆå°è¯•æ‰€æœ‰æ ¸å¿ƒè¯„ä¼°å™¨
        for model in self.core_evaluators:
            print(f"\nğŸ” æ­£åœ¨ä½¿ç”¨æ ¸å¿ƒè¯„ä¼°å™¨ {model}...")
            
            result = self._evaluate_with_single_model(model, input_file, output_dir)
            core_results[model] = result
            
            if result['success']:
                big5_str = ", ".join([f"{trait[0].upper()}:{score}" for trait, score in result['big5_scores'].items()])
                print(f"âœ… {model} - Big5: {big5_str} - MBTI: {result['mbti_type']}")
            else:
                print(f"âŒ {model} è¯„ä¼°å¤±è´¥: {result.get('error', 'Unknown error')}")

        # æ£€æŸ¥æ ¸å¿ƒè¯„ä¼°å™¨çš„ä¸€è‡´æ€§
        consensus_analysis = self._check_core_consensus(core_results)
        
        if consensus_analysis['consensus_achieved']:
            print(f"ğŸ‰ æ ¸å¿ƒè¯„ä¼°å™¨è¾¾æˆå…±è¯† (ä¸€è‡´æ€§: {consensus_analysis['consensus_score']:.2f})")
            return {
                'success': True,
                'results': core_results,
                'consensus_analysis': consensus_analysis,
                'evaluators_used': self.core_evaluators,
                'backup_used': False
            }
        else:
            print(f"âš ï¸ æ ¸å¿ƒè¯„ä¼°å™¨æœªè¾¾æˆå…±è¯† (ä¸€è‡´æ€§: {consensus_analysis['consensus_score']:.2f})")
            print("ğŸ”„ å¯ç”¨å¤‡ç”¨è¯„ä¼°å™¨...")
            return self._evaluate_with_backup_models(input_file, output_dir, core_results)

    def _evaluate_with_single_model(self, model: str, input_file: Path, output_dir: Path) -> Dict:
        """ä½¿ç”¨å•ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°"""
        try:
            analyzer = EnhancedCloudAnalyzer(
                model=model,
                api_key=self.api_key
            )

            if not analyzer.api_available:
                return {
                    'success': False,
                    'error': 'APIä¸å¯ç”¨'
                }

            # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºç‹¬ç«‹ç›®å½•
            model_dir = output_dir / model
            model_dir.mkdir(exist_ok=True)

            # æ‰§è¡Œåˆ†æ
            result = analyzer.analyze_full_assessment(str(input_file), str(model_dir))

            if result['success']:
                final_scores = result.get('final_scores', {})
                mbti_result = result.get('mbti_result', {})
                
                return {
                    'success': True,
                    'big5_scores': {trait: data.get('final_score', 3) for trait, data in final_scores.items()},
                    'mbti_type': mbti_result.get('type', 'Unknown'),
                    'final_scores_detailed': final_scores,
                    'mbti_detailed': mbti_result,
                    'summary_file': result.get('summary_file', 'N/A'),
                    'evidence_file': result.get('evidence_file', 'N/A')
                }
            else:
                return {
                    'success': False,
                    'error': result.get('error', 'Unknown error')
                }

        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def _check_core_consensus(self, core_results: Dict) -> Dict:
        """æ£€æŸ¥æ ¸å¿ƒè¯„ä¼°å™¨ä¹‹é—´çš„ä¸€è‡´æ€§"""
        successful_results = {k: v for k, v in core_results.items() if v['success']}
        
        if len(successful_results) < 2:
            return {
                'consensus_achieved': False,
                'consensus_score': 0.0,
                'successful_count': len(successful_results),
                'total_count': len(self.core_evaluators)
            }

        # è®¡ç®—Big5åˆ†æ•°çš„ä¸€è‡´æ€§
        big5_consensus_scores = []
        models = list(successful_results.keys())
        
        for i in range(len(models)):
            for j in range(i + 1, len(models)):
                model1, model2 = models[i], models[j]
                scores1 = successful_results[model1]['big5_scores']
                scores2 = successful_results[model2]['big5_scores']
                
                # è®¡ç®—ä¸¤ä¸ªæ¨¡å‹é—´çš„ä¸€è‡´æ€§åˆ†æ•°
                consensus = self._calculate_pair_consensus(scores1, scores2)
                big5_consensus_scores.append(consensus)

        # è®¡ç®—MBTIç±»å‹çš„ä¸€è‡´æ€§
        mbti_types = [result['mbti_type'] for result in successful_results.values()]
        mbti_consensus = len(set(mbti_types)) == 1  # æ‰€æœ‰ç±»å‹æ˜¯å¦ç›¸åŒ
        
        overall_consensus = sum(big5_consensus_scores) / len(big5_consensus_scores) if big5_consensus_scores else 0
        
        return {
            'consensus_achieved': overall_consensus >= self.consensus_threshold and mbti_consensus,
            'consensus_score': overall_consensus,
            'mbti_consensus': mbti_consensus,
            'successful_count': len(successful_results),
            'total_count': len(self.core_evaluators),
            'big5_consensus_scores': big5_consensus_scores
        }

    def _calculate_pair_consensus(self, scores1: Dict, scores2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ¨¡å‹å¯¹çš„ä¸€è‡´æ€§åˆ†æ•°"""
        if not scores1 or not scores2:
            return 0.0
            
        differences = []
        for trait in ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
            if trait in scores1 and trait in scores2:
                diff = abs(scores1[trait] - scores2[trait])
                differences.append(diff)
        
        if not differences:
            return 0.0
            
        # è½¬æ¢ä¸ºä¸€è‡´æ€§åˆ†æ•°ï¼ˆå·®å¼‚è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜ï¼‰
        avg_diff = sum(differences) / len(differences)
        consensus = max(0, 1 - (avg_diff / 5))  # å‡è®¾5åˆ†åˆ¶ï¼Œæœ€å¤§å·®å¼‚ä¸º5
        
        return consensus

    def _evaluate_with_backup_models(self, input_file: Path, output_dir: Path, core_results: Dict) -> Dict:
        """ä½¿ç”¨å¤‡ç”¨è¯„ä¼°å™¨è¿›è¡Œè¡¥å……è¯„ä¼°"""
        print(f"ğŸ”„ å¯ç”¨å¤‡ç”¨è¯„ä¼°å™¨: {', '.join(self.backup_evaluators)}")
        
        all_results = core_results.copy()
        
        # å°è¯•å¤‡ç”¨è¯„ä¼°å™¨
        for model in self.backup_evaluators:
            print(f"\nğŸ” æ­£åœ¨ä½¿ç”¨å¤‡ç”¨è¯„ä¼°å™¨ {model}...")
            
            result = self._evaluate_with_single_model(model, input_file, output_dir)
            all_results[model] = result
            
            if result['success']:
                big5_str = ", ".join([f"{trait[0].upper()}:{score}" for trait, score in result['big5_scores'].items()])
                print(f"âœ… {model} - Big5: {big5_str} - MBTI: {result['mbti_type']}")
            else:
                print(f"âŒ {model} è¯„ä¼°å¤±è´¥: {result.get('error', 'Unknown error')}")

        # é‡æ–°è®¡ç®—æ‰€æœ‰è¯„ä¼°å™¨çš„ä¸€è‡´æ€§
        final_consensus = self._check_final_consensus(all_results)
        
        successful_evaluators = [model for model, result in all_results.items() if result['success']]
        
        return {
            'success': len(successful_evaluators) > 0,
            'results': all_results,
            'consensus_analysis': final_consensus,
            'evaluators_used': self.core_evaluators + self.backup_evaluators,
            'backup_used': True,
            'successful_evaluators': successful_evaluators
        }

    def _check_final_consensus(self, all_results: Dict) -> Dict:
        """æ£€æŸ¥æ‰€æœ‰è¯„ä¼°å™¨çš„æœ€ç»ˆä¸€è‡´æ€§"""
        successful_results = {k: v for k, v in all_results.items() if v['success']}
        
        if len(successful_results) < 2:
            return {
                'consensus_achieved': False,
                'consensus_score': 0.0,
                'successful_count': len(successful_results),
                'total_count': len(all_results)
            }

        # è®¡ç®—å¤šæ•°æŠ•ç¥¨
        big5_scores_by_trait = {}
        mbti_types = []
        
        for result in successful_results.values():
            mbti_types.append(result['mbti_type'])
            for trait, score in result['big5_scores'].items():
                if trait not in big5_scores_by_trait:
                    big5_scores_by_trait[trait] = []
                big5_scores_by_trait[trait].append(score)

        # è®¡ç®—å…±è¯†åˆ†æ•°
        consensus_scores = []
        for trait_scores in big5_scores_by_trait.values():
            if len(trait_scores) > 1:
                avg_score = sum(trait_scores) / len(trait_scores)
                variance = sum((score - avg_score) ** 2 for score in trait_scores) / len(trait_scores)
                consensus = max(0, 1 - (variance / 4))  # å‡è®¾æ–¹å·®æœ€å¤§ä¸º4
                consensus_scores.append(consensus)

        overall_consensus = sum(consensus_scores) / len(consensus_scores) if consensus_scores else 0
        
        # MBTIå¤šæ•°æŠ•ç¥¨
        from collections import Counter
        mbti_counter = Counter(mbti_types)
        most_common_mbti, mbti_count = mbti_counter.most_common(1)[0]
        mbti_consensus = mbti_count / len(mbti_types)
        
        return {
            'consensus_achieved': overall_consensus >= self.consensus_threshold and mbti_consensus >= 0.5,
            'consensus_score': overall_consensus,
            'mbti_consensus': mbti_consensus >= 0.5,
            'mbti_majority': most_common_mbti,
            'mbti_confidence': mbti_consensus,
            'successful_count': len(successful_results),
            'total_count': len(all_results)
        }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–çš„å¤šè¯„ä¼°å™¨ç³»ç»Ÿ')
    parser.add_argument('input_file', help='è¾“å…¥æµ‹è¯„æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', default='./optimized_evaluation_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--api_key', help='APIå¯†é’¥')
    
    args = parser.parse_args()
    
    evaluator = OptimizedMultiEvaluator(api_key=args.api_key)
    
    result = evaluator.evaluate_with_core_models(
        Path(args.input_file), 
        Path(args.output_dir)
    )
    
    print(f"\nğŸ¯ æœ€ç»ˆç»“æœ:")
    print(f"æˆåŠŸ: {'æ˜¯' if result['success'] else 'å¦'}")
    print(f"ä½¿ç”¨çš„è¯„ä¼°å™¨: {len(result['evaluators_used'])} ä¸ª")
    print(f"æ˜¯å¦ä½¿ç”¨å¤‡ç”¨è¯„ä¼°å™¨: {'æ˜¯' if result.get('backup_used', False) else 'å¦'}")
    print(f"ä¸€è‡´æ€§åˆ†æ•°: {result['consensus_analysis']['consensus_score']:.3f}")
    print(f"æ˜¯å¦è¾¾æˆå…±è¯†: {'æ˜¯' if result['consensus_analysis']['consensus_achieved'] else 'å¦'}")

if __name__ == '__main__':
    main()