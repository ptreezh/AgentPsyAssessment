#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§å¹¶å‘åˆ†æå™¨ - 5é¢˜åˆ†æ®µ + 1ç§’å»¶è¿Ÿ + æ™ºèƒ½å¹¶å‘ä¼˜åŒ–
"""

import sys
import os
import json
import asyncio
import hashlib
import aiohttp
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import threading
import queue

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DASHSCOPE_API_KEY'] = 'sk-3f16ac9d87e34ca88bf3925c3651624f'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class AdvancedConcurrentAnalyzer:
    def __init__(self, models: List[str] = None, cache_dir: str = "advanced_cache"):
        self.models = models or ["qwen-max", "deepseek-v3.2-exp", "Moonshot-Kimi-K2-Instruct"]
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # é«˜çº§ä¼˜åŒ–å‚æ•°
        self.segment_size = 5  # 5é¢˜æ¯æ®µ
        self.delay_between_batches = 1  # 1ç§’å»¶è¿Ÿ
        self.max_concurrent_per_model = 4  # æ¯ä¸ªæ¨¡å‹æœ€å¤§å¹¶å‘
        self.max_total_concurrent = 8  # æ€»æœ€å¤§å¹¶å‘
        self.adaptive_batch_size = True  # è‡ªé€‚åº”æ‰¹é‡å¤§å°

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'cache_hits': 0,
            'api_calls': 0,
            'total_segments': 0,
            'concurrent_batches': 0,
            'adaptive_adjustments': 0,
            'processing_time': 0,
            'avg_response_time': 0
        }

        # åŠ¨æ€è°ƒæ•´å‚æ•°
        self.current_concurrent_limit = self.max_concurrent_per_model
        self.response_times = queue.Queue(maxsize=10)

    def _get_smart_cache_key(self, questions: List[Dict], model: str) -> str:
        """æ™ºèƒ½ç¼“å­˜é”® - è€ƒè™‘é—®é¢˜é¡ºåºå’Œå†…å®¹"""
        # æ ‡å‡†åŒ–é—®é¢˜å†…å®¹
        normalized_content = []
        for q in questions:
            normalized = f"{q['question'].strip()}_{q['answer'].strip()}"
            normalized_content.append(normalized)

        # åˆ›å»ºé¡ºåºæ•æ„Ÿçš„å“ˆå¸Œ
        content_str = f"{model}_" + "_".join(normalized_content)
        return hashlib.sha256(content_str.encode()).hexdigest()

    def _load_smart_cache(self, cache_key: str) -> Optional[Dict]:
        """åŠ è½½æ™ºèƒ½ç¼“å­˜"""
        cache_file = self.cache_dir / f"smart_{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_entry = json.load(f)

                # æ£€æŸ¥ç¼“å­˜æ—¶æ•ˆæ€§ï¼ˆ7å¤©ï¼‰
                cache_time = datetime.fromisoformat(cache_entry.get('timestamp', '1970-01-01'))
                if (datetime.now() - cache_time).days > 7:
                    cache_file.unlink()
                    return None

                self.stats['cache_hits'] += 1
                return cache_entry['result']

            except Exception as e:
                print(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
                try:
                    cache_file.unlink()
                except:
                    pass

        return None

    def _save_smart_cache(self, cache_key: str, result: Dict):
        """ä¿å­˜æ™ºèƒ½ç¼“å­˜"""
        cache_file = self.cache_dir / f"smart_{cache_key}.json"
        try:
            cache_entry = {
                'timestamp': datetime.now().isoformat(),
                'result': result
            }

            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_entry, f, ensure_ascii=False, indent=2)

        except Exception as e:
            print(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def _create_advanced_batch_prompt(self, questions: List[Dict], segment_num: int) -> str:
        """åˆ›å»ºé«˜çº§æ‰¹é‡æç¤º - 5é¢˜åˆ†æ"""
        prompt = f"""ä½œä¸ºä¸“ä¸šçš„äººæ ¼åˆ†æä¸“å®¶ï¼Œè¯·åŒæ—¶åˆ†æä»¥ä¸‹{len(questions)}ä¸ªé—®é¢˜å¹¶å›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ã€‚

"""

        for i, q in enumerate(questions, 1):
            prompt += f"""
ğŸ“‹ é—®é¢˜ {i}:
é—®é¢˜: {q['question']}
å›ç­”: {q['answer']}
"""

        prompt += f"""

ğŸ¯ åˆ†æè¦æ±‚:
1. å¯¹æ¯ä¸ªé—®é¢˜ç‹¬ç«‹è¯„åˆ†ï¼ˆ1-5åˆ†åˆ¶ï¼‰
2. è€ƒè™‘é—®é¢˜é—´çš„ç›¸äº’å½±å“å’Œä¸€è‡´æ€§
3. æä¾›å…·ä½“çš„åˆ†æè¯æ®
4. è®¡ç®—5é¢˜çš„ç»¼åˆè¯„åˆ†

ğŸ“Š è¿”å›JSONæ ¼å¼:
{{
  "segment_number": {segment_num},
  "questions_count": {len(questions)},
  "individual_analysis": [
    {{
      "question_index": 1,
      "scores": {{
        "openness_to_experience": 1-5,
        "conscientiousness": 1-5,
        "extraversion": 1-5,
        "agreeableness": 1-5,
        "neuroticism": 1-5
      }},
      "confidence": 1-100,
      "evidence": {{
        "openness_to_experience": ["å…·ä½“è¯æ®1", "å…·ä½“è¯æ®2"],
        "conscientiousness": ["å…·ä½“è¯æ®1", "å…·ä½“è¯æ®2"],
        "extraversion": ["å…·ä½“è¯æ®1", "å…·ä½“è¯æ®2"],
        "agreeableness": ["å…·ä½“è¯æ®1", "å…·ä½“è¯æ®2"],
        "neuroticism": ["å…·ä½“è¯æ®1", "å…·ä½“è¯æ®2"]
      }}
    }}
  ],
  "segment_summary": {{
    "average_scores": {{
      "openness_to_experience": 1-5,
      "conscientiousness": 1-5,
      "extraversion": 1-5,
      "agreeableness": 1-5,
      "neuroticism": 1-5
    }},
    "overall_confidence": 1-100,
    "consistency_score": 1-100,
    "key_insights": ["å…³é”®æ´å¯Ÿ1", "å…³é”®æ´å¯Ÿ2", "å…³é”®æ´å¯Ÿ3"]
  }}
}}

âœ… è´¨é‡è¦æ±‚:
- æ‰€æœ‰è¯„åˆ†å¿…é¡»åœ¨1-5èŒƒå›´å†…
- æä¾›å…·ä½“çš„åˆ†æè¯æ®
- è¯„ä¼°é—®é¢˜é—´çš„ä¸€è‡´æ€§
- è®¡ç®—å‡†ç¡®çš„å¹³å‡åˆ†"""

        return prompt

    async def _call_api_with_monitoring(self, prompt: str, model: str) -> Tuple[str, float]:
        """å¸¦ç›‘æ§çš„APIè°ƒç”¨"""
        start_time = time.time()

        try:
            # ä½¿ç”¨çº¿ç¨‹æ± æ‰§è¡ŒåŒæ­¥APIè°ƒç”¨
            from enhanced_cloud_analyzer import EnhancedCloudAnalyzer
            analyzer = EnhancedCloudAnalyzer(model=model)

            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor(max_workers=1) as executor:
                response = await loop.run_in_executor(
                    executor,
                    analyzer._call_api,
                    prompt
                )

            response_time = time.time() - start_time

            # æ›´æ–°å“åº”æ—¶é—´ç»Ÿè®¡
            try:
                self.response_times.put(response_time, block=False)
                if self.response_times.qsize() > 10:
                    self.response_times.get(block=False)  # ç§»é™¤æœ€æ—§çš„
            except queue.Full:
                pass

            self.stats['avg_response_time'] = response_time
            self.stats['api_calls'] += 1

            return response, response_time

        except Exception as e:
            response_time = time.time() - start_time
            print(f"APIè°ƒç”¨å¤±è´¥ ({model}): {e}")
            return "", response_time

    def _adaptive_adjust_concurrency(self, avg_response_time: float):
        """è‡ªé€‚åº”è°ƒæ•´å¹¶å‘æ•°"""
        if not self.adaptive_batch_size:
            return

        # æ ¹æ®å“åº”æ—¶é—´è°ƒæ•´å¹¶å‘æ•°
        if avg_response_time < 20:  # å“åº”å¾ˆå¿«ï¼Œå¢åŠ å¹¶å‘
            new_limit = min(self.max_concurrent_per_model + 1, 6)
            if new_limit != self.current_concurrent_limit:
                self.current_concurrent_limit = new_limit
                self.stats['adaptive_adjustments'] += 1
                print(f"ğŸ“ˆ è‡ªé€‚åº”è°ƒæ•´: å¹¶å‘æ•°æå‡åˆ° {self.current_concurrent_limit}")

        elif avg_response_time > 60:  # å“åº”è¾ƒæ…¢ï¼Œå‡å°‘å¹¶å‘
            new_limit = max(self.current_concurrent_limit - 1, 2)
            if new_limit != self.current_concurrent_limit:
                self.current_concurrent_limit = new_limit
                self.stats['adaptive_adjustments'] += 1
                print(f"ğŸ“‰ è‡ªé€‚åº”è°ƒæ•´: å¹¶å‘æ•°é™ä½åˆ° {self.current_concurrent_limit}")

    async def _analyze_segment_advanced(self, questions: List[Dict], segment_num: int, model: str) -> Dict:
        """é«˜çº§åˆ†æ®µåˆ†æ"""
        self.stats['total_segments'] += 1

        # æ£€æŸ¥æ™ºèƒ½ç¼“å­˜
        cache_key = self._get_smart_cache_key(questions, model)
        cached_result = self._load_smart_cache(cache_key)

        if cached_result:
            print(f"  ğŸ“¦ æ™ºèƒ½ç¼“å­˜å‘½ä¸­: {model} æ®µ {segment_num}")
            return cached_result

        # æ‰§è¡ŒAPIè°ƒç”¨
        print(f"  ğŸ” é«˜çº§åˆ†æ: {model} æ®µ {segment_num} ({len(questions)} é¢˜)")

        try:
            # åˆ›å»ºé«˜çº§æç¤º
            prompt = self._create_advanced_batch_prompt(questions, segment_num)

            # è°ƒç”¨API
            response, response_time = await self._call_api_with_monitoring(prompt, model)

            # è§£æå“åº”
            result = self._parse_advanced_response(response, segment_num, len(questions))

            # è‡ªé€‚åº”è°ƒæ•´å¹¶å‘æ•°
            if len(self.response_times.queue) >= 5:
                recent_times = list(self.response_times.queue)
                avg_time = sum(recent_times) / len(recent_times)
                self._adaptive_adjust_concurrency(avg_time)

            # ä¿å­˜ç¼“å­˜
            if result.get('success', False):
                self._save_smart_cache(cache_key, result)

            result['response_time'] = response_time
            return result

        except Exception as e:
            return {
                'success': False,
                'segment_number': segment_num,
                'model': model,
                'error': str(e),
                'questions_count': len(questions)
            }

    def _parse_advanced_response(self, response: str, segment_num: int, questions_count: int) -> Dict:
        """è§£æé«˜çº§å“åº”"""
        try:
            import re

            # å°è¯•æå–JSON
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                result = json.loads(json_str)

                # éªŒè¯å…³é”®å­—æ®µ
                if 'segment_summary' in result and 'average_scores' in result['segment_summary']:
                    result['segment_number'] = segment_num
                    result['questions_count'] = questions_count
                    result['success'] = True

                    # éªŒè¯å¹¶ä¿®å¤è¯„åˆ†
                    for trait, score in result['segment_summary']['average_scores'].items():
                        result['segment_summary']['average_scores'][trait] = max(1, min(5, int(score)))

                    return result

        except Exception as e:
            print(f"é«˜çº§å“åº”è§£æå¤±è´¥: {e}")

        return {
            'success': False,
            'segment_number': segment_num,
            'questions_count': questions_count,
            'error': 'å“åº”è§£æå¤±è´¥'
        }

    def _create_adaptive_segments(self, questions: List[Dict]) -> List[List[Dict]]:
        """åˆ›å»ºè‡ªé€‚åº”åˆ†æ®µ"""
        segments = []
        total_questions = len(questions)

        # æ ‡å‡†åˆ†æ®µï¼ˆ5é¢˜ä¸€æ®µï¼‰
        for i in range(0, total_questions, self.segment_size):
            segment_questions = questions[i:i + self.segment_size]
            segments.append(segment_questions)

        # å¤„ç†å‰©ä½™é—®é¢˜ï¼ˆå¦‚æœä¸å¤Ÿ5é¢˜ï¼Œåˆå¹¶åˆ°å‰ä¸€æ®µï¼‰
        if len(segments) > 1 and len(segments[-1]) < 3:
            last_segment = segments.pop()
            segments[-1].extend(last_segment)

        return segments

    async def _process_model_concurrent(self, questions: List[Dict], model: str) -> List[Dict]:
        """å¹¶å‘å¤„ç†å•ä¸ªæ¨¡å‹çš„æ‰€æœ‰åˆ†æ®µ"""
        segments = self._create_adaptive_segments(questions)
        print(f"ğŸ¤– å¤„ç†æ¨¡å‹ {model}: {len(segments)} ä¸ªåˆ†æ®µ")

        # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
        tasks = []
        for i, segment_questions in enumerate(segments, 1):
            task = self._analyze_segment_advanced(segment_questions, i, model)
            tasks.append(task)

        # æ§åˆ¶å¹¶å‘æ•°é‡
        results = []
        semaphore = asyncio.Semaphore(self.current_concurrent_limit)

        async def controlled_task(task):
            async with semaphore:
                return await task

        # åˆ†æ‰¹å¤„ç†ä»»åŠ¡
        batch_size = self.max_total_concurrent // len(self.models)
        for i in range(0, len(tasks), batch_size):
            batch_tasks = tasks[i:i + batch_size]
            batch_results = await asyncio.gather(*[controlled_task(task) for task in batch_tasks])
            results.extend(batch_results)

            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if i + batch_size < len(tasks):
                await asyncio.sleep(self.delay_between_batches)

        # ç»Ÿè®¡æˆåŠŸåˆ†æ®µ
        success_count = len([r for r in results if r.get('success', False)])
        print(f"  âœ… {model}: {success_count}/{len(segments)} åˆ†æ®µæˆåŠŸ")

        return results

    async def analyze_file_advanced(self, file_path: Path, output_dir: Path) -> Dict:
        """é«˜çº§æ–‡ä»¶åˆ†æ"""
        start_time = time.time()
        print(f"ğŸš€ é«˜çº§å¹¶å‘åˆ†æ: {file_path.name}")
        print(f"ğŸ“Š é…ç½®: {self.segment_size}é¢˜/æ®µ, {self.delay_between_scores}ç§’å»¶è¿Ÿ, è‡ªé€‚åº”å¹¶å‘")

        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–é—®é¢˜
            questions = []
            for item in data:
                if 'question' in item and 'answer' in item:
                    questions.append({
                        'question': item['question'],
                        'answer': item['answer']
                    })

            if not questions:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆé—®é¢˜',
                    'file': str(file_path)
                }

            # åˆ›å»ºè‡ªé€‚åº”åˆ†æ®µ
            segments = self._create_adaptive_segments(questions)
            print(f"ğŸ“Š {len(questions)} ä¸ªé—®é¢˜åˆ†ä¸º {len(segments)} ä¸ªåˆ†æ®µ")

            # å¹¶å‘å¤„ç†æ‰€æœ‰æ¨¡å‹
            all_model_results = {}
            model_tasks = []

            for model in self.models:
                task = self._process_model_concurrent(questions, model)
                model_tasks.append((model, task))

            # ç­‰å¾…æ‰€æœ‰æ¨¡å‹å®Œæˆ
            for model, task in model_tasks:
                all_model_results[model] = await task

            # è®¡ç®—æœ€ç»ˆç»“æœ
            final_scores = self._calculate_advanced_final_scores(all_model_results)
            mbti_type = self._calculate_mbti(final_scores)
            model_consistency = self._calculate_advanced_consistency(all_model_results)

            # ç”Ÿæˆç»“æœ
            processing_time = time.time() - start_time
            self.stats['processing_time'] += processing_time

            result = {
                'success': True,
                'file': str(file_path),
                'processing_time': processing_time,
                'advanced_config': {
                    'segment_size': self.segment_size,
                    'segments_count': len(segments),
                    'models_count': len(self.models),
                    'initial_concurrent': self.max_concurrent_per_model,
                    'final_concurrent': self.current_concurrent_limit,
                    'adaptive_adjustments': self.stats['adaptive_adjustments'],
                    'delay_between_batches': self.delay_between_batches
                },
                'file_info': {
                    'filename': file_path.name,
                    'total_questions': len(questions),
                    'segments': len(segments),
                    'original_segments': len(questions) // 2,  # åŸæ¥2é¢˜åˆ†æ®µ
                    'segment_reduction': (len(questions) // 2) / len(segments)
                },
                'final_scores': final_scores,
                'mbti_type': mbti_type,
                'model_consistency': model_consistency,
                'model_results': all_model_results,
                'performance_stats': {
                    'cache_hit_rate': self.stats['cache_hits'] / max(1, self.stats['total_segments']) * 100,
                    'api_calls': self.stats['api_calls'],
                    'api_calls_saved': self.stats['cache_hits'],
                    'avg_response_time': self.stats['avg_response_time'],
                    'segments_per_second': (self.stats['total_segments'] * len(self.models)) / processing_time,
                    'concurrent_efficiency': self.current_concurrent_limit / self.max_concurrent_per_model
                }
            }

            # ä¿å­˜ç»“æœ
            output_file = output_dir / f"{file_path.stem}_advanced_concurrent.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"\nâœ… é«˜çº§å¹¶å‘åˆ†æå®Œæˆ: {output_file}")
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {processing_time:.1f} ç§’")
            print(f"ğŸ“Š åˆ†æ®µä¼˜åŒ–: {result['file_info']['original_segments']} â†’ {len(segments)} (å‡å°‘ {result['file_info']['segment_reduction']:.1f}å€)")
            print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {result['performance_stats']['segments_per_second']:.2f} æ®µ/ç§’")
            print(f"ğŸ¯ æœ€ç»ˆè¯„åˆ†: {final_scores}")
            print(f"ğŸ§  MBTI: {mbti_type}")
            print(f"ğŸ”§ è‡ªé€‚åº”è°ƒæ•´: {self.stats['adaptive_adjustments']} æ¬¡")

            return result

        except Exception as e:
            return {
                'success': False,
                'file': str(file_path),
                'error': str(e),
                'processing_time': time.time() - start_time
            }

    def _calculate_advanced_final_scores(self, all_model_results: Dict) -> Dict:
        """è®¡ç®—é«˜çº§æœ€ç»ˆè¯„åˆ†"""
        trait_scores = {}

        for model, results in all_model_results.items():
            model_trait_scores = {}

            for result in results:
                if result.get('success', False) and 'segment_summary' in result:
                    for trait, score in result['segment_summary']['average_scores'].items():
                        if trait not in model_trait_scores:
                            model_trait_scores[trait] = []
                        model_trait_scores[trait].append(score)

            # è®¡ç®—æ¨¡å‹å¹³å‡åˆ†
            for trait, scores in model_trait_scores.items():
                if scores:
                    avg_score = sum(scores) / len(scores)
                    if trait not in trait_scores:
                        trait_scores[trait] = []
                    trait_scores[trait].append(avg_score)

        # è®¡ç®—è·¨æ¨¡å‹å¹³å‡åˆ†
        final_scores = {}
        for trait, scores in trait_scores.items():
            if scores:
                final_scores[trait] = round(sum(scores) / len(scores))
            else:
                final_scores[trait] = 3

        return final_scores

    def _calculate_mbti(self, scores: Dict) -> str:
        """è®¡ç®—MBTIç±»å‹"""
        try:
            e_i = "E" if scores.get('extraversion', 3) > 3 else "I"
            s_n = "S" if scores.get('openness_to_experience', 3) < 4 else "N"
            t_f = "T" if scores.get('agreeableness', 3) < 3 else "F"
            j_p = "J" if scores.get('conscientiousness', 3) > 3 else "P"

            return f"{e_i}{s_n}{t_f}{j_p}"
        except:
            return "UNKNOWN"

    def _calculate_advanced_consistency(self, all_model_results: Dict) -> Dict:
        """è®¡ç®—é«˜çº§ä¸€è‡´æ€§"""
        consistency = {}

        for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
            model_scores = []

            for model, results in all_model_results.items():
                trait_scores = []
                for result in results:
                    if result.get('success', False) and 'segment_summary' in result:
                        score = result['segment_summary']['average_scores'].get(trait, 3)
                        trait_scores.append(score)

                if trait_scores:
                    model_avg = sum(trait_scores) / len(trait_scores)
                    model_scores.append(model_avg)

            if len(model_scores) >= 2:
                max_score = max(model_scores)
                min_score = min(model_scores)
                consistency[trait] = max(0, 100 - (max_score - min_score) * 20)
            else:
                consistency[trait] = 100

        return consistency

async def test_advanced_concurrent():
    print("ğŸš€ æµ‹è¯•é«˜çº§å¹¶å‘åˆ†æå™¨...")

    try:
        analyzer = AdvancedConcurrentAnalyzer()

        # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
        results_dir = Path("results/results")
        json_files = list(results_dir.glob("*.json"))

        if not json_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
            return

        # é€‰æ‹©æµ‹è¯•æ–‡ä»¶
        test_file = json_files[25] if len(json_files) > 25 else json_files[0]
        print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file.name}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("advanced_concurrent_results")
        output_dir.mkdir(exist_ok=True)

        # æ‰§è¡Œé«˜çº§å¹¶å‘åˆ†æ
        result = await analyzer.analyze_file_advanced(test_file, output_dir)

        if result['success']:
            print(f"\nâœ… é«˜çº§å¹¶å‘æµ‹è¯•æˆåŠŸ!")

            config = result['advanced_config']
            perf = result['performance_stats']

            print(f"ğŸ“Š é«˜çº§é…ç½®:")
            print(f"   åˆ†æ®µå¤§å°: {config['segment_size']}é¢˜/æ®µ")
            print(f"   å¹¶å‘æ•°: {config['initial_concurrent']} â†’ {config['final_concurrent']}")
            print(f"   è‡ªé€‚åº”è°ƒæ•´: {config['adaptive_adjustments']} æ¬¡")

            print(f"ğŸš€ æ€§èƒ½ç»Ÿè®¡:")
            print(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.1f} ç§’")
            print(f"   åˆ†æ®µå‡å°‘: {result['file_info']['segment_reduction']:.1f}å€")
            print(f"   ç¼“å­˜å‘½ä¸­ç‡: {perf['cache_hit_rate']:.1f}%")
            print(f"   å¤„ç†é€Ÿåº¦: {perf['segments_per_second']:.2f} æ®µ/ç§’")
            print(f"   å¹¶å‘æ•ˆç‡: {perf['concurrent_efficiency']:.1%}")

            # ä¼°ç®—æ€»ä½“æå‡
            original_segments = result['file_info']['original_segments']
            new_segments = result['file_info']['segments']
            speedup = (original_segments * 3) / (new_segments * len(analyzer.models) / 8)  # è€ƒè™‘8æ€»å¹¶å‘
            print(f"   ğŸ¯ é¢„ä¼°æ•´ä½“åŠ é€Ÿ: {speedup:.1f}å€")

        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {result['error']}")

    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_advanced_concurrent())