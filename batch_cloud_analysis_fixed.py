#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡ä¿®å¤ç‰ˆäº‘è¯„ä¼°å™¨åˆ†æ®µå¼å¿ƒç†è¯„ä¼°åˆ†æå™¨
ä½¿ç”¨ä¿®å¤ç‰ˆç®—æ³•è¿›è¡Œå®Œæ•´çš„50é“é¢˜ç›®åˆ†æï¼Œæ”¯æŒqwen-longå’Œqwen-maxæ¨¡å‹
"""

import json
import sys
import time
import argparse
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from cloud_segmented_analysis_fixed import FixedCloudSegmentedPersonalityAnalyzer

class BatchFixedCloudAnalyzer:
    """æ‰¹é‡ä¿®å¤ç‰ˆäº‘è¯„ä¼°å™¨åˆ†æå™¨"""

    def __init__(self, model: str = "qwen-long", api_key: str = None, max_workers: int = 1):
        self.model = model
        self.api_key = api_key or "sk-ffd03518254b495b8d27e723cd413fc1"
        self.max_workers = max_workers
        self.results = []

    def analyze_single_file(self, input_file: Path, output_dir: Path) -> dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            # åˆ›å»ºåˆ†æå™¨
            analyzer = FixedCloudSegmentedPersonalityAnalyzer(
                model=self.model,
                api_key=self.api_key
            )

            # æ£€æŸ¥APIå¯ç”¨æ€§
            if not analyzer.api_available:
                return {
                    'file': str(input_file),
                    'success': False,
                    'error': f'API connection failed for {self.model}',
                    'skipped': True
                }

            # æ‰§è¡Œåˆ†æ
            result = analyzer.analyze_full_assessment(str(input_file), str(output_dir))

            if result['success']:
                # ä»åˆ†æå™¨å®ä¾‹ä¸­è·å–è¯„åˆ†æ•°æ®
                final_scores = analyzer.calculate_final_scores()
                mbti_result = analyzer.generate_mbti_type(final_scores)

                # ç”Ÿæˆæ‘˜è¦
                summary = {
                    'file': str(input_file),
                    'summary_file': result['summary_file'],
                    'evidence_file': result['evidence_file'],
                    'model': self.model,
                    'success': True,
                    'big5_final_scores': {trait: data.get('final_score', 3) for trait, data in final_scores.items()},
                    'mbti_type': mbti_result['type'],
                    'analysis_quality': {
                        'success_rate': 100.0,  # å¦‚æœæˆåŠŸå®Œæˆï¼Œè¯´æ˜æ‰€æœ‰åˆ†æ®µéƒ½æˆåŠŸäº†
                        'successful_segments': len(analyzer.segment_results),
                        'total_segments': len(analyzer.segment_results)
                    }
                }

                big5_str = ", ".join([f"{trait[0].upper()}:{score}" for trait, score in summary['big5_final_scores'].items()])
                print(f"âœ… {input_file.name} - Big5: {big5_str} - MBTI: {summary['mbti_type']}")
                return summary
            else:
                print(f"âŒ {input_file.name} - {result.get('error', 'Unknown error')}")
                return {
                    'file': str(input_file),
                    'success': False,
                    'error': result.get('error', 'Unknown error'),
                    'model': self.model,
                    'skipped': False
                }

        except Exception as e:
            print(f"ğŸ’¥ {input_file.name} - å¼‚å¸¸: {e}")
            return {
                'file': str(input_file),
                'success': False,
                'error': str(e),
                'model': self.model,
                'skipped': False
            }

    def analyze_batch(self, input_files: list[Path], output_dir: Path,
                     progress_callback=None, delay_between_files: int = 3) -> list[dict]:
        """æ‰¹é‡åˆ†ææ–‡ä»¶"""
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¿®å¤ç‰ˆäº‘è¯„ä¼°å™¨åˆ†æ")
        print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {len(input_files)} ä¸ª")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"âš¡ å¹¶å‘æ•°: {self.max_workers}")
        print(f"â±ï¸  æ–‡ä»¶é—´å»¶è¿Ÿ: {delay_between_files} ç§’")

        output_dir.mkdir(parents=True, exist_ok=True)

        results = []
        completed = 0
        api_failure_count = 0

        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(self.analyze_single_file, file, output_dir): file
                for file in input_files
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_file):
                file = future_to_file[future]
                try:
                    result = future.result()
                    results.append(result)
                    completed += 1

                    # æ£€æŸ¥APIå¤±è´¥
                    if result.get('skipped') and 'API connection failed' in result.get('error', ''):
                        api_failure_count += 1
                        # å¦‚æœè¿ç»­APIå¤±è´¥ï¼Œåœæ­¢æ‰¹é‡å¤„ç†
                        if api_failure_count >= 3:
                            print(f"\nâš ï¸  è¿ç»­{api_failure_count}æ¬¡APIå¤±è´¥ï¼Œåœæ­¢æ‰¹é‡å¤„ç†")
                            break

                    if progress_callback:
                        progress_callback(completed, len(input_files), result)

                    # æ˜¾ç¤ºè¿›åº¦
                    status = "âœ…" if result['success'] else ("â­ï¸" if result.get('skipped') else "âŒ")
                    print(f"[{completed}/{len(input_files)}] {status} {file.name}")

                    # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                    if completed < len(input_files):
                        time.sleep(delay_between_files)

                except Exception as e:
                    print(f"ğŸ’¥ å¤„ç† {file.name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    results.append({
                        'file': str(file),
                        'success': False,
                        'error': str(e),
                        'model': self.model
                    })

        return results

    def generate_summary_report(self, results: list[dict], output_dir: Path):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        successful_results = [r for r in results if r['success']]
        failed_results = [r for r in results if not r['success']]
        skipped_results = [r for r in results if r.get('skipped', False)]

        # ç»Ÿè®¡Big5è¯„åˆ†åˆ†å¸ƒ
        big5_stats = {}
        mbti_stats = {}
        confidence_stats = []

        for result in successful_results:
            # Big5ç»Ÿè®¡
            for trait, score in result['big5_final_scores'].items():
                if trait not in big5_stats:
                    big5_stats[trait] = {1: 0, 3: 0, 5: 0}
                big5_stats[trait][score] += 1

            # MBTIç»Ÿè®¡
            mbti_type = result['mbti_type']
            if mbti_type not in mbti_stats:
                mbti_stats[mbti_type] = 0
            mbti_stats[mbti_type] += 1

            # ç½®ä¿¡åº¦ç»Ÿè®¡
            if 'analysis_quality' in result:
                confidence_stats.append(result['analysis_quality']['success_rate'])

        # ç”Ÿæˆæ±‡æ€»æ•°æ®
        summary = {
            'summary': {
                'total_files': len(results),
                'successful': len(successful_results),
                'failed': len(failed_results),
                'skipped_due_to_api': len(skipped_results),
                'success_rate': len(successful_results) / len(results) * 100 if results else 0,
                'model_used': self.model,
                'analysis_timestamp': datetime.now().isoformat(),
                'algorithm_version': 'fixed_v1.0'
            },
            'big5_distribution': big5_stats,
            'mbti_distribution': mbti_stats,
            'analysis_quality': {
                'average_success_rate': sum(confidence_stats) / len(confidence_stats) if confidence_stats else 0,
                'confidence_stats': confidence_stats
            },
            'detailed_results': results
        }

        # ä¿å­˜JSONæ±‡æ€»
        json_summary = output_dir / f"batch_summary_fixed_{self.model}.json"
        with open(json_summary, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_content = f"""# ä¿®å¤ç‰ˆæ‰¹é‡äº‘è¯„ä¼°å™¨åˆ†ææŠ¥å‘Š

**åˆ†ææ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**ä½¿ç”¨æ¨¡å‹:** {self.model}
**ç®—æ³•ç‰ˆæœ¬:** fixed_v1.0
**è¯„åˆ†æ ‡å‡†:** ä¸¥æ ¼1-3-5è¯„åˆ† (1=ä½, 3=ä¸­, 5=é«˜)

## æ±‡æ€»ç»Ÿè®¡

- **æ€»æ–‡ä»¶æ•°:** {len(results)}
- **æˆåŠŸåˆ†æ:** {len(successful_results)}
- **å¤±è´¥åˆ†æ:** {len(failed_results)}
- **APIè·³è¿‡:** {len(skipped_results)}
- **æˆåŠŸç‡:** {summary['summary']['success_rate']:.1f}%

## åˆ†æè´¨é‡

- **å¹³å‡åˆ†æ®µæˆåŠŸç‡:** {summary['analysis_quality']['average_success_rate']:.1f}%
- **ä½¿ç”¨ä¿®å¤ç‰ˆç®—æ³•:** âœ… è¯„åˆ†èŒƒå›´éªŒè¯ã€ç½®ä¿¡åº¦è®¡ç®—ã€é”™è¯¯å¤„ç†ä¼˜åŒ–

## Big5è¯„åˆ†åˆ†å¸ƒ (ä¿®å¤ç‰ˆ)

"""

        for trait, scores in big5_stats.items():
            total = sum(scores.values())
            md_content += f"### {trait.replace('_', ' ').title()}\n"
            md_content += f"- 1åˆ† (ä½): {scores[1]} ({scores[1]/total*100:.1f}%)\n"
            md_content += f"- 3åˆ† (ä¸­): {scores[3]} ({scores[3]/total*100:.1f}%)\n"
            md_content += f"- 5åˆ† (é«˜): {scores[5]} ({scores[5]/total*100:.1f}%)\n\n"

        md_content += "## MBTIç±»å‹åˆ†å¸ƒ\n\n"

        for mbti_type, count in sorted(mbti_stats.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(successful_results) * 100
            md_content += f"- **{mbti_type}:** {count} ({percentage:.1f}%)\n"

        md_content += "\n## è¯¦ç»†ç»“æœ\n\n"

        for result in successful_results:
            filename = Path(result['file']).name
            big5_str = ", ".join([f"{trait[0].upper()}:{score}" for trait, score in result['big5_final_scores'].items()])
            quality = result.get('analysis_quality', {}).get('success_rate', 0)
            md_content += f"- **{filename}** - Big5: {big5_str} - MBTI: {result['mbti_type']} - è´¨é‡: {quality:.1f}%\n"

        if failed_results:
            md_content += "\n## å¤±è´¥çš„æ–‡ä»¶\n\n"
            for result in failed_results:
                filename = Path(result['file']).name
                md_content += f"- **{filename}** - é”™è¯¯: {result.get('error', 'Unknown error')}\n"

        if skipped_results:
            md_content += "\n## APIè·³è¿‡çš„æ–‡ä»¶\n\n"
            for result in skipped_results:
                filename = Path(result['file']).name
                md_content += f"- **{filename}** - åŸå› : {result.get('error', 'Unknown reason')}\n"

        # ä¿å­˜MarkdownæŠ¥å‘Š
        md_summary = output_dir / f"batch_report_fixed_{self.model}.md"
        with open(md_summary, 'w', encoding='utf-8') as f:
            f.write(md_content)

        print(f"\nğŸ“Š ä¿®å¤ç‰ˆæ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"   JSON: {json_summary}")
        print(f"   Markdown: {md_summary}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰¹é‡ä¿®å¤ç‰ˆäº‘è¯„ä¼°å™¨åˆ†æ®µå¼Big5åˆ†æ')
    parser.add_argument('input_path', help='è¾“å…¥æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('--model', default='qwen-long', choices=['qwen-long', 'qwen-max'],
                       help='ä½¿ç”¨çš„äº‘æ¨¡å‹')
    parser.add_argument('--output', default='fixed_cloud_segmented_results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--sample', type=int, help='é‡‡æ ·æ–‡ä»¶æ•°é‡')
    parser.add_argument('--filter', help='æ–‡ä»¶åè¿‡æ»¤æ¨¡å¼')
    parser.add_argument('--workers', type=int, default=1, help='å¹¶å‘å·¥ä½œæ•°ï¼ˆå»ºè®®1é¿å…APIé™åˆ¶ï¼‰')
    parser.add_argument('--delay', type=int, default=5, help='æ–‡ä»¶é—´å»¶è¿Ÿç§’æ•°')

    args = parser.parse_args()

    # ç¡®å®šè¾“å…¥æ–‡ä»¶
    input_path = Path(args.input_path)
    if input_path.is_file():
        input_files = [input_path]
    elif input_path.is_dir():
        input_files = list(input_path.glob("*.json"))
    else:
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_path}")
        return

    # åº”ç”¨è¿‡æ»¤å™¨
    if args.filter:
        input_files = [f for f in input_files if args.filter.lower() in f.name.lower()]

    # é‡‡æ ·
    if args.sample:
        input_files = input_files[:args.sample]

    if not input_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
        return

    print(f"ğŸ” æ‰¾åˆ° {len(input_files)} ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†æ")

    # åˆ›å»ºæ‰¹é‡åˆ†æå™¨
    analyzer = BatchFixedCloudAnalyzer(
        model=args.model,
        max_workers=args.workers
    )

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # æ‰§è¡Œæ‰¹é‡åˆ†æ
    def progress_callback(completed, total, result):
        success_rate = sum(1 for r in analyzer.results if r.get('success', False)) / len(analyzer.results) * 100 if analyzer.results else 0
        print(f"ğŸ“ˆ è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) - æˆåŠŸç‡: {success_rate:.1f}%")

    results = analyzer.analyze_batch(
        input_files,
        output_dir,
        progress_callback=progress_callback,
        delay_between_files=args.delay
    )

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print(f"\nğŸ“‹ ç”Ÿæˆä¿®å¤ç‰ˆæ±‡æ€»æŠ¥å‘Š...")
    analyzer.generate_summary_report(results, output_dir)

    # æœ€ç»ˆç»Ÿè®¡
    successful = sum(1 for r in results if r['success'])
    api_skipped = sum(1 for r in results if r.get('skipped', False))
    print(f"\nğŸ‰ ä¿®å¤ç‰ˆæ‰¹é‡åˆ†æå®Œæˆ!")
    print(f"âœ… æˆåŠŸ: {successful}/{len(results)} ({successful/len(results)*100:.1f}%)")
    if api_skipped > 0:
        print(f"â­ï¸  APIè·³è¿‡: {api_skipped} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ”§ ä½¿ç”¨ä¿®å¤ç‰ˆç®—æ³•: è¯„åˆ†éªŒè¯ + ç½®ä¿¡åº¦è®¡ç®— + åˆ†ç¦»æ–‡ä»¶è¾“å‡º")

if __name__ == "__main__":
    main()