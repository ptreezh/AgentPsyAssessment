#!/usr/bin/env python3
"""
å¤šæ¨¡å‹ä¸€è‡´æ€§åˆ†æ - ä½¿ç”¨ä¸åŒ iFlow æ¨¡å‹è¯„ä¼°ç›¸åŒæ•°æ®å¹¶åˆ†æä¸€è‡´æ€§
"""

import asyncio
import json
import statistics
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
from iflow_sdk_evaluator import IFlowSDKEvaluator, IFlowBatchProcessor


class MultiModelConsistencyAnalyzer:
    """å¤šæ¨¡å‹ä¸€è‡´æ€§åˆ†æå™¨"""
    
    def __init__(self, models: List[str] = None):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            models: æ¨¡å‹åˆ—è¡¨ï¼Œé»˜è®¤ä½¿ç”¨å¤šä¸ª iFlow æ¨¡å‹
        """
        if models is None:
            self.models = [
                "deepseek-v3.2-exp",
                "deepseek-r1:70b",
                "deepseek-r1:8b",
                "deepseek-chat"
            ]
        else:
            self.models = models
        
        self.evaluators = {model: IFlowSDKEvaluator(model=model) for model in self.models}
    
    def load_assessment_file(self, file_path: Path) -> List[Dict]:
        """åŠ è½½æµ‹è¯„æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        questions = []
        if isinstance(data, dict) and 'assessment_results' in data:
            assessment_results = data['assessment_results']
            
            for item in assessment_results:
                if isinstance(item, dict) and 'question_data' in item and 'conversation_log' in item:
                    question_data = item['question_data']
                    conversation_log = item['conversation_log']
                    
                    question = None
                    answer = None
                    
                    for log_item in conversation_log:
                        if log_item.get('role') == 'user':
                            content = log_item.get('content', '')
                            if '[ASSESSMENT_QUESTION]' in content:
                                question = content.replace('[ASSESSMENT_QUESTION]', '').strip()
                        elif log_item.get('role') == 'assistant':
                            answer = log_item.get('content', '')
                            if '\n\n' in answer:
                                answer = answer.split('\n\n')[-1].strip()
                    
                    if question and answer:
                        questions.append({
                            'question': question,
                            'answer': answer,
                            'dimension': question_data.get('dimension', ''),
                            'question_id': question_data.get('question_id', '')
                        })
        
        return questions
    
    def create_segments(self, questions: List[Dict], segment_size: int = 5) -> List[List[Dict]]:
        """å°†é—®é¢˜åˆ—è¡¨åˆ†æˆ5é¢˜åˆ†æ®µ"""
        segments = []
        for i in range(0, len(questions), segment_size):
            segment = questions[i:i + segment_size]
            if segment:
                segments.append(segment)
        return segments
    
    async def evaluate_with_model(self, segments: List[List[Dict]], model: str) -> Dict[str, Any]:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¯„ä¼°åˆ†æ®µ"""
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ {model} è¿›è¡Œè¯„ä¼°...")
        
        processor = IFlowBatchProcessor(model=model)
        segment_results = await processor.batch_evaluate(segments)
        final_scores = processor.calculate_final_scores(segment_results['results'])
        
        return {
            'model': model,
            'segment_results': segment_results,
            'final_scores': final_scores,
            'stats': processor.stats
        }
    
    def calculate_consistency(self, all_results: Dict[str, Dict]) -> Dict[str, Any]:
        """è®¡ç®—æ¨¡å‹é—´çš„ä¸€è‡´æ€§åˆ†æ"""
        
        traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        consistency_analysis = {}
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„åˆ†æ•°
        model_scores = {}
        for model, result in all_results.items():
            if result['final_scores'].get('success', False):
                model_scores[model] = result['final_scores']['big5_scores']
        
        # è®¡ç®—æ¯ä¸ªç‰¹è´¨çš„ä¸€è‡´æ€§
        for trait in traits:
            scores = [model_scores[model][trait] for model in model_scores if trait in model_scores[model]]
            if scores:
                mean = statistics.mean(scores)
                stdev = statistics.stdev(scores) if len(scores) > 1 else 0
                min_score = min(scores)
                max_score = max(scores)
                
                consistency_analysis[trait] = {
                    'mean': round(mean, 2),
                    'stdev': round(stdev, 2),
                    'min': min_score,
                    'max': max_score,
                    'range': max_score - min_score,
                    'consistency_level': self._get_consistency_level(stdev)
                }
        
        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§
        all_scores = []
        for model_scores_dict in model_scores.values():
            all_scores.extend(list(model_scores_dict.values()))
        
        overall_stdev = statistics.stdev(all_scores) if len(all_scores) > 1 else 0
        
        return {
            'traits_consistency': consistency_analysis,
            'overall_consistency': {
                'stdev': round(overall_stdev, 2),
                'consistency_level': self._get_consistency_level(overall_stdev)
            },
            'models_compared': list(model_scores.keys()),
            'total_models': len(model_scores)
        }
    
    def _get_consistency_level(self, stdev: float) -> str:
        """æ ¹æ®æ ‡å‡†å·®åˆ¤æ–­ä¸€è‡´æ€§ç­‰çº§"""
        if stdev < 0.5:
            return "éå¸¸é«˜"
        elif stdev < 1.0:
            return "é«˜"
        elif stdev < 1.5:
            return "ä¸­ç­‰"
        else:
            return "ä½"
    
    def find_disagreements(self, all_results: Dict[str, Dict]) -> List[Dict]:
        """æ‰¾å‡ºæ¨¡å‹é—´çš„ä¸»è¦åˆ†æ­§ç‚¹"""
        
        traits = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        disagreements = []
        
        # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„åˆ†æ•°
        model_scores = {}
        for model, result in all_results.items():
            if result['final_scores'].get('success', False):
                model_scores[model] = result['final_scores']['big5_scores']
        
        # æ‰¾å‡ºåˆ†æ­§è¾ƒå¤§çš„ç‰¹è´¨
        for trait in traits:
            scores = [model_scores[model][trait] for model in model_scores if trait in model_scores[model]]
            if scores:
                score_range = max(scores) - min(scores)
                if score_range >= 2:  # åˆ†æ•°å·®å¼‚å¤§äº2åˆ†è§†ä¸ºæ˜¾è‘—åˆ†æ­§
                    disagreement = {
                        'trait': trait,
                        'score_range': score_range,
                        'model_scores': {model: model_scores[model][trait] for model in model_scores}
                    }
                    disagreements.append(disagreement)
        
        return disagreements
    
    async def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„å¤šæ¨¡å‹ä¸€è‡´æ€§"""
        
        print(f"\nğŸ“Š åˆ†ææ–‡ä»¶: {file_path.name}")
        
        # åŠ è½½æ•°æ®
        questions = self.load_assessment_file(file_path)
        if not questions:
            return {'success': False, 'error': f'æ— æ³•åŠ è½½æˆ–è§£ææ–‡ä»¶: {file_path.name}'}
        
        print(f"   æ‰¾åˆ° {len(questions)} ä¸ªé—®ç­”å¯¹")
        
        # åˆ›å»ºåˆ†æ®µ
        segments = self.create_segments(questions, segment_size=5)
        print(f"   åˆ†æˆ {len(segments)} ä¸ªåˆ†æ®µ")
        
        # ä½¿ç”¨ä¸åŒæ¨¡å‹è¿›è¡Œè¯„ä¼°
        all_results = {}
        for model in self.models:
            try:
                result = await self.evaluate_with_model(segments, model)
                all_results[model] = result
                
                if result['final_scores']['success']:
                    scores = result['final_scores']['big5_scores']
                    score_str = ", ".join([f"{trait[0].upper()}:{score}" for trait, score in scores.items()])
                    print(f"   âœ… {model}: {score_str}")
                else:
                    print(f"   âŒ {model}: è¯„ä¼°å¤±è´¥")
                    
            except Exception as e:
                print(f"   âŒ {model}: è¯„ä¼°å‡ºé”™ - {e}")
                all_results[model] = {'error': str(e)}
        
        # è®¡ç®—ä¸€è‡´æ€§åˆ†æ
        consistency = self.calculate_consistency(all_results)
        disagreements = self.find_disagreements(all_results)
        
        return {
            'success': True,
            'file': str(file_path),
            'total_questions': len(questions),
            'segments': len(segments),
            'model_results': all_results,
            'consistency_analysis': consistency,
            'disagreements': disagreements,
            'analysis_time': datetime.now().isoformat()
        }


def print_consistency_report(analysis_result: Dict):
    """æ‰“å°ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š"""
    
    print("\n" + "="*60)
    print("ğŸ“ˆ å¤šæ¨¡å‹ä¸€è‡´æ€§åˆ†ææŠ¥å‘Š")
    print("="*60)
    
    if not analysis_result.get('success', False):
        print(f"âŒ åˆ†æå¤±è´¥: {analysis_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        return
    
    print(f"ğŸ“ åˆ†ææ–‡ä»¶: {analysis_result['file']}")
    print(f"ğŸ“Š æ€»é—®é¢˜æ•°: {analysis_result['total_questions']}")
    print(f"ğŸ“‹ åˆ†æ®µæ•°é‡: {analysis_result['segments']}")
    
    consistency = analysis_result['consistency_analysis']
    print(f"\nğŸ¤– å‚ä¸æ¨¡å‹ ({consistency['total_models']}ä¸ª): {', '.join(consistency['models_compared'])}")
    
    print("\nğŸ“Š ç‰¹è´¨ä¸€è‡´æ€§åˆ†æ:")
    for trait, stats in consistency['traits_consistency'].items():
        print(f"   {trait:20}: å‡å€¼={stats['mean']}, æ ‡å‡†å·®={stats['stdev']}, èŒƒå›´={stats['min']}-{stats['max']}, ä¸€è‡´æ€§={stats['consistency_level']}")
    
    overall = consistency['overall_consistency']
    print(f"\nğŸ“Š æ€»ä½“ä¸€è‡´æ€§: æ ‡å‡†å·®={overall['stdev']}, ç­‰çº§={overall['consistency_level']}")
    
    disagreements = analysis_result['disagreements']
    if disagreements:
        print(f"\nâš ï¸  å‘ç° {len(disagreements)} ä¸ªæ˜¾è‘—åˆ†æ­§:")
        for d in disagreements:
            print(f"   {d['trait']}: åˆ†æ•°èŒƒå›´ {d['score_range']}")
            for model, score in d['model_scores'].items():
                print(f"     - {model}: {score}")
    else:
        print("\nâœ… æ¨¡å‹é—´æ— æ˜¾è‘—åˆ†æ­§")


async def main():
    """ä¸»å‡½æ•°"""
    
    # é€‰æ‹©æµ‹è¯•æ–‡ä»¶
    test_files = [
        "results/results/asses_deepseek_r1_70b_agent_big_five_50_complete2_a1_e0_t0_0_09271.json",
        "results/results/asses_gemma3_latest_agent_big_five_50_complete2_def_e0_t0_0_09201.json"
    ]
    
    analyzer = MultiModelConsistencyAnalyzer()
    
    all_results = {}
    for file_path_str in test_files:
        file_path = Path(file_path_str)
        if file_path.exists():
            result = await analyzer.analyze_file(file_path)
            all_results[file_path.name] = result
            print_consistency_report(result)
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    # ä¿å­˜ç»“æœ
    output_file = f"multi_model_consistency_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ä¸€è‡´æ€§åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    asyncio.run(main())