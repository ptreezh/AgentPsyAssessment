#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§ç‰ˆæœ¬æ‰¹é‡æµ‹è¯„æŠ¥å‘Šå¤„ç†å™¨
å¤„ç†å¤§é‡çœŸå®çš„æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­è·‘å’Œé«˜æ€§èƒ½å¤„ç†
"""

import sys
import os
import json
from pathlib import Path
from datetime import datetime
import time
import argparse
import logging

# æ·»åŠ åŒ…ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from single_report_pipeline import TransparentPipeline


class ProductionBatchProcessor:
    """ç”Ÿäº§ç‰ˆæœ¬æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self, input_dir: str, output_dir: str, checkpoint_interval: int = 10):
        """
        åˆå§‹åŒ–ç”Ÿäº§æ‰¹å¤„ç†å™¨
        
        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            checkpoint_interval: æ£€æŸ¥ç‚¹é—´éš”
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.checkpoint_interval = checkpoint_interval
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥ç‚¹æ–‡ä»¶
        self.checkpoint_file = self.output_dir / "production_checkpoint.pkl"
        self.results_file = self.output_dir / "production_results.json"
        self.log_file = self.output_dir / "production_processing.log"
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
        
        # åˆ›å»ºæµæ°´çº¿å®ä¾‹
        self.pipeline = TransparentPipeline()
        
        # çŠ¶æ€å˜é‡
        self.processed_files = set()
        self.results = []
        self.start_time = datetime.now()
        self.total_files = 0
        self.current_index = 0
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if self.checkpoint_file.exists():
            try:
                import pickle
                with open(self.checkpoint_file, 'rb') as f:
                    checkpoint_data = pickle.load(f)
                
                self.processed_files = set(checkpoint_data.get('processed_files', []))
                self.results = checkpoint_data.get('results', [])
                self.start_time = checkpoint_data.get('start_time', datetime.now())
                self.total_files = checkpoint_data.get('total_files', 0)
                self.current_index = checkpoint_data.get('current_index', 0)
                
                self.logger.info(f"å·²åŠ è½½æ£€æŸ¥ç‚¹: å¤„ç†äº† {len(self.processed_files)} ä¸ªæ–‡ä»¶")
                return True
            except Exception as e:
                self.logger.warning(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                return False
        else:
            self.logger.info("æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶")
            return False
    
    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        import pickle
        checkpoint_data = {
            'processed_files': list(self.processed_files),
            'results': self.results,
            'start_time': self.start_time,
            'total_files': self.total_files,
            'current_index': self.current_index
        }
        
        try:
            with open(self.checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint_data, f)
            self.logger.info("æ£€æŸ¥ç‚¹å·²ä¿å­˜")
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        results_data = {
            'processing_info': {
                'start_time': self.start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'total_files': self.total_files,
                'processed_files': len(self.processed_files),
                'remaining_files': self.total_files - len(self.processed_files),
                'duration_seconds': (datetime.now() - self.start_time).total_seconds()
            },
            'results': self.results
        }
        
        try:
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False
    
    def find_json_files(self, pattern: str = "*.json") -> list:
        """æŸ¥æ‰¾JSONæ–‡ä»¶"""
        json_files = list(self.input_dir.glob(pattern))
        json_files.sort()  # æŒ‰æ–‡ä»¶åæ’åºç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´
        return json_files
    
    def process_single_report(self, file_path: Path) -> dict:
        """
        å¤„ç†å•ä¸ªæµ‹è¯„æŠ¥å‘Š
        
        Args:
            file_path: æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœ
        """
        self.logger.info(f"å¤„ç†: {file_path.name}")
        
        start_time = time.time()
        
        try:
            # å¤„ç†æµ‹è¯„æŠ¥å‘Š
            result = self.pipeline.process_single_report(str(file_path))
            
            processing_time = time.time() - start_time
            
            if result and result.get('success', False):
                self.logger.info(f"å®Œæˆ: {file_path.name}")
                self.logger.info(f"  å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")
                self.logger.info(f"  å¤§äº”äººæ ¼: {result.get('big5_scores', {})}")
                self.logger.info(f"  MBTIç±»å‹: {result.get('mbti_type', 'Unknown')}")
                return {
                    **result,
                    'file_path': str(file_path),
                    'processing_time': round(processing_time, 1),
                    'success': True
                }
            else:
                self.logger.error(f"å¤±è´¥: {file_path.name}")
                error_msg = result.get('error', 'Unknown error') if result else 'No result'
                self.logger.error(f"  é”™è¯¯: {error_msg}")
                return {
                    'success': False,
                    'file_path': str(file_path),
                    'error': error_msg,
                    'processing_time': round(processing_time, 1)
                }
                
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.exception(f"å¼‚å¸¸: {file_path.name} - {e}")
            return {
                'success': False,
                'file_path': str(file_path),
                'error': str(e),
                'processing_time': round(processing_time, 1)
            }
    
    def run_production_batch(self, pattern: str = "*.json", limit: int = None, 
                           resume: bool = True, no_save: bool = False) -> bool:
        """
        è¿è¡Œç”Ÿäº§æ‰¹å¤„ç†
        
        Args:
            pattern: æ–‡ä»¶åŒ¹é…æ¨¡å¼
            limit: é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡
            resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
            no_save: æ˜¯å¦ä¸ä¿å­˜ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        self.logger.info("ğŸš€ ç”Ÿäº§ç‰ˆæœ¬æ‰¹é‡å¤„ç†å¯åŠ¨")
        self.logger.info("="*80)
        self.logger.info(f"è¾“å…¥ç›®å½•: {self.input_dir}")
        self.logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info(f"æ£€æŸ¥ç‚¹é—´éš”: æ¯ {self.checkpoint_interval} ä¸ªæ–‡ä»¶")
        self.logger.info(f"æ¢å¤æ¨¡å¼: {'å¯ç”¨' if resume else 'ç¦ç”¨'}")
        self.logger.info()
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        if resume:
            self.load_checkpoint()
        
        # æŸ¥æ‰¾æ–‡ä»¶
        self.logger.info("ğŸ“‚ æŸ¥æ‰¾æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶...")
        json_files = self.find_json_files(pattern)
        
        if not json_files:
            self.logger.error("æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
            return False
        
        self.total_files = len(json_files)
        if limit:
            json_files = json_files[:limit]
            self.total_files = len(json_files)
        
        self.logger.info(f"æ‰¾åˆ° {len(json_files)} ä¸ªæµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
        self.logger.info(f"å·²å¤„ç†: {len(self.processed_files)} ä¸ª")
        self.logger.info(f"å‰©ä½™: {len(json_files) - len(self.processed_files)} ä¸ª")
        self.logger.info()
        
        # ç¡®å®šèµ·å§‹ä½ç½®
        start_index = 0
        if resume and self.current_index < len(json_files):
            start_index = self.current_index
        
        self.logger.info(f"â–¶ï¸  ä»ç¬¬ {start_index + 1} ä¸ªæ–‡ä»¶å¼€å§‹å¤„ç†")
        self.logger.info()
        
        # å¤„ç†æ–‡ä»¶
        processed_count = 0
        success_count = 0
        failed_count = 0
        
        for i, file_path in enumerate(json_files[start_index:], start_index):
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
            if str(file_path) in self.processed_files:
                self.logger.info(f"â­ï¸  è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}")
                continue
            
            # å¤„ç†æ–‡ä»¶
            result = self.process_single_report(file_path)
            
            # æ›´æ–°çŠ¶æ€
            self.processed_files.add(str(file_path))
            self.results.append(result)
            self.current_index = i + 1
            
            if result.get('success', False):
                success_count += 1
            else:
                failed_count += 1
            
            processed_count += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if processed_count % 100 == 0:
                self.logger.info(f"ğŸ“Š è¿›åº¦: {processed_count} ä¸ªæ–‡ä»¶å·²å¤„ç† "
                               f"(æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count})")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if processed_count % self.checkpoint_interval == 0 and not no_save:
                self.logger.info(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...")
                self.save_checkpoint()
                self.save_results()
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIè¿‡è½½
            time.sleep(0.5)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.logger.info(f"\nğŸ æ‰¹é‡å¤„ç†å®Œæˆ!")
        self.logger.info("="*80)
        self.logger.info(f"æ€»æ–‡ä»¶æ•°: {len(json_files)}")
        self.logger.info(f"å·²å¤„ç†æ•°: {processed_count}")
        self.logger.info(f"æˆåŠŸå¤„ç†: {success_count}")
        self.logger.info(f"å¤„ç†å¤±è´¥: {failed_count}")
        self.logger.info(f"æˆåŠŸç‡: {success_count/processed_count*100:.1f}%" if processed_count > 0 else "N/A")
        
        if not no_save:
            self.save_checkpoint()
            self.save_results()
        
        self.logger.info(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        self.logger.info(f"ğŸ” å¦‚éœ€ç»§ç»­å¤„ç†å‰©ä½™æ–‡ä»¶ï¼Œè¯·é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”Ÿäº§ç‰ˆæœ¬æ‰¹é‡æµ‹è¯„æŠ¥å‘Šå¤„ç†å™¨ - æ”¯æŒæ–­ç‚¹ç»­è·‘')
    parser.add_argument('--input-dir', default=r'D:\AIDevelop\portable_psyagent\results\readonly-original',
                       help='è¾“å…¥ç›®å½• (é»˜è®¤: D:\\AIDevelop\\portable_psyagent\\results\\readonly-original)')
    parser.add_argument('--output-dir', default=r'D:\AIDevelop\portable_psyagent\results\production-batch-results',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: D:\\AIDevelop\\portable_psyagent\\results\\production-batch-results)')
    parser.add_argument('--pattern', default='*.json',
                       help='æ–‡ä»¶åŒ¹é…æ¨¡å¼ (é»˜è®¤: *.json)')
    parser.add_argument('--limit', type=int,
                       help='é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡')
    parser.add_argument('--checkpoint-interval', type=int, default=10,
                       help='æ£€æŸ¥ç‚¹é—´éš” (é»˜è®¤: æ¯10ä¸ªæ–‡ä»¶)')
    parser.add_argument('--no-resume', action='store_true',
                       help='ä¸ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œé‡æ–°å¼€å§‹')
    parser.add_argument('--no-save', action='store_true',
                       help='ä¸ä¿å­˜ç»“æœï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç”Ÿäº§æ‰¹å¤„ç†å™¨
    processor = ProductionBatchProcessor(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        checkpoint_interval=args.checkpoint_interval
    )
    
    # è¿è¡Œç”Ÿäº§æ‰¹å¤„ç†
    success = processor.run_production_batch(
        pattern=args.pattern,
        limit=args.limit,
        resume=not args.no_resume,
        no_save=args.no_save
    )
    
    if success:
        print("\nğŸ‰ ç”Ÿäº§ç‰ˆæœ¬æ‰¹é‡å¤„ç†æˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("\nâŒ ç”Ÿäº§ç‰ˆæœ¬æ‰¹é‡å¤„ç†å¤±è´¥!")
        return 1


if __name__ == "__main__":
    sys.exit(main())