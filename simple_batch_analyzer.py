#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆæ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æè„šæœ¬ - æ”¯æŒæ–­ç‚¹ç»­è·‘
å¤„ç†å¤šä¸ªæµ‹è¯„æŠ¥å‘Šæ–‡ä»¶ï¼Œæ”¯æŒä¸­æ–­åç»§ç»­è¿è¡Œ
"""

import sys
import os
import json
import argparse
from pathlib import Path
from datetime import datetime
import time
import pickle

# æ·»åŠ åŒ…ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from single_report_pipeline import TransparentPipeline


class SimpleBatchAnalyzer:
    """ç®€åŒ–ç‰ˆæ‰¹é‡åˆ†æå™¨ - æ”¯æŒæ–­ç‚¹ç»­è·‘"""
    
    def __init__(self, input_dir: str, output_dir: str):
        """
        åˆå§‹åŒ–æ‰¹é‡åˆ†æå™¨
        
        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        self.checkpoint_file = self.output_dir / "checkpoint.pkl"
        self.results_file = self.output_dir / "results.json"
        
        # åˆ›å»ºæµæ°´çº¿å®ä¾‹
        self.pipeline = TransparentPipeline()
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.processed_files = set()
        self.results = []
        self.start_time = datetime.now()
    
    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'rb') as f:
                    checkpoint_data = pickle.load(f)
                
                self.processed_files = set(checkpoint_data.get('processed_files', []))
                self.results = checkpoint_data.get('results', [])
                
                # è§£ææ—¶é—´æˆ³
                start_time_str = checkpoint_data.get('start_time')
                if start_time_str:
                    try:
                        self.start_time = datetime.fromisoformat(start_time_str)
                    except:
                        self.start_time = datetime.now()
                else:
                    self.start_time = datetime.now()
                
                print(f"âœ… å·²åŠ è½½æ£€æŸ¥ç‚¹: å¤„ç†äº† {len(self.processed_files)} ä¸ªæ–‡ä»¶")
                
            except Exception as e:
                print(f"âš ï¸  åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                self.processed_files = set()
                self.results = []
                self.start_time = datetime.now()
        else:
            print("â„¹ï¸  æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå¼€å§‹å…¨æ–°åˆ†æ")
            self.processed_files = set()
            self.results = []
            self.start_time = datetime.now()
    
    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_data = {
            'processed_files': list(self.processed_files),
            'results': self.results,
            'start_time': self.start_time.isoformat()
        }
        
        try:
            with open(self.checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint_data, f)
            print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        # ç¡®ä¿start_timeæ˜¯datetimeå¯¹è±¡
        if isinstance(self.start_time, str):
            try:
                start_time_obj = datetime.fromisoformat(self.start_time)
            except:
                start_time_obj = datetime.now()
        else:
            start_time_obj = self.start_time or datetime.now()
        
        results_data = {
            'analysis_info': {
                'start_time': start_time_obj.isoformat(),
                'end_time': datetime.now().isoformat(),
                'processed_files': len(self.processed_files),
                'duration_seconds': (datetime.now() - start_time_obj).total_seconds()
            },
            'results': self.results
        }
        
        try:
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def find_json_files(self) -> list:
        """æŸ¥æ‰¾JSONæ–‡ä»¶"""
        json_files = list(self.input_dir.glob("*.json"))
        json_files.sort()
        return json_files
    
    def process_single_report(self, file_path: Path) -> dict:
        """
        å¤„ç†å•ä¸ªæµ‹è¯„æŠ¥å‘Š
        
        Args:
            file_path: æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœ
        """
        print(f"ğŸ” å¤„ç†: {file_path.name}")
        
        try:
            # å¤„ç†æµ‹è¯„æŠ¥å‘Š
            result = self.pipeline.process_single_report(str(file_path))
            
            if result and result.get('success', False):
                print(f"  âœ… å®Œæˆ")
                print(f"    å¤§äº”äººæ ¼: {result.get('big5_scores', {})}")
                print(f"    MBTIç±»å‹: {result.get('mbti_type', 'Unknown')}")
                return result
            else:
                print(f"  âŒ å¤±è´¥")
                error_msg = result.get('error', 'Unknown error') if result else 'No result'
                return {
                    'success': False,
                    'file_path': str(file_path),
                    'error': error_msg
                }
                
        except Exception as e:
            print(f"  âŒ å¼‚å¸¸: {e}")
            return {
                'success': False,
                'file_path': str(file_path),
                'error': str(e)
            }
    
    def run_batch_analysis(self, limit: int = None):
        """
        è¿è¡Œæ‰¹é‡åˆ†æ
        
        Args:
            limit: é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡
        """
        print("ğŸš€ ç®€åŒ–ç‰ˆæ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æå™¨")
        print("="*60)
        print(f"è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print()
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        self.load_checkpoint()
        
        # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
        print("ğŸ“‚ æŸ¥æ‰¾æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶...")
        json_files = self.find_json_files()
        
        if not json_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
            return
        
        if limit:
            json_files = json_files[:limit]
        
        print(f"  æ‰¾åˆ° {len(json_files)} ä¸ªæµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
        print(f"  å·²å¤„ç†: {len(self.processed_files)} ä¸ª")
        print(f"  å‰©ä½™: {len(json_files) - len(self.processed_files)} ä¸ª")
        print()
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        processed_count = 0
        success_count = 0
        failed_count = 0
        
        for i, file_path in enumerate(json_files):
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡
            if str(file_path) in self.processed_files:
                print(f"â­ï¸  è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}")
                continue
            
            print(f"ğŸ“ˆ [{i+1}/{len(json_files)}] å¤„ç†æ–‡ä»¶: {file_path.name}")
            
            # å¤„ç†æ–‡ä»¶
            result = self.process_single_report(file_path)
            
            # æ›´æ–°çŠ¶æ€
            self.processed_files.add(str(file_path))
            self.results.append(result)
            
            if result.get('success', False):
                success_count += 1
            else:
                failed_count += 1
            
            processed_count += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if processed_count % 5 == 0:
                print(f"  ğŸ“Š è¿›åº¦: {processed_count} ä¸ªæ–‡ä»¶å·²å¤„ç† "
                      f"(æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count})")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if processed_count % 10 == 0:
                print(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...")
                self.save_checkpoint()
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIè¿‡è½½
            time.sleep(1)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        print(f"\nğŸ æ‰¹é‡åˆ†æå®Œæˆ!")
        print("="*60)
        print(f"æ€»æ–‡ä»¶æ•°: {len(json_files)}")
        print(f"å·²å¤„ç†æ•°: {processed_count}")
        print(f"æˆåŠŸå¤„ç†: {success_count}")
        print(f"å¤„ç†å¤±è´¥: {failed_count}")
        print(f"æˆåŠŸç‡: {success_count/processed_count*100:.1f}%" if processed_count > 0 else "N/A")
        
        self.save_checkpoint()
        self.save_results()
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")
        print(f"ğŸ” å¦‚éœ€ç»§ç»­å¤„ç†å‰©ä½™æ–‡ä»¶ï¼Œè¯·é‡æ–°è¿è¡Œæ­¤è„šæœ¬")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç®€åŒ–ç‰ˆæ‰¹é‡æµ‹è¯„æŠ¥å‘Šåˆ†æå™¨ - æ”¯æŒæ–­ç‚¹ç»­è·‘')
    parser.add_argument('--input-dir', default='./results/readonly-original',
                       help='è¾“å…¥ç›®å½• (é»˜è®¤: ./results/readonly-original)')
    parser.add_argument('--output-dir', default='./results/batch-analysis-results',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: ./results/batch-analysis-results)')
    parser.add_argument('--limit', type=int,
                       help='é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰¹é‡åˆ†æå™¨
    analyzer = SimpleBatchAnalyzer(
        input_dir=args.input_dir,
        output_dir=args.output_dir
    )
    
    # è¿è¡Œæ‰¹é‡åˆ†æ
    analyzer.run_batch_analysis(limit=args.limit)


if __name__ == "__main__":
    main()