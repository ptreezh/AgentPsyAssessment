#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆæ‰¹é‡åˆ†æè„šæœ¬ - æé«˜å¤„ç†æ•ˆç‡
"""

import asyncio
import json
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import argparse
from iflow_sdk_evaluator import IFlowSDKEvaluator, IFlowBatchProcessor


class OptimizedBatchAnalyzer:
    """ä¼˜åŒ–ç‰ˆæ‰¹é‡åˆ†æå™¨"""
    
    def __init__(self, model: str = "deepseek-v3.2-exp", batch_size: int = 10):
        self.model = model
        self.batch_size = batch_size
        self.processed_files = 0
        self.successful_files = 0
        self.failed_files = 0
        self.start_time = datetime.now()
    
    def load_assessment_file(self, file_path: Path) -> Optional[List[Dict]]:
        """åŠ è½½æµ‹è¯„æ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            questions = []
            if isinstance(data, dict) and 'assessment_results' in data:
                assessment_results = data['assessment_results']
                
                for item in assessment_results:
                    if isinstance(item, dict) and 'question_data' in item and 'conversation_log' in item:
                        question_data = item['question_data']
                        conversation_log = item['conversation_log']
                        
                        question = None
                        answer = None
                        
                        for log_item in conversation_log:
                            if log_item.get('role') == 'user':
                                content = log_item.get('content', '')
                                if '[ASSESSMENT_QUESTION]' in content:
                                    question = content.replace('[ASSESSMENT_QUESTION]', '').strip()
                            elif log_item.get('role') == 'assistant':
                                answer = log_item.get('content', '')
                                if '\n\n' in answer:
                                    answer = answer.split('\n\n')[-1].strip()
                        
                        if question and answer:
                            questions.append({
                                'question': question,
                                'answer': answer,
                                'dimension': question_data.get('dimension', ''),
                                'question_id': question_data.get('question_id', '')
                            })
            
            return questions
        
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶ {file_path.name} å¤±è´¥: {e}")
            return None
    
    def create_segments(self, questions: List[Dict], segment_size: int = 5) -> List[List[Dict]]:
        """å°†é—®é¢˜åˆ—è¡¨åˆ†æˆ5é¢˜åˆ†æ®µ"""
        segments = []
        for i in range(0, len(questions), segment_size):
            segment = questions[i:i + segment_size]
            if segment:
                segments.append(segment)
        return segments
    
    async def analyze_single_file(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        
        self.processed_files += 1
        print(f"\nğŸ“Š [{self.processed_files}] åˆ†ææ–‡ä»¶: {file_path.name}")
        
        # åŠ è½½æ•°æ®
        questions = self.load_assessment_file(file_path)
        if not questions:
            self.failed_files += 1
            return {
                'success': False,
                'error': f'æ— æ³•åŠ è½½æˆ–è§£ææ–‡ä»¶: {file_path.name}',
                'file': str(file_path)
            }
        
        print(f"   æ‰¾åˆ° {len(questions)} ä¸ªé—®ç­”å¯¹")
        
        # åˆ›å»ºåˆ†æ®µ
        segments = self.create_segments(questions, segment_size=5)
        print(f"   åˆ†æˆ {len(segments)} ä¸ªåˆ†æ®µ")
        
        # æ‰¹é‡å¤„ç†
        try:
            processor = IFlowBatchProcessor(model=self.model)
            segment_results = await processor.batch_evaluate(segments)
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_scores = processor.calculate_final_scores(segment_results['results'])
            
            if final_scores['success']:
                self.successful_files += 1
                scores = final_scores['big5_scores']
                score_str = ", ".join([f"{trait[0].upper()}:{score}" for trait, score in scores.items()])
                print(f"   âœ… åˆ†ææˆåŠŸ: {score_str}")
            else:
                self.failed_files += 1
                print(f"   âŒ åˆ†æå¤±è´¥: {final_scores.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            return {
                'success': True,
                'file': str(file_path),
                'total_questions': len(questions),
                'segments': len(segments),
                'segment_results': segment_results,
                'final_scores': final_scores,
                'analysis_time': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.failed_files += 1
            print(f"   âŒ åˆ†æå¼‚å¸¸: {e}")
            return {
                'success': False,
                'error': str(e),
                'file': str(file_path)
            }
    
    async def batch_analyze_files(self, results_dir: str) -> Dict[str, Any]:
        """æ‰¹é‡åˆ†æç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶"""
        
        results_path = Path(results_dir)
        if not results_path.exists():
            return {
                'success': False,
                'error': f'ç›®å½•ä¸å­˜åœ¨: {results_dir}'
            }
        
        # æŸ¥æ‰¾æ‰€æœ‰ JSON æ–‡ä»¶
        json_files = list(results_path.glob("*.json"))
        if not json_files:
            return {
                'success': False,
                'error': f'åœ¨ {results_dir} ä¸­æ²¡æœ‰æ‰¾åˆ° JSON æ–‡ä»¶'
            }
        
        print(f"ğŸ” æ‰¾åˆ° {len(json_files)} ä¸ª JSON æ–‡ä»¶")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"âš¡ å¼€å§‹æ‰¹é‡åˆ†æ...")
        
        results = {}
        for file_path in json_files:
            file_result = await self.analyze_single_file(file_path)
            results[file_path.name] = file_result
            
            # æ˜¾ç¤ºè¿›åº¦
            if self.processed_files % 10 == 0:
                elapsed_time = (datetime.now() - self.start_time).total_seconds()
                print(f"\nğŸ“ˆ è¿›åº¦: {self.processed_files}/{len(json_files)} (æˆåŠŸ: {self.successful_files}, å¤±è´¥: {self.failed_files})")
                print(f"â±ï¸  å·²ç”¨æ—¶: {elapsed_time:.1f}ç§’")
        
        # ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = (datetime.now() - self.start_time).total_seconds()
        
        return {
            'success': True,
            'total_files': len(json_files),
            'successful_files': self.successful_files,
            'failed_files': self.failed_files,
            'processed_files': self.processed_files,
            'processing_time': elapsed_time,
            'results': results,
            'analysis_time': datetime.now().isoformat(),
            'model_used': self.model
        }
    
    def save_results(self, results: Dict[str, Any], output_file: str = None):
        """ä¿å­˜åˆ†æç»“æœ"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"optimized_iflow_batch_analysis_results_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def print_summary(self, results: Dict[str, Any]):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        
        print("\n" + "="*60)
        print("ğŸ“‹ æ‰¹é‡åˆ†ææ‘˜è¦")
        print("="*60)
        
        if not results.get('success', False):
            print(f"âŒ åˆ†æå¤±è´¥: {results.get('error', 'æœªçŸ¥é”™è¯¯')}")
            return
        
        total_files = results['total_files']
        successful_files = results['successful_files']
        failed_files = results['failed_files']
        processing_time = results['processing_time']
        
        print(f"ğŸ“Š æ–‡ä»¶æ€»æ•°: {total_files}")
        print(f"âœ… æˆåŠŸåˆ†æ: {successful_files}")
        print(f"âŒ åˆ†æå¤±è´¥: {failed_files}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {results.get('model_used', 'æœªçŸ¥')}")
        print(f"â±ï¸  æ€»ç”¨æ—¶: {processing_time:.1f}ç§’")
        print(f"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {total_files/processing_time*60:.1f} æ–‡ä»¶/åˆ†é’Ÿ")
        
        if successful_files > 0:
            print("\nğŸ“ˆ æˆåŠŸæ–‡ä»¶çš„å¤§äº”äººæ ¼å¹³å‡åˆ†:")
            
            all_scores = []
            for filename, file_result in results['results'].items():
                if file_result.get('success', False) and file_result.get('final_scores', {}).get('success', False):
                    scores = file_result['final_scores']['big5_scores']
                    all_scores.append(scores)
            
            if all_scores:
                print("\nğŸ“Š æ‰€æœ‰æ–‡ä»¶å¹³å‡åˆ†:")
                avg_scores = {}
                for trait in ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                    avg_score = sum(s[trait] for s in all_scores) / len(all_scores)
                    avg_scores[trait] = round(avg_score, 2)
                    print(f"     {trait:15}: {avg_scores[trait]}")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆ iFlow SDK æ‰¹é‡åˆ†æå™¨')
    parser.add_argument('--dir', default='results/results', help='ç»“æœç›®å½•è·¯å¾„ (é»˜è®¤: results/results)')
    parser.add_argument('--model', default='deepseek-v3.2-exp', help='æ¨¡å‹åç§° (é»˜è®¤: deepseek-v3.2-exp)')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆ iFlow SDK æ‰¹é‡åˆ†æ")
    
    analyzer = OptimizedBatchAnalyzer(model=args.model)
    
    # æ‰¹é‡åˆ†æ
    results = await analyzer.batch_analyze_files(args.dir)
    
    # ä¿å­˜ç»“æœ
    output_file = analyzer.save_results(results, args.output)
    
    # æ‰“å°æ‘˜è¦
    analyzer.print_summary(results)


if __name__ == "__main__":
    asyncio.run(main())