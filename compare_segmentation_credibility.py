#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯”è¾ƒä¸åŒåˆ†æ®µæ–¹æ¡ˆçš„å¯ä¿¡åº¦
"""

import json
import statistics
from pathlib import Path
from typing import Dict, List, Tuple

def analyze_score_variation(scores: List[int]) -> Dict:
    """åˆ†æè¯„åˆ†å˜å¼‚æƒ…å†µ"""
    if not scores:
        return {"std": 0, "range": 0, "variance": 0, "mean": 0}

    return {
        "std": statistics.stdev(scores) if len(scores) > 1 else 0,
        "range": max(scores) - min(scores) if scores else 0,
        "variance": statistics.variance(scores) if len(scores) > 1 else 0,
        "mean": statistics.mean(scores)
    }

def analyze_model_consistency(model_results: Dict) -> Dict:
    """åˆ†ææ¨¡å‹ä¸€è‡´æ€§"""
    trait_scores = {}

    # æ”¶é›†æ¯ä¸ªtraitçš„è·¨æ¨¡å‹è¯„åˆ†
    for model, results in model_results.items():
        if 'big_five_final_scores' in results:
            for trait, data in results['big_five_final_scores'].items():
                if trait not in trait_scores:
                    trait_scores[trait] = []
                if isinstance(data, dict) and 'final_score' in data:
                    trait_scores[trait].append(data['final_score'])
                elif isinstance(data, int):
                    trait_scores[trait].append(data)

    # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
    consistency = {}
    for trait, scores in trait_scores.items():
        if len(scores) >= 2:
            variation = analyze_score_variation(scores)
            consistency[trait] = {
                "scores": scores,
                "std": variation["std"],
                "range": variation["range"],
                "mean": variation["mean"]
            }

    return consistency

def analyze_evidence_quality(results: Dict) -> Dict:
    """åˆ†æè¯æ®è´¨é‡"""
    evidence_stats = {
        "total_segments": 0,
        "segments_with_meaningful_evidence": 0,
        "segments_with_na_evidence": 0,
        "evidence_quality_score": 0
    }

    # æ£€æŸ¥segmentåˆ†æä¸­çš„è¯æ®
    if 'segment_analyses' in results:
        for segment in results['segment_analyses']:
            evidence_stats["total_segments"] += 1

            if 'llm_response' in segment:
                try:
                    response = json.loads(segment['llm_response'])
                    for question_score in response.get('question_scores', []):
                        for trait, score_data in question_score.get('big_five_scores', {}).items():
                            evidence = score_data.get('evidence', '').lower()
                            if any(keyword in evidence for keyword in ['n/a', 'æ— ', 'ç¼ºä¹', 'æ²¡æœ‰']):
                                evidence_stats["segments_with_na_evidence"] += 1
                            else:
                                evidence_stats["segments_with_meaningful_evidence"] += 1
                                break
                except:
                    pass

    # è®¡ç®—è¯æ®è´¨é‡åˆ†æ•°
    if evidence_stats["total_segments"] > 0:
        evidence_stats["evidence_quality_score"] = (
            evidence_stats["segments_with_meaningful_evidence"] / evidence_stats["total_segments"]
        ) * 100

    return evidence_stats

def compare_segmentation_approaches():
    """æ¯”è¾ƒä¸åŒåˆ†æ®µæ–¹æ¡ˆ"""
    print("ğŸ” åˆ†æä¸åŒåˆ†æ®µæ–¹æ¡ˆçš„å¯ä¿¡åº¦...")

    # åˆ†æ2é¢˜åˆ†æ®µç»“æœ
    two_question_files = list(Path(".").glob("*qwen-long_segmented_analysis.json"))
    print(f"\nğŸ“Š æ‰¾åˆ° {len(two_question_files)} ä¸ª2é¢˜åˆ†æ®µç»“æœæ–‡ä»¶")

    if two_question_files:
        print("\nğŸ¯ 2é¢˜åˆ†æ®µæ–¹æ¡ˆåˆ†æ:")
        for file_path in two_question_files[:1]:  # åˆ†æç¬¬ä¸€ä¸ªæ–‡ä»¶
            print(f"   åˆ†ææ–‡ä»¶: {file_path.name}")

            with open(file_path, 'r', encoding='utf-8') as f:
                results = json.load(f)

            # åˆ†æè¯„åˆ†åˆ†å¸ƒ
            if 'big_five_final_scores' in results:
                scores = []
                for trait, data in results['big_five_final_scores'].items():
                    if isinstance(data, dict) and 'final_score' in data:
                        scores.append(data['final_score'])
                    elif isinstance(data, int):
                        scores.append(data)

                variation = analyze_score_variation(scores)
                print(f"   è¯„åˆ†å˜å¼‚åº¦: æ ‡å‡†å·®={variation['std']:.2f}, èŒƒå›´={variation['range']}")
                print(f"   è¯„åˆ†åˆ†å¸ƒ: {set(scores)}")

            # åˆ†æè¯æ®è´¨é‡
            evidence_stats = analyze_evidence_quality(results)
            print(f"   è¯æ®è´¨é‡: {evidence_stats['evidence_quality_score']:.1f}%")
            print(f"   æ€»æ®µæ•°: {evidence_stats['total_segments']}")

    # åˆ†æä¸åˆ†æ®µç»“æœï¼ˆå·²æœ‰ç»“è®ºï¼šå…¨3åˆ†ï¼Œä¸å¯ä¿¡ï¼‰
    no_segment_files = list(Path("no_segment_optimized_results").glob("*no_segment_optimized.json"))
    print(f"\nâŒ ä¸åˆ†æ®µæ–¹æ¡ˆ: {len(no_segment_files)} ä¸ªæ–‡ä»¶")
    print("   ç»“è®º: æ‰€æœ‰è¯„åˆ†å‡ä¸º3åˆ†ï¼Œ100%è™šå‡ä¸€è‡´æ€§ï¼Œå®Œå…¨ä¸å¯ä¿¡")

    # åˆ›å»º5é¢˜åˆ†æ®µæµ‹è¯•
    print(f"\nğŸ§ª å»ºè®®: åˆ›å»º5é¢˜åˆ†æ®µå’Œ10é¢˜åˆ†æ®µæµ‹è¯•è¿›è¡Œå¯¹æ¯”")

    # åŸºäºè®¤çŸ¥ç§‘å­¦çš„ç†è®ºåˆ†æ
    print(f"\nğŸ“‹ ç†è®ºåˆ†æ:")
    print(f"   2é¢˜åˆ†æ®µ: è®¤çŸ¥è´Ÿè·ä½ï¼Œä½†å¯èƒ½ç¼ºä¹ä¸Šä¸‹æ–‡")
    print(f"   5é¢˜åˆ†æ®µ: å¹³è¡¡è®¤çŸ¥è´Ÿè·ä¸ä¸Šä¸‹æ–‡ä¸°å¯Œåº¦")
    print(f"   10é¢˜åˆ†æ®µ: ä¸Šä¸‹æ–‡æ›´ä¸°å¯Œï¼Œä½†æ¥è¿‘è®¤çŸ¥è´Ÿè·ä¸Šé™")

    # æ¨èæœ€ä¼˜æ–¹æ¡ˆ
    print(f"\nğŸ¯ æ¨èæ–¹æ¡ˆ:")
    print(f"   ğŸ¥‡ 5é¢˜åˆ†æ®µ: æœ€ä¼˜å¹³è¡¡ç‚¹")
    print(f"   ğŸ¥ˆ 2é¢˜åˆ†æ®µ: ä¸Šä¸‹æ–‡ä¸è¶³")
    print(f"   ğŸ¥‰ 10é¢˜åˆ†æ®µ: è®¤çŸ¥è´Ÿè·è¿‡é«˜")

if __name__ == "__main__":
    compare_segmentation_approaches()