#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆæ‰¹é‡å¯ä¿¡è¯„ä¼°åˆ†æå™¨
ä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹ä½œä¸ºä¸»è¦è¯„ä¼°å™¨å¤„ç†æ‰€æœ‰åŸå§‹æµ‹è¯„æŠ¥å‘Š
"""
import sys
import os
import json
import glob
from pathlib import Path
from datetime import datetime
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from segmented_scoring_evaluator import SegmentedScoringEvaluator


def run_fixed_batch_analysis(input_dir="results/readonly-original", output_dir="fixed_segmented_scoring_results", max_files=None):
    """
    è¿è¡Œä¿®å¤ç‰ˆæ‰¹é‡åˆ†æï¼Œä½¿ç”¨Ollamaæœ¬åœ°æ¨¡å‹ä½œä¸ºä¸»è¦è¯„ä¼°å™¨
    """
    print("="*60)
    print("ğŸš€ ä¿®å¤ç‰ˆæ‰¹é‡å¯ä¿¡è¯„ä¼°åˆ†æå™¨")
    print("="*60)
    print("ğŸ“‹ ç³»ç»Ÿé…ç½®:")
    print("   ğŸ¦™ Ollamaæœ¬åœ°æ¨¡å‹: ä½œä¸ºä¸»è¦è¯„ä¼°å™¨")
    print("   â˜ï¸  OpenRouter API: ç”±äºAPIå¯†é’¥æ³„éœ²å·²åœç”¨")
    print("   ğŸ“ è¯„ä¼°æ–¹æ³•: 5é¢˜åˆ†æ®µç‹¬ç«‹è¯„ä¼°")
    print("   ğŸ¤– ä¸»è¦æ¨¡å‹: qwen3:4b, gemma2:2b, llama3.2:3b")
    print()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_pattern = os.path.join(input_dir, "*.json")
    all_files = glob.glob(json_pattern)
    
    if not all_files:
        print(f"âŒ åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°JSONæ–‡ä»¶")
        return
    
    # é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    files_to_process = all_files[:max_files] if max_files else all_files
    
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š æ‰¾åˆ° {len(all_files)} ä¸ªJSONæ–‡ä»¶ï¼Œå°†å¤„ç† {len(files_to_process)} ä¸ª")
    print()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = SegmentedScoringEvaluator()
    
    # ç»Ÿè®¡ä¿¡æ¯
    processed_count = 0
    success_count = 0
    failed_count = 0
    total_consistency = 0
    total_reliability = 0
    passed_reliability_count = 0
    
    start_time = time.time()
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for i, file_path in enumerate(files_to_process, 1):
        filename = os.path.basename(file_path)
        print(f"ğŸ“ˆ [{i}/{len(files_to_process)}] å¤„ç†: {filename}")
        
        try:
            # æ‰§è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨Ollamaæ¨¡å¼ï¼‰
            result = evaluator.evaluate_file_with_multiple_models(file_path, output_dir)
            
            if result['success']:
                processed_count += 1
                success_count += 1
                
                consistency_score = result.get('consistency_score', 0)
                reliability_score = result.get('reliability_score', 0)
                reliability_passed = result.get('reliability_passed', False)
                
                total_consistency += consistency_score
                total_reliability += reliability_score
                
                if reliability_passed:
                    passed_reliability_count += 1
                
                print(f"   âœ… ä¸€è‡´æ€§: {consistency_score:.2f}%")
                print(f"   âœ… ä¿¡åº¦: {reliability_score:.2f}%")
                print(f"   âœ… ä¿¡åº¦éªŒè¯: {'é€šè¿‡' if reliability_passed else 'æœªé€šè¿‡'}")
                print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜: {result['output_path']}")
            else:
                processed_count += 1
                failed_count += 1
                error_msg = result.get('error', 'Unknown error')
                print(f"   âŒ å¤„ç†å¤±è´¥: {error_msg}")
                
        except Exception as e:
            processed_count += 1
            failed_count += 1
            print(f"   âŒ å¼‚å¸¸: {str(e)}")
            continue
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # æ‰“å°æ±‡æ€»ç»Ÿè®¡
    print(f"\n" + "="*60)
    print(f"ğŸ“Š ä¿®å¤ç‰ˆæ‰¹é‡åˆ†æå®ŒæˆæŠ¥å‘Š")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â° ç»“æŸæ—¶é—´: {datetime.fromtimestamp(end_time).strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {len(files_to_process)}")
    print(f"âœ… æˆåŠŸå¤„ç†: {success_count}")
    print(f"âŒ å¤„ç†å¤±è´¥: {failed_count}")
    print(f"ğŸ¯ æˆåŠŸç‡: {(success_count/len(files_to_process))*100:.1f}%" if len(files_to_process) > 0 else "N/A")
    
    if success_count > 0:
        avg_consistency = total_consistency / success_count
        avg_reliability = total_reliability / success_count
        
        print(f"ğŸ“ˆ å¹³å‡ä¸€è‡´æ€§: {avg_consistency:.2f}%")
        print(f"âœ… ä¿¡åº¦éªŒè¯é€šè¿‡ç‡: {passed_reliability_count}/{success_count} ({(passed_reliability_count/success_count)*100:.1f}%)")
        print(f"ğŸ“ˆ å¹³å‡ä¿¡åº¦: {avg_reliability:.2f}%")
    
    print(f"ğŸ’¾ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("="*60)


def main():
    """
    ä¸»å‡½æ•°
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¿®å¤ç‰ˆæ‰¹é‡å¯ä¿¡è¯„ä¼°åˆ†æå™¨')
    parser.add_argument('--input_dir', type=str, default='results/readonly-original',
                        help='è¾“å…¥ç›®å½•è·¯å¾„ (é»˜è®¤: results/readonly-original)')
    parser.add_argument('--output_dir', type=str, default='fixed_segmented_scoring_results',
                        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: fixed_segmented_scoring_results)')
    parser.add_argument('--max_files', type=int, 
                        help='æœ€å¤§å¤„ç†æ–‡ä»¶æ•° (å¯é€‰ï¼Œç”¨äºæµ‹è¯•)')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œæ‰¹é‡åˆ†æ
    run_fixed_batch_analysis(args.input_dir, args.output_dir, args.max_files)


if __name__ == "__main__":
    main()