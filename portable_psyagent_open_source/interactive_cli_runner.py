import sys
import os
import json
import argparse
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime

# Add the parent directory to the Python path so we can import modules
project_root = os.path.join(os.path.dirname(__file__), '..')
sys.path.insert(0, project_root)

# Ensure UTF-8 encoding for stdout/stderr on Windows
if sys.platform.startswith('win'):
    import io
    if sys.stdout.encoding != 'utf-8':
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    if sys.stderr.encoding != 'utf-8':
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# Import i18n support
from i18n import i18n

# Correct, absolute imports from the project root
from llm_assessment.services.llm_client import LLMClient
from llm_assessment.services.model_manager import ModelManager
from llm_assessment.run_assessment_unified import (
    test_model_connectivity,
    load_test_data,
    load_role_prompt,
    run_assessment,
    save_results,
    TEST_ABBREVIATIONS
)

# Constants
ROLES_DIR = "llm_assessment/roles"
TESTS_DIR = "llm_assessment/test_files"
RESULTS_DIR = "results"
AVAILABLE_MODELS_FILE = "available_models.txt"


def load_available_models() -> Dict[str, Dict[str, str]]:
    """Load available models from configuration file"""
    models = {}
    if os.path.exists(AVAILABLE_MODELS_FILE):
        with open(AVAILABLE_MODELS_FILE, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    parts = line.split('|')
                    if len(parts) >= 3:
                        model_id, status, last_tested = parts[0], parts[1], parts[2]
                        models[model_id] = {
                            'status': status,
                            'last_tested': last_tested
                        }
    return models


def save_available_models(models: Dict[str, Dict[str, str]]) -> None:
    """Save available models to configuration file"""
    with open(AVAILABLE_MODELS_FILE, 'w', encoding='utf-8') as f:
        f.write("# Available models configuration\n")
        f.write("# This file is automatically generated and updated by the test runner\n")
        f.write("# Format: model_id|status|last_tested\n")
        f.write("# Status: available, unavailable, unknown\n\n")
        
        today = datetime.now().strftime('%Y-%m-%d')
        for model_id, info in models.items():
            status = info.get('status', 'unknown')
            last_tested = info.get('last_tested', today)
            f.write(f"{model_id}|{status}|{last_tested}\n")


def get_available_tests(language='en') -> List[str]:
    """
    Get list of available test files, filtered by language.
    
    Args:
        language (str): Language code ('en' or 'zh')
        
    Returns:
        List[str]: List of available test file names
    """
    tests = []
    if os.path.exists(TESTS_DIR):
        for file in os.listdir(TESTS_DIR):
            if file.endswith('.json') and 'archive' not in file:
                test_name = file
                # Filter by language
                if language == 'en':
                    # For English, include files with _en suffix
                    if '_en' in test_name:
                        tests.append(test_name)
                else:
                    # For Chinese, include files without _en suffix
                    if '_en' not in test_name:
                        tests.append(test_name)
    return tests


def get_available_roles(language='en') -> List[str]:
    """
    Get list of available role files, filtered by language.
    
    Args:
        language (str): Language code ('en' or 'zh')
        
    Returns:
        List[str]: List of available role names
    """
    roles = ['default']  # Always include default option
    if os.path.exists(ROLES_DIR):
        for file in os.listdir(ROLES_DIR):
            if file.endswith('.txt'):
                role_name = file[:-4]  # Remove .txt extension
                # Filter by language
                if language == 'en':
                    # For English, include files with _en suffix
                    if '_en' in role_name:
                        roles.append(role_name)
                else:
                    # For Chinese, include files without _en suffix
                    if '_en' not in role_name:
                        roles.append(role_name)
    return roles


def select_multiple_options(options: List[str], prompt: str) -> List[str]:
    """Let user select multiple options from a list"""
    print(f"\n{prompt}")
    for i, option in enumerate(options, 1):
        print(f"  {i}. {option}")
    
    while True:
        try:
            selection = input(f"\n{i18n.t('Please enter option numbers (multiple selections allowed, separated by commas, enter')} 'all' {i18n.t('to select all')}): ").strip()
            if selection.lower() == 'all':
                return options
            
            indices = [int(x.strip()) - 1 for x in selection.split(',')]
            if all(0 <= idx < len(options) for idx in indices):
                return [options[idx] for idx in indices]
            else:
                print(i18n.t("Input error, please enter valid option numbers."))
        except ValueError:
            print(i18n.t("Input error, please enter valid option numbers or 'all'."))
        except EOFError:
            # When EOF is encountered (e.g., in non-interactive environments)
            print(f"\n{i18n.t('Unable to read user input, will use all options as default selection.')}")
            return options


def select_single_option(options: List[str], prompt: str) -> str:
    """Let user select a single option from a list"""
    print(f"\n{prompt}")
    for i, option in enumerate(options, 1):
        print(f"  {i}. {option}")
    
    while True:
        try:
            selection = int(input(f"\n{i18n.t('Please enter option number')}: ").strip()) - 1
            if 0 <= selection < len(options):
                return options[selection]
            else:
                print(i18n.t("Input error, please enter a valid option number."))
        except ValueError:
            print(i18n.t("Input error, please enter a valid option number."))
        except EOFError:
            # When EOF is encountered (e.g., in non-interactive environments)
            print(f"\n{i18n.t('Unable to read user input, will use the first option as default selection.')}")
            return options[0] if options else ""


def run_tests(client, selected_models, selected_tests, selected_roles, stress_config=None):
    """Run tests with selected models, tests, and roles"""
    # Run tests
    print(f"\n=== {i18n.t('Starting test execution')} ===")
    for model in selected_models:
        # For cloud models, we need to load the model first
        if not model.startswith("ollama/") and "/" in model:
            print(f"{i18n.t('Loading cloud model')}: {model}")
            model_manager = ModelManager()
            if not model_manager.load_model(model):
                print(f"  !! {i18n.t('Unable to load cloud model')}: {model}")
                continue
            # Set the current model in the manager
            model_manager.current_model = model
        
        for test_name in selected_tests:
            for role_name in selected_roles:
                print(f"\n-> {i18n.t('Testing model')}: {model}, {i18n.t('test')}: {test_name}, {i18n.t('role')}: {role_name}")
                
                # Load test data
                # Handle test file abbreviation
                if test_name in TEST_ABBREVIATIONS:
                    test_file_path = os.path.join(TESTS_DIR, TEST_ABBREVIATIONS[test_name])
                else:
                    test_file_path = test_name
                
                try:
                    test_data = load_test_data(test_file_path)
                except Exception as e:
                    print(f"  !! {i18n.t('Failed to load test file')}: {e}")
                    continue
                
                # Prepare configuration for run_assessment
                config = {
                    'role_name': role_name,
                    'test_file': test_name,
                    'debug': False
                }
                
                # Add stress testing parameters if provided
                if stress_config:
                    config.update(stress_config)
                
                # Run assessment
                try:
                    results = run_assessment(client, model, test_data, config, args.debug)
                    
                    # Save results
                    save_results(results, model, test_name, role_name,
                               config.get('emotional_stress_level', 0),
                               config.get('cognitive_trap_type'),
                               config.get('context_load_tokens', 0))
                except Exception as e:
                    print(f"  !! {i18n.t('Error during test execution')}: {e}")
                    continue
    
    print(f"\n=== {i18n.t('All tests completed')} ===")


def select_stress_testing_parameters():
    """Prompts user to select stress testing parameters."""
    print(f"\n--- {i18n.t('Advanced stress testing parameters')} ---")
    
    # Emotional stress level selection
    emotional_stress_levels = list(range(0, 5))  # 0-4
    print(f"\n{i18n.t('Select emotional stress level (0=none)')}):")
    for i, level in enumerate(emotional_stress_levels):
        level_desc = [i18n.t("none"), i18n.t("low"), i18n.t("medium"), i18n.t("high"), i18n.t("extreme")][level]
        print(f"  {i}. {level} ({level_desc})")
    
    while True:
        try:
            emotional_choice = input(i18n.t("Please select emotional stress level (0-4): ")).strip()
            emotional_level = int(emotional_choice)
            if 0 <= emotional_level <= 4:
                break
            else:
                print(i18n.t("Please enter a valid level (0-4)."))
        except ValueError:
            print(i18n.t("Please enter a valid number."))
    
    # Cognitive trap type selection
    cognitive_trap_types = {
        'p': i18n.t('paradox (p)'),
        'c': i18n.t('circularity (c)'),
        's': i18n.t('semantic_fallacy (s)'),
        'r': i18n.t('procedural (r)')
    }
    print(f"\n{i18n.t('Select cognitive trap type:')}")
    print(f"  0. {i18n.t('none')}")
    for key, desc in cognitive_trap_types.items():
        print(f"  {key}. {desc}")
    
    while True:
        cognitive_choice = input(i18n.t("Please select cognitive trap type (0/p/c/s/r): ")).strip().lower()
        if cognitive_choice == '0':
            cognitive_trap = None
            break
        elif cognitive_choice in cognitive_trap_types:
            cognitive_trap = cognitive_choice
            break
        else:
            print(i18n.t("Please enter a valid option."))
    
    # Context load tokens selection
    context_load_options = [0, 512, 1024, 2048, 4096]
    print(f"\n{i18n.t('Select context load token count (0=none)')}):")
    for i, tokens in enumerate(context_load_options):
        print(f"  {i}. {tokens} tokens")
    
    while True:
        try:
            context_choice = input(i18n.t("Please select context load token count (enter option number): ")).strip()
            context_index = int(context_choice)
            if 0 <= context_index < len(context_load_options):
                context_load = context_load_options[context_index]
                break
            else:
                print(i18n.t("Please enter a valid option number."))
        except ValueError:
            print(i18n.t("Please enter a valid number."))
    
    # New tmpr setting selection
    print(f"\n{i18n.t('Select model tmpr setting (0=default)')}:")
    tmpr_options = [None, 0.0, 0.3, 0.8, 1.3]
    tmpr_descriptions = [
        i18n.t("Default Temperature (Model Default)"),
        i18n.t("0 (Deterministic)"),
        i18n.t("0.3 (Low Creativity)"), 
        i18n.t("0.8 (Medium Creativity)"),
        i18n.t("1.3 (High Creativity)")
    ]
    for i, (temp, desc) in enumerate(zip(tmpr_options, tmpr_descriptions)):
        print(f"  {i}. {desc}")
    
    while True:
        try:
            temp_choice = input(i18n.t("Please select tmpr setting (0-4): ")).strip()
            temp_index = int(temp_choice)
            if 0 <= temp_index < len(tmpr_options):
                tmpr = tmpr_options[temp_index]
                break
            else:
                print(i18n.t("Please enter a valid option number (0-4)."))
        except ValueError:
            print(i18n.t("Please enter a valid number."))
    
    # New context length mode selection
    print(f"\n{i18n.t('Select context length mode')}:")
    context_length_modes = ['auto', 'static', 'dynamic']
    context_length_mode_descriptions = [
        i18n.t("Auto (Detect model capabilities)"),
        i18n.t("Static (Fixed length)"),
        i18n.t("Dynamic (Percentage-based)")
    ]
    for i, (mode, desc) in enumerate(zip(context_length_modes, context_length_mode_descriptions)):
        print(f"  {i}. {desc}")
    
    while True:
        try:
            mode_choice = input(i18n.t("Please select context length mode (0-2): ")).strip()
            mode_index = int(mode_choice)
            if 0 <= mode_index < len(context_length_modes):
                context_length_mode = context_length_modes[mode_index]
                break
            else:
                print(i18n.t("Please enter a valid option number (0-2)."))
        except ValueError:
            print(i18n.t("Please enter a valid number."))
    
    # New static context length selection (only if static mode selected)
    static_context_length = 0
    if context_length_mode == 'static':
        print(f"\n{i18n.t('Select static context length')}:")
        static_context_options = [0, 2, 4, 8, 16, 32, 64]
        static_context_descriptions = [
            i18n.t("0K (No context)"),
            i18n.t("2K Context"),
            i18n.t("4K Context"),
            i18n.t("8K Context"),
            i18n.t("16K Context"),
            i18n.t("32K Context"),
            i18n.t("64K Context")
        ]
        for i, (length, desc) in enumerate(zip(static_context_options, static_context_descriptions)):
            print(f"  {i}. {desc}")
        
        while True:
            try:
                static_choice = input(i18n.t("Please select static context length (0-6): ")).strip()
                static_index = int(static_choice)
                if 0 <= static_index < len(static_context_options):
                    static_context_length = static_context_options[static_index]
                    break
                else:
                    print(i18n.t("Please enter a valid option number (0-6)."))
            except ValueError:
                print(i18n.t("Please enter a valid number."))
    
    # New dynamic context length selection (only if dynamic mode selected)
    dynamic_context_ratio = '1/2'
    if context_length_mode == 'dynamic':
        print(f"\n{i18n.t('Select dynamic context length ratio')}:")
        dynamic_context_options = ['1/4', '1/2', '3/4', '9/10']
        dynamic_context_descriptions = [
            i18n.t("1/4 of Model Max Context"),
            i18n.t("1/2 of Model Max Context"),
            i18n.t("3/4 of Model Max Context"),
            i18n.t("9/10 of Model Max Context")
        ]
        for i, (ratio, desc) in enumerate(zip(dynamic_context_options, dynamic_context_descriptions)):
            print(f"  {i}. {desc}")
        
        while True:
            try:
                dynamic_choice = input(i18n.t("Please select dynamic context length ratio (0-3): ")).strip()
                dynamic_index = int(dynamic_choice)
                if 0 <= dynamic_index < len(dynamic_context_options):
                    dynamic_context_ratio = dynamic_context_options[dynamic_index]
                    break
                else:
                    print(i18n.t("Please enter a valid option number (0-3)."))
            except ValueError:
                print(i18n.t("Please enter a valid number."))
    
    return {
        'emotional_stress_level': emotional_level,
        'cognitive_trap_type': cognitive_trap,
        'context_load_tokens': context_load,
        'tmpr': tmpr,
        'context_length_mode': context_length_mode,
        'context_length_static': static_context_length,
        'context_length_dynamic': dynamic_context_ratio
    }


def main():
    """Main interactive test runner"""
    parser = argparse.ArgumentParser(description=i18n.t('Run psychological assessments with LLMs'))
    parser.add_argument("--models", nargs="*", help=i18n.t('List of models to test (e.g., \'qwen3:8b\' \'llama3:instruct\')'))
    parser.add_argument("--tests", nargs="*", help=i18n.t('List of tests to run (e.g., \'big5\' \'agent-big-five-50-complete.json\')'))
    parser.add_argument("--roles", nargs="*", help=i18n.t('List of roles to use (e.g., \'default\' \'a1\' \'1\')'))
    parser.add_argument("--all-models", action="store_true", help=i18n.t('Use all available models'))
    parser.add_argument("--all-tests", action="store_true", help=i18n.t('Use all available tests'))
    parser.add_argument("--all-roles", action="store_true", help=i18n.t('Use all available roles'))
    parser.add_argument("--stress-test", action="store_true", help=i18n.t('Enable stress testing parameters'))
    
    args = parser.parse_args()
    
    print(f"=== AgentPsy {i18n.t('Psychological Assessment Interactive Test Tool')} ===")
    
    # Initialize client with no specific provider to get all models
    client = LLMClient()
    # Temporarily set provider to empty string to get all models
    original_provider = client.provider
    client.provider = ""
    print(f"\n{i18n.t('Current configured Provider')}: {original_provider}")
    
    # Load previously tested models
    available_models_config = load_available_models()
    
    # Get all available models from both local and cloud
    all_models = client.list_models()
    print(f"{i18n.t('All available models')}: {all_models}")
    
    # Restore original provider
    client.provider = original_provider
    
    # Determine which models to test
    models_to_test = []
    models_status = {}
    
    # Check if we have recent test results for any models
    today = datetime.now().strftime('%Y-%m-%d')
    for model in all_models:
        if model in available_models_config:
            model_info = available_models_config[model]
            # If tested today, use the cached result
            if model_info['last_tested'] == today and model_info['status'] == 'available':
                models_status[model] = 'available'
                print(f"{i18n.t('Using cached result')}: {model} is available")
            else:
                # Need to retest
                models_to_test.append(model)
        else:
            # New model, need to test
            models_to_test.append(model)
    
    # Test models that need testing
    for model in models_to_test:
        print(f"{i18n.t('Testing model connectivity')}: {model}")
        try:
            if test_model_connectivity(model):
                models_status[model] = 'available'
            else:
                models_status[model] = 'unavailable'
        except Exception as e:
            print(f"{i18n.t('Error testing model {model}')}: {e}")
            models_status[model] = 'unavailable'
        
        # Update config
        available_models_config[model] = {
            'status': models_status[model],
            'last_tested': today
        }
    
    # Save updated model status
    save_available_models(available_models_config)
    
    # Filter to only available models
    connectable_models = [model for model, status in models_status.items() if status == 'available']
    
    if not connectable_models:
        print(i18n.t("No connectable models, exiting program."))
        return 1
    
    # Handle command-line arguments
    if args.models:
        # Use specified models
        selected_models = [model for model in args.models if model in connectable_models]
        if not selected_models:
            print(i18n.t("No valid models specified."))
            return 1
    elif args.all_models:
        # Use all available models
        selected_models = connectable_models
    else:
        # Interactive selection
        selected_models = select_multiple_options(
            connectable_models, 
            i18n.t("Please select models to test (multiple selections allowed):")
        )
    
    # Get available tests
    available_tests = get_available_tests()
    if not available_tests:
        print(i18n.t("No available test files found."))
        return 1
    
    # Map abbreviations to full names for display
    test_display_names = []
    test_mapping = {}  # Map display names to actual test names
    for test in available_tests:
        # Find if there's an abbreviation for this test
        abbr = None
        for k, v in TEST_ABBREVIATIONS.items():
            if v == test:
                abbr = k
                break
        
        if abbr:
            display_name = f"{test} ({abbr})"
            test_display_names.append(display_name)
            test_mapping[display_name] = test
        else:
            test_display_names.append(test)
            test_mapping[test] = test
    
    if args.tests:
        # Use specified tests
        selected_tests_display = []
        for test in args.tests:
            # Check if it's an abbreviation
            if test in TEST_ABBREVIATIONS:
                full_name = TEST_ABBREVIATIONS[test]
                # Find display name
                for display_name, actual_name in test_mapping.items():
                    if actual_name == full_name:
                        selected_tests_display.append(display_name)
                        break
            else:
                # Check if it's a full name or display name
                found = False
                for display_name, actual_name in test_mapping.items():
                    if actual_name == test or display_name == test:
                        selected_tests_display.append(display_name)
                        found = True
                        break
                if not found:
                    # Add as is, will be filtered later
                    selected_tests_display.append(test)
        
        # Convert display names back to actual test names
        selected_tests = []
        for display_name in selected_tests_display:
            if display_name in test_mapping:
                selected_tests.append(test_mapping[display_name])
            else:
                # Check if it's a direct match with available tests
                if display_name in available_tests:
                    selected_tests.append(display_name)
    elif args.all_tests:
        # Use all available tests
        selected_tests = available_tests
    else:
        # Interactive selection
        selected_tests_display = select_multiple_options(
            test_display_names,
            i18n.t("Please select tests to run (multiple selections allowed):")
        )
        
        # Convert display names back to actual test names
        selected_tests = []
        for display_name in selected_tests_display:
            if display_name in test_mapping:
                selected_tests.append(test_mapping[display_name])
            else:
                selected_tests.append(display_name)
    
    # Get available roles
    available_roles = get_available_roles()
    
    if args.roles:
        # Use specified roles
        selected_roles = [role for role in args.roles if role in available_roles]
        if not selected_roles:
            print(i18n.t("No valid roles specified."))
            return 1
    elif args.all_roles:
        # Use all available roles
        selected_roles = available_roles
    else:
        # Interactive selection
        selected_roles = select_multiple_options(
            available_roles,
            i18n.t("Please select roles to use (multiple selections allowed, 'default' means no specific role):")
        )
    
    # Handle stress testing parameters
    stress_config = None
    if args.stress_test:
        stress_config = select_stress_testing_parameters()
    else:
        stress_choice = input(f"\n{i18n.t('Enable advanced stress testing parameters? (y/N): ')}").strip().lower()
        if stress_choice == 'y':
            stress_config = select_stress_testing_parameters()
    
    # Run tests
    run_tests(client, selected_models, selected_tests, selected_roles, stress_config)
    return 0


from python_utf8_config import ensure_utf8

if __name__ == '__main__':
    ensure_utf8()
    sys.exit(main())