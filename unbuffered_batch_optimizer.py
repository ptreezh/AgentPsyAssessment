#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ— ç¼“å†²æ‰¹é‡ä¼˜åŒ–å™¨ - è§£å†³è¾“å‡ºå†²çªé—®é¢˜
"""

import sys
import os
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import time

# å¼ºåˆ¶æ— ç¼“å†²è¾“å‡º
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DASHSCOPE_API_KEY'] = 'sk-3f16ac9d87e34ca88bf3925c3651624f'
os.environ['PYTHONUNBUFFERED'] = '1'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class UnbufferedBatchOptimizer:
    def __init__(self, models: List[str] = None):
        self.models = models or ["qwen-max", "deepseek-v3.2-exp", "Moonshot-Kimi-K2-Instruct"]

        # ä¼˜åŒ–å‚æ•° - 5é¢˜åˆ†æ®µï¼Œ1ç§’å»¶è¿Ÿ
        self.segment_size = 5
        self.delay_between_files = 1
        self.delay_between_segments = 1

        # ç¼“å­˜è®¾ç½®
        self.cache_dir = Path("unbuffered_cache")
        self.cache_dir.mkdir(exist_ok=True)

        # è¿›åº¦è·Ÿè¸ª
        self.progress_file = Path("unbuffered_batch_progress.json")
        self.completed_files = set()
        self.failed_files = set()

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'start_time': time.time(),
            'files_processed': 0,
            'segments_processed': 0,
            'cache_hits': 0,
            'api_calls': 0,
            'total_processing_time': 0
        }

        print(f"ğŸš€ æ— ç¼“å†²æ‰¹é‡ä¼˜åŒ–å™¨å·²åˆå§‹åŒ–")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {', '.join(self.models)}")
        print(f"ğŸ“Š ä¼˜åŒ–é…ç½®: {self.segment_size}é¢˜/æ®µ, {self.delay_between_files}så»¶è¿Ÿ")
        print(f"ğŸ”‘ APIå¯†é’¥å·²è®¾ç½®")
        sys.stdout.flush()

    def load_progress(self) -> bool:
        """åŠ è½½è¿›åº¦ä¿¡æ¯"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                    self.completed_files = set(progress.get('completed_files', []))
                    self.failed_files = set(progress.get('failed_files', []))
                    self.stats.update(progress.get('stats', {}))
                    return True
            except Exception as e:
                print(f"âŒ è¿›åº¦åŠ è½½å¤±è´¥: {e}")
                sys.stdout.flush()
        return False

    def save_progress(self):
        """ä¿å­˜è¿›åº¦ä¿¡æ¯"""
        try:
            progress = {
                'models': self.models,
                'completed_files': list(self.completed_files),
                'failed_files': list(self.failed_files),
                'stats': self.stats,
                'last_update': datetime.now().isoformat(),
                'total_processed': len(self.completed_files) + len(self.failed_files)
            }
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ è¿›åº¦ä¿å­˜å¤±è´¥: {e}")
            sys.stdout.flush()

    def _create_cache_key(self, questions: List[Dict], model: str) -> str:
        """åˆ›å»ºç¼“å­˜é”®"""
        content = f"{model}_"
        for q in questions:
            content += f"{q['question'][:50]}_{q['answer'][:50]}_"
        return hashlib.md5(content.encode()).hexdigest()

    def _load_cache(self, cache_key: str) -> Optional[Dict]:
        """åŠ è½½ç¼“å­˜"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                    self.stats['cache_hits'] += 1
                    return result
            except:
                pass
        return None

    def _save_cache(self, cache_key: str, result: Dict):
        """ä¿å­˜ç¼“å­˜"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            sys.stdout.flush()

    def _create_segment_prompt(self, questions: List[Dict], segment_num: int) -> str:
        """åˆ›å»ºåˆ†æ®µåˆ†ææç¤º"""
        prompt = f"""è¯·åˆ†æä»¥ä¸‹{len(questions)}ä¸ªé—®é¢˜å’Œå›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ï¼ˆ1-5åˆ†ï¼‰ï¼š

"""

        for i, q in enumerate(questions, 1):
            prompt += f"""
é—®é¢˜ {i}: {q['question']}
å›ç­”: {q['answer']}
"""

        prompt += f"""

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "segment_number": {segment_num},
  "questions_count": {len(questions)},
  "scores": {{
    "openness_to_experience": 1-5,
    "conscientiousness": 1-5,
    "extraversion": 1-5,
    "agreeableness": 1-5,
    "neuroticism": 1-5
  }},
  "evidence": {{
    "openness_to_experience": ["å…·ä½“è¯æ®"],
    "conscientiousness": ["å…·ä½“è¯æ®"],
    "extraversion": ["å…·ä½“è¯æ®"],
    "agreeableness": ["å…·ä½“è¯æ®"],
    "neuroticism": ["å…·ä½“è¯æ®"]
  }}
}}

ç¡®ä¿æ¯ä¸ªè¯„åˆ†éƒ½æ˜¯1-5çš„æ•´æ•°ï¼Œå¹¶æä¾›å…·ä½“çš„åˆ†æè¯æ®ã€‚"""

        return prompt

    def _analyze_segment(self, questions: List[Dict], segment_num: int, model: str) -> Dict:
        """åˆ†æå•ä¸ªåˆ†æ®µ"""
        self.stats['segments_processed'] += 1

        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._create_cache_key(questions, model)
        cached_result = self._load_cache(cache_key)
        if cached_result:
            print(f"  ğŸ“¦ ç¼“å­˜å‘½ä¸­: {model} æ®µ{segment_num}")
            sys.stdout.flush()
            return cached_result

        # æ‰§è¡ŒAPIè°ƒç”¨
        self.stats['api_calls'] += 1
        print(f"  ğŸ” åˆ†ææ®µ{segment_num}: {model} ({len(questions)}é¢˜)")
        sys.stdout.flush()

        try:
            from enhanced_cloud_analyzer import EnhancedCloudAnalyzer
            analyzer = EnhancedCloudAnalyzer(model=model)

            # å‡†å¤‡segmentæ•°æ®æ ¼å¼
            segment_data = []
            for q in questions:
                segment_data.append({
                    'question': q['question'],
                    'answer': q['answer']
                })

            # ä½¿ç”¨analyze_segmentæ–¹æ³•
            segment_result = analyzer.analyze_segment(segment_data, segment_num)

            # è½¬æ¢ç»“æœæ ¼å¼
            if segment_result.get('success', False):
                result = {
                    'success': True,
                    'segment_number': segment_num,
                    'model': model,
                    'questions_count': len(questions),
                    'scores': segment_result.get('scores', {}),
                    'evidence': segment_result.get('evidence', {})
                }
            else:
                result = {
                    'success': False,
                    'segment_number': segment_num,
                    'model': model,
                    'questions_count': len(questions),
                    'error': segment_result.get('error', 'åˆ†æå¤±è´¥')
                }

            # ä¿å­˜ç¼“å­˜
            if result.get('success', False):
                self._save_cache(cache_key, result)

            return result

        except Exception as e:
            error_result = {
                'success': False,
                'segment_number': segment_num,
                'model': model,
                'error': str(e),
                'questions_count': len(questions)
            }
            print(f"  âŒ åˆ†æå¤±è´¥: {model} æ®µ{segment_num} - {e}")
            sys.stdout.flush()
            return error_result

    def _parse_response(self, response: str, segment_num: int, questions_count: int, model: str) -> Dict:
        """è§£æå“åº”"""
        try:
            import re

            # æå–JSON
            json_start = response.find('{')
            json_end = response.rfind('}') + 1

            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]

                # æ¸…ç†JSONå­—ç¬¦ä¸²
                json_str = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', json_str)
                json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)

                result = json.loads(json_str)

                if 'scores' in result:
                    result['segment_number'] = segment_num
                    result['questions_count'] = questions_count
                    result['model'] = model
                    result['success'] = True

                    # ç¡®ä¿è¯„åˆ†åœ¨1-5èŒƒå›´å†…
                    for trait in result['scores']:
                        score = result['scores'][trait]
                        result['scores'][trait] = max(1, min(5, int(score)))

                    return result

        except Exception as e:
            print(f"  âš ï¸ å“åº”è§£æå¤±è´¥: {e}")
            sys.stdout.flush()

        return {
            'success': False,
            'segment_number': segment_num,
            'model': model,
            'questions_count': questions_count,
            'error': 'å“åº”è§£æå¤±è´¥'
        }

    def analyze_file(self, file_path: Path, output_dir: Path) -> Dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        start_time = time.time()
        print(f"\nğŸš€ å¼€å§‹åˆ†æ: {file_path.name}")
        print(f"ğŸ“Š é…ç½®: {self.segment_size}é¢˜/æ®µ, {len(self.models)}ä¸ªæ¨¡å‹")
        sys.stdout.flush()

        try:
            # è¯»å–æ–‡ä»¶ï¼Œå°è¯•å¤šç§ç¼–ç 
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except UnicodeDecodeError:
                try:
                    with open(file_path, 'r', encoding='gbk') as f:
                        data = json.load(f)
                except:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        data = json.load(f)

            # æå–é—®é¢˜ - é€‚åº”æ–°çš„JSONç»“æ„
            questions = []

            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„è¯„ä¼°ç»“æœæ ¼å¼
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for item in data['assessment_results']:
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']

                        if isinstance(question_data, dict):
                            # ä»question_dataä¸­æå–é—®é¢˜
                            question_text = question_data.get('prompt_for_agent', question_data.get('mapped_ipip_concept', ''))

                            # ä»extracted_responseæˆ–conversation_logä¸­æå–å›ç­”
                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']
                            elif 'conversation_log' in item and isinstance(item['conversation_log'], list):
                                # ä»å¯¹è¯æ—¥å¿—ä¸­æå–agentçš„å›ç­”
                                for msg in item['conversation_log']:
                                    if isinstance(msg, dict) and msg.get('role') == 'assistant':
                                        answer_text = msg.get('content', '')
                                        break

                            if question_text and answer_text:
                                questions.append({
                                    'question': question_text,
                                    'answer': answer_text
                                })
            else:
                # åŸæœ‰çš„ç›´æ¥æ ¼å¼
                for item in data:
                    if 'question' in item and 'answer' in item:
                        questions.append({
                            'question': item['question'],
                            'answer': item['answer']
                        })

            if not questions:
                return {
                    'success': False,
                    'file': str(file_path),
                    'error': 'æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆé—®é¢˜'
                }

            # åˆ›å»ºåˆ†æ®µ
            segments = []
            for i in range(0, len(questions), self.segment_size):
                segment_questions = questions[i:i + self.segment_size]
                segments.append((i // self.segment_size + 1, segment_questions))

            print(f"ğŸ“‹ {len(questions)}é¢˜åˆ†ä¸º{len(segments)}æ®µ")
            sys.stdout.flush()

            # åˆ†ææ‰€æœ‰æ¨¡å‹
            all_results = {}

            for model in self.models:
                print(f"\nğŸ¤– å¤„ç†æ¨¡å‹: {model}")
                sys.stdout.flush()

                model_results = []
                for segment_num, segment_questions in segments:
                    result = self._analyze_segment(segment_questions, segment_num, model)
                    model_results.append(result)

                    # æ®µé—´å»¶è¿Ÿ
                    if segment_num < len(segments):
                        time.sleep(self.delay_between_segments)

                all_results[model] = model_results

                # æ˜¾ç¤ºè¿›åº¦
                success_count = len([r for r in model_results if r.get('success', False)])
                print(f"  âœ… {model}: {success_count}/{len(segments)}æ®µæˆåŠŸ")
                sys.stdout.flush()

            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            final_scores = self._calculate_final_scores(all_results)
            mbti_type = self._calculate_mbti(final_scores)

            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            self.stats['total_processing_time'] += processing_time
            self.stats['files_processed'] += 1

            result = {
                'success': True,
                'file': str(file_path),
                'processing_time': processing_time,
                'optimization_info': {
                    'segment_size': self.segment_size,
                    'segments_count': len(segments),
                    'models_count': len(self.models),
                    'original_segments': len(questions) // 2,
                    'optimization_ratio': (len(questions) // 2) / len(segments)
                },
                'final_scores': final_scores,
                'mbti_type': mbti_type,
                'model_results': all_results,
                'performance_stats': {
                    'cache_hit_rate': self.stats['cache_hits'] / max(1, self.stats['segments_processed']) * 100,
                    'avg_time_per_segment': processing_time / (len(segments) * len(self.models)),
                    'files_per_hour': 3600 / processing_time if processing_time > 0 else 0
                }
            }

            # ä¿å­˜ç»“æœ
            output_file = output_dir / f"{file_path.stem}_unbuffered_optimized.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"\nâœ… æ–‡ä»¶åˆ†æå®Œæˆ: {output_file}")
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")
            print(f"ğŸ¯ æœ€ç»ˆè¯„åˆ†: {final_scores}")
            print(f"ğŸ§  MBTI: {mbti_type}")
            print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {result['performance_stats']['files_per_hour']:.1f}æ–‡ä»¶/å°æ—¶")
            sys.stdout.flush()

            return result

        except Exception as e:
            error_result = {
                'success': False,
                'file': str(file_path),
                'error': str(e),
                'processing_time': time.time() - start_time
            }
            print(f"âŒ æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
            sys.stdout.flush()
            return error_result

    def _calculate_final_scores(self, all_results: Dict) -> Dict:
        """è®¡ç®—æœ€ç»ˆè¯„åˆ†"""
        trait_scores = {}

        for model, results in all_results.items():
            model_trait_scores = {}

            for result in results:
                if result.get('success', False) and 'scores' in result:
                    for trait, score in result['scores'].items():
                        if trait not in model_trait_scores:
                            model_trait_scores[trait] = []
                        model_trait_scores[trait].append(score)

            # è®¡ç®—æ¨¡å‹å¹³å‡åˆ†
            for trait, scores in model_trait_scores.items():
                if scores:
                    avg_score = sum(scores) / len(scores)
                    if trait not in trait_scores:
                        trait_scores[trait] = []
                    trait_scores[trait].append(avg_score)

        # è®¡ç®—è·¨æ¨¡å‹å¹³å‡åˆ†
        final_scores = {}
        for trait, scores in trait_scores.items():
            if scores:
                final_scores[trait] = round(sum(scores) / len(scores))
            else:
                final_scores[trait] = 3

        return final_scores

    def _calculate_mbti(self, scores: Dict) -> str:
        """è®¡ç®—MBTIç±»å‹"""
        try:
            e_i = "E" if scores.get('extraversion', 3) > 3 else "I"
            s_n = "S" if scores.get('openness_to_experience', 3) < 4 else "N"
            t_f = "T" if scores.get('agreeableness', 3) < 3 else "F"
            j_p = "J" if scores.get('conscientiousness', 3) > 3 else "P"

            return f"{e_i}{s_n}{t_f}{j_p}"
        except:
            return "UNKNOWN"

    def run_batch_analysis(self, input_dir: Path, output_dir: Path):
        """è¿è¡Œæ‰¹é‡åˆ†æ"""
        print(f"\nğŸ¯ å¼€å§‹æ— ç¼“å†²æ‰¹é‡åˆ†æ")
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        sys.stdout.flush()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir.mkdir(exist_ok=True)

        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        json_files = list(input_dir.glob("*.json"))
        if not json_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
            return

        print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ä»¶")
        sys.stdout.flush()

        # åŠ è½½è¿›åº¦
        if self.load_progress():
            print(f"ğŸ“‚ å‘ç°è¿›åº¦ä¿¡æ¯:")
            print(f"   å·²å®Œæˆ: {len(self.completed_files)} ä¸ªæ–‡ä»¶")
            print(f"   å¤±è´¥: {len(self.failed_files)} ä¸ªæ–‡ä»¶")
            sys.stdout.flush()

        # è¿‡æ»¤å¾…å¤„ç†æ–‡ä»¶
        remaining_files = [f for f in json_files if str(f) not in self.completed_files and str(f) not in self.failed_files]

        print(f"ğŸ“Š å‰©ä½™å¾…å¤„ç†: {len(remaining_files)} ä¸ªæ–‡ä»¶")
        sys.stdout.flush()

        if not remaining_files:
            print("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ")
            return

        # å¤„ç†æ–‡ä»¶
        total_files = len(remaining_files)
        successful = 0

        for i, file_path in enumerate(remaining_files, 1):
            print(f"\nğŸ“ˆ è¿›åº¦: {i}/{total_files} ({i/total_files*100:.1f}%)")
            sys.stdout.flush()

            result = self.analyze_file(file_path, output_dir)

            if result['success']:
                self.completed_files.add(str(file_path))
                successful += 1
                print(f"âœ… æˆåŠŸå¤„ç†: {file_path.name}")
            else:
                self.failed_files.add(str(file_path))
                print(f"âŒ å¤„ç†å¤±è´¥: {file_path.name} - {result.get('error', 'Unknown error')}")

            sys.stdout.flush()

            # ä¿å­˜è¿›åº¦
            self.save_progress()

            # æ–‡ä»¶é—´å»¶è¿Ÿ
            if i < total_files:
                print(f"â³ ç­‰å¾… {self.delay_between_files}s åå¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶...")
                time.sleep(self.delay_between_files)

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        total_time = time.time() - self.stats['start_time']
        print(f"\nğŸ“Š æ‰¹é‡åˆ†æå®Œæˆ!")
        print(f"âœ… æˆåŠŸ: {successful}/{total_files}")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time/3600:.1f} å°æ—¶")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {successful/(total_time/3600):.1f} æ–‡ä»¶/å°æ—¶")
        print(f"ğŸ“¦ ç¼“å­˜å‘½ä¸­ç‡: {self.stats['cache_hits']/max(1, self.stats['segments_processed'])*100:.1f}%")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        sys.stdout.flush()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æ— ç¼“å†²æ‰¹é‡ä¼˜åŒ–å™¨")
    sys.stdout.flush()

    try:
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = UnbufferedBatchOptimizer()

        # è®¾ç½®ç›®å½•
        input_dir = Path("results/results")
        output_dir = Path("unbuffered_optimized_results")

        # è¿è¡Œåˆ†æ
        optimizer.run_batch_analysis(input_dir, output_dir)

    except Exception as e:
        print(f"ğŸ’¥ ç¨‹åºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.stdout.flush()

if __name__ == "__main__":
    main()