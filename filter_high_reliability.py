#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç­›é€‰é«˜å¯ä¿¡åº¦è¯„ä¼°ç»“æœ
ä»æ‰€æœ‰å·²å®Œæˆçš„è¯„ä¼°ç»“æœä¸­ç­›é€‰å‡ºä¸€è‡´æ€§é«˜ä¸”ä¿¡åº¦é€šè¿‡éªŒè¯çš„æŠ¥å‘Š
"""
import sys
import os
import json
import glob
from pathlib import Path
from datetime import datetime


def filter_high_reliability_results(input_dir="segmented_scoring_results", output_dir="high_reliability_results", 
                                 min_consistency=80.0, min_reliability=0.8):
    """
    ç­›é€‰é«˜å¯ä¿¡åº¦è¯„ä¼°ç»“æœ
    """
    print("="*60)
    print("ğŸ” ç­›é€‰é«˜å¯ä¿¡åº¦è¯„ä¼°ç»“æœ")
    print("="*60)
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰è¯„ä¼°ç»“æœæ–‡ä»¶
    json_pattern = os.path.join(input_dir, "*_segmented_scoring_evaluation.json")
    all_files = glob.glob(json_pattern)
    
    if not all_files:
        print(f"âŒ åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°è¯„ä¼°ç»“æœæ–‡ä»¶")
        return
    
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š æ‰¾åˆ° {len(all_files)} ä¸ªè¯„ä¼°ç»“æœæ–‡ä»¶")
    print(f"ğŸ¯ ç­›é€‰æ ‡å‡†:")
    print(f"   æœ€å°ä¸€è‡´æ€§: {min_consistency}%")
    print(f"   æœ€å°ä¿¡åº¦: {min_reliability}")
    print()
    
    # ç­›é€‰ç»“æœç»Ÿè®¡
    high_reliability_count = 0
    total_consistency = 0
    total_reliability = 0
    filtered_files = []
    
    # ç­›é€‰æ¯ä¸ªæ–‡ä»¶
    for i, file_path in enumerate(all_files, 1):
        try:
            # è¯»å–è¯„ä¼°ç»“æœ
            with open(file_path, 'r', encoding='utf-8') as f:
                result_data = json.load(f)
            
            # æå–ä¸€è‡´æ€§åˆ†æ•°å’Œä¿¡åº¦åˆ†æ•°
            consistency_score = result_data.get('consistency_analysis', {}).get('overall_consistency', 0)
            reliability_score = result_data.get('reliability_analysis', {}).get('metrics', {}).get('overall_reliability', 0)
            reliability_passed = result_data.get('reliability_analysis', {}).get('report', {}).get('validation_passed', False)
            
            # ç´¯ç§¯ç»Ÿè®¡
            total_consistency += consistency_score
            total_reliability += reliability_score
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç­›é€‰æ ‡å‡†
            if consistency_score >= min_consistency and reliability_score >= min_reliability and reliability_passed:
                high_reliability_count += 1
                filtered_files.append({
                    'file_path': file_path,
                    'consistency': consistency_score,
                    'reliability': reliability_score,
                    'filename': os.path.basename(file_path)
                })
                
                # å¤åˆ¶æ–‡ä»¶åˆ°é«˜å¯ä¿¡åº¦ç›®å½•
                output_file = os.path.join(output_dir, os.path.basename(file_path))
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(result_data, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… [{i}/{len(all_files)}] é«˜å¯ä¿¡åº¦: {os.path.basename(file_path)}")
                print(f"   ä¸€è‡´æ€§: {consistency_score:.2f}%, ä¿¡åº¦: {reliability_score:.2f}")
            else:
                print(f"âŒ [{i}/{len(all_files)}] æœªè¾¾æ ‡: {os.path.basename(file_path)}")
                print(f"   ä¸€è‡´æ€§: {consistency_score:.2f}%, ä¿¡åº¦: {reliability_score:.2f}")
                
        except Exception as e:
            print(f"âŒ [{i}/{len(all_files)}] å¤„ç†å¤±è´¥: {os.path.basename(file_path)} - {str(e)}")
            continue
    
    # è®¡ç®—å¹³å‡å€¼
    avg_consistency = total_consistency / len(all_files) if all_files else 0
    avg_reliability = total_reliability / len(all_files) if all_files else 0
    
    # ç”Ÿæˆç­›é€‰æŠ¥å‘Š
    filter_report = {
        "filter_date": datetime.now().isoformat(),
        "input_directory": input_dir,
        "output_directory": output_dir,
        "criteria": {
            "min_consistency": min_consistency,
            "min_reliability": min_reliability
        },
        "statistics": {
            "total_files": len(all_files),
            "high_reliability_files": high_reliability_count,
            "filter_rate": high_reliability_count / len(all_files) if all_files else 0,
            "average_consistency": avg_consistency,
            "average_reliability": avg_reliability
        },
        "high_reliability_files": filtered_files
    }
    
    # ä¿å­˜ç­›é€‰æŠ¥å‘Š
    report_file = os.path.join(output_dir, "high_reliability_filter_report.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(filter_report, f, ensure_ascii=False, indent=2)
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print(f"\n" + "="*60)
    print(f"ğŸ“Š é«˜å¯ä¿¡åº¦ç­›é€‰å®ŒæˆæŠ¥å‘Š")
    print(f"ğŸ“ˆ æ€»æ–‡ä»¶æ•°: {len(all_files)}")
    print(f"âœ… é«˜å¯ä¿¡åº¦æ–‡ä»¶: {high_reliability_count}")
    print(f"ğŸ¯ ç­›é€‰é€šè¿‡ç‡: {(high_reliability_count/len(all_files))*100:.1f}%" if all_files else "N/A")
    print(f"ğŸ“ˆ å¹³å‡ä¸€è‡´æ€§: {avg_consistency:.2f}%")
    print(f"ğŸ“ˆ å¹³å‡ä¿¡åº¦: {avg_reliability:.2f}")
    print(f"ğŸ’¾ é«˜å¯ä¿¡åº¦æ–‡ä»¶ä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“„ ç­›é€‰æŠ¥å‘Šä¿å­˜åœ¨: {report_file}")
    print("="*60)
    
    return filter_report


def main():
    """
    ä¸»å‡½æ•°
    """
    print("é«˜å¯ä¿¡åº¦è¯„ä¼°ç»“æœç­›é€‰å™¨")
    print()
    
    # è¿è¡Œç­›é€‰
    filter_high_reliability_results()


if __name__ == "__main__":
    main()