#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¹¶å‘ä¼˜åŒ–å™¨ - 4é¢˜åˆ†æ®µ + å¹¶å‘å¤„ç†
"""

import sys
import os
import json
import asyncio
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import time

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DASHSCOPE_API_KEY'] = 'sk-3f16ac9d87e34ca88bf3925c3651624f'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class BatchConcurrentOptimizer:
    def __init__(self, models: List[str] = None, cache_dir: str = "batch_cache",
                 segment_size: int = 4, max_concurrent: int = 3):
        self.models = models or ["qwen-max", "deepseek-v3.2-exp", "Moonshot-Kimi-K2-Instruct"]
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        # ä¼˜åŒ–å‚æ•°
        self.segment_size = segment_size  # æ¯æ®µé—®é¢˜æ•°é‡ï¼ˆä»2å¢åŠ åˆ°4ï¼‰
        self.max_concurrent = max_concurrent  # æœ€å¤§å¹¶å‘æ•°

        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'cache_hits': 0,
            'api_calls': 0,
            'total_segments': 0,
            'segments_per_file': 0,
            'processing_time': 0
        }

    def _create_segment_cache_key(self, questions: List[Dict], model: str) -> str:
        """åˆ›å»ºåˆ†æ®µç¼“å­˜é”®"""
        # åŸºäºé—®é¢˜å†…å®¹çš„å“ˆå¸Œ
        content = f"{model}_" + "_".join([q['question'] + q['answer'] for q in questions])
        return hashlib.sha256(content.encode()).hexdigest()

    def _load_segment_cache(self, cache_key: str) -> Optional[Dict]:
        """åŠ è½½åˆ†æ®µç¼“å­˜"""
        cache_file = self.cache_dir / f"segment_{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return None

    def _save_segment_cache(self, cache_key: str, result: Dict):
        """ä¿å­˜åˆ†æ®µç¼“å­˜"""
        cache_file = self.cache_dir / f"segment_{cache_key}.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

    def _create_batch_prompt(self, questions: List[Dict], segment_num: int) -> str:
        """åˆ›å»ºæ‰¹é‡åˆ†ææç¤º - 4é¢˜ä¸€æ®µ"""
        prompt = f"""è¯·åŒæ—¶åˆ†æä»¥ä¸‹{len(questions)}ä¸ªé—®é¢˜å’Œå›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ã€‚è¯·ä¸ºæ¯ä¸ªé—®é¢˜æä¾›ç‹¬ç«‹çš„è¯„åˆ†ï¼Œç„¶åè®¡ç®—è¯¥åˆ†æ®µçš„æ•´ä½“å¹³å‡åˆ†ã€‚

"""

        for i, q in enumerate(questions, 1):
            prompt += f"\n=== é—®é¢˜ {i} ===\n"
            prompt += f"é—®é¢˜: {q['question']}\n"
            prompt += f"å›ç­”: {q['answer']}\n"

        prompt += f"""

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "segment_number": {segment_num},
  "questions_count": {len(questions)},
  "individual_scores": [
    {{
      "question_index": 1,
      "scores": {{
        "openness_to_experience": 1-5,
        "conscientiousness": 1-5,
        "extraversion": 1-5,
        "agreeableness": 1-5,
        "neuroticism": 1-5
      }},
      "evidence": {{
        "openness_to_experience": ["å…·ä½“è¯æ®"],
        ...
      }}
    }},
    ...
  ],
  "segment_average_scores": {{
    "openness_to_experience": 1-5,
    "conscientiousness": 1-5,
    "extraversion": 1-5,
    "agreeableness": 1-5,
    "neuroticism": 1-5
  }},
  "segment_evidence": {{
    "openness_to_experience": ["ç»¼åˆè¯æ®"],
    ...
  }}
}}

è¯·ç¡®ä¿ï¼š
1. æ¯ä¸ªè¯„åˆ†éƒ½åœ¨1-5ä¹‹é—´
2. æä¾›å…·ä½“çš„åˆ†æè¯æ®
3. segment_average_scoresæ˜¯individual_scoresçš„å¹³å‡å€¼
"""

        return prompt

    async def _analyze_segment_async(self, questions: List[Dict], segment_num: int, model: str) -> Dict:
        """å¼‚æ­¥åˆ†æåˆ†æ®µ"""
        self.stats['total_segments'] += 1

        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._create_segment_cache_key(questions, model)
        cached_result = self._load_segment_cache(cache_key)

        if cached_result:
            self.stats['cache_hits'] += 1
            print(f"  ğŸ“¦ ç¼“å­˜å‘½ä¸­: æ¨¡å‹ {model} æ®µ {segment_num}")
            return cached_result

        # æ‰§è¡ŒAPIè°ƒç”¨
        self.stats['api_calls'] += 1
        print(f"  ğŸ” åˆ†ææ®µ {segment_num}: æ¨¡å‹ {model} ({len(questions)} é¢˜)")

        try:
            # åˆ›å»ºæ‰¹é‡æç¤º
            prompt = self._create_batch_prompt(questions, segment_num)

            # è°ƒç”¨APIï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒåŒæ­¥è°ƒç”¨ï¼‰
            from enhanced_cloud_analyzer import EnhancedCloudAnalyzer
            analyzer = EnhancedCloudAnalyzer(model=model)

            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor(max_workers=1) as executor:
                response = await loop.run_in_executor(
                    executor,
                    analyzer._call_api,
                    prompt
                )

            # è§£æå“åº”
            result = self._parse_batch_response(response, segment_num, len(questions))

            # ä¿å­˜ç¼“å­˜
            if result.get('success', False):
                self._save_segment_cache(cache_key, result)

            return result

        except Exception as e:
            return {
                'success': False,
                'segment_number': segment_num,
                'model': model,
                'error': str(e),
                'questions_count': len(questions)
            }

    def _parse_batch_response(self, response: str, segment_num: int, questions_count: int) -> Dict:
        """è§£ææ‰¹é‡å“åº”"""
        try:
            import re

            # å°è¯•æå–JSON
            json_start = response.find('{')
            json_end = response.rfind('}') + 1

            if json_start != -1 and json_end > json_start:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)

                # éªŒè¯ç»“æœæ ¼å¼
                if 'segment_average_scores' in result:
                    result['segment_number'] = segment_num
                    result['questions_count'] = questions_count
                    result['success'] = True

                    # ç¡®ä¿æ‰€æœ‰è¯„åˆ†éƒ½åœ¨1-5èŒƒå›´å†…
                    for trait in result['segment_average_scores']:
                        score = result['segment_average_scores'][trait]
                        result['segment_average_scores'][trait] = max(1, min(5, int(score)))

                    return result

        except Exception as e:
            print(f"å“åº”è§£æå¤±è´¥: {e}")

        return {
            'success': False,
            'segment_number': segment_num,
            'questions_count': questions_count,
            'error': 'å“åº”è§£æå¤±è´¥'
        }

    def _create_segments(self, questions: List[Dict]) -> List[List[Dict]]:
        """åˆ›å»ºåˆ†æ®µ - æ¯æ®µ4é¢˜"""
        segments = []

        for i in range(0, len(questions), self.segment_size):
            segment_questions = questions[i:i + self.segment_size]
            segments.append(segment_questions)

        self.stats['segments_per_file'] = len(segments)
        return segments

    async def analyze_file_optimized(self, file_path: Path, output_dir: Path) -> Dict:
        """ä¼˜åŒ–æ–‡ä»¶åˆ†æ"""
        start_time = time.time()
        print(f"ğŸš€ æ‰¹é‡å¹¶å‘ä¼˜åŒ–åˆ†æ: {file_path.name}")
        print(f"ğŸ“Š åˆ†æ®µå¤§å°: {self.segment_size}é¢˜/æ®µ, å¹¶å‘æ•°: {self.max_concurrent}")

        try:
            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–é—®é¢˜
            questions = []
            for item in data:
                if 'question' in item and 'answer' in item:
                    questions.append({
                        'question': item['question'],
                        'answer': item['answer']
                    })

            if not questions:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆé—®é¢˜',
                    'file': str(file_path)
                }

            # åˆ›å»ºåˆ†æ®µï¼ˆ4é¢˜ä¸€æ®µï¼‰
            segments = self._create_segments(questions)
            print(f"ğŸ“Š {len(questions)} ä¸ªé—®é¢˜åˆ†ä¸º {len(segments)} ä¸ªåˆ†æ®µ")

            # å¹¶å‘åˆ†ææ‰€æœ‰æ¨¡å‹
            all_model_results = {}

            for model in self.models:
                print(f"\nğŸ¤– å¤„ç†æ¨¡å‹: {model}")

                # åˆ›å»ºå¹¶å‘ä»»åŠ¡
                tasks = []
                for i, segment_questions in enumerate(segments, 1):
                    task = self._analyze_segment_async(segment_questions, i, model)
                    tasks.append(task)

                # æ§åˆ¶å¹¶å‘æ•°é‡
                model_results = []
                for i in range(0, len(tasks), self.max_concurrent):
                    batch_tasks = tasks[i:i + self.max_concurrent]
                    batch_results = await asyncio.gather(*batch_tasks)
                    model_results.extend(batch_results)

                all_model_results[model] = model_results

                # æ˜¾ç¤ºè¿›åº¦
                success_count = len([r for r in model_results if r.get('success', False)])
                print(f"  âœ… {model}: {success_count}/{len(segments)} åˆ†æ®µæˆåŠŸ")

            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            final_scores = self._calculate_final_scores(all_model_results)
            mbti_type = self._calculate_mbti(final_scores)

            # è®¡ç®—æ¨¡å‹ä¸€è‡´æ€§
            model_consistency = self._calculate_model_consistency(all_model_results)

            # ç”Ÿæˆç»“æœ
            processing_time = time.time() - start_time
            self.stats['processing_time'] += processing_time

            result = {
                'success': True,
                'file': str(file_path),
                'processing_time': processing_time,
                'optimization_info': {
                    'segment_size': self.segment_size,
                    'segments_count': len(segments),
                    'models_count': len(self.models),
                    'max_concurrent': self.max_concurrent,
                    'original_segments': len(questions) // 2,  # åŸæ¥2é¢˜ä¸€æ®µçš„æ•°é‡
                    'optimization_ratio': (len(questions) // 2) / len(segments)
                },
                'file_info': {
                    'filename': file_path.name,
                    'total_questions': len(questions),
                    'segments': len(segments),
                    'models': self.models
                },
                'final_scores': final_scores,
                'mbti_type': mbti_type,
                'model_consistency': model_consistency,
                'model_results': all_model_results,
                'performance_stats': {
                    'cache_hit_rate': self.stats['cache_hits'] / max(1, self.stats['total_segments']) * 100,
                    'api_calls_saved': self.stats['cache_hits'],
                    'total_api_calls': self.stats['api_calls'],
                    'avg_time_per_segment': processing_time / (len(segments) * len(self.models)),
                    'segments_per_second': (len(segments) * len(self.models)) / processing_time
                }
            }

            # ä¿å­˜ç»“æœ
            output_file = output_dir / f"{file_path.stem}_batch_optimized.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"\nâœ… æ‰¹é‡å¹¶å‘ä¼˜åŒ–å®Œæˆ: {output_file}")
            print(f"â±ï¸  å¤„ç†æ—¶é—´: {processing_time:.1f} ç§’")
            print(f"ğŸ“Š ä¼˜åŒ–æ•ˆæœ: APIè°ƒç”¨å‡å°‘ {result['optimization_info']['optimization_ratio']:.1f}å€")
            print(f"ğŸ¯ æœ€ç»ˆè¯„åˆ†: {final_scores}")
            print(f"ğŸ§  MBTI: {mbti_type}")
            print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {result['performance_stats']['segments_per_second']:.2f} æ®µ/ç§’")

            return result

        except Exception as e:
            return {
                'success': False,
                'file': str(file_path),
                'error': str(e),
                'processing_time': time.time() - start_time
            }

    def _calculate_final_scores(self, all_model_results: Dict) -> Dict:
        """è®¡ç®—æœ€ç»ˆè¯„åˆ†"""
        trait_scores = {}

        for model, results in all_model_results.items():
            model_trait_scores = {}

            for result in results:
                if result.get('success', False) and 'segment_average_scores' in result:
                    for trait, score in result['segment_average_scores'].items():
                        if trait not in model_trait_scores:
                            model_trait_scores[trait] = []
                        model_trait_scores[trait].append(score)

            # è®¡ç®—æ¨¡å‹å¹³å‡åˆ†
            for trait, scores in model_trait_scores.items():
                if scores:
                    avg_score = sum(scores) / len(scores)
                    if trait not in trait_scores:
                        trait_scores[trait] = []
                    trait_scores[trait].append(avg_score)

        # è®¡ç®—è·¨æ¨¡å‹å¹³å‡åˆ†
        final_scores = {}
        for trait, scores in trait_scores.items():
            if scores:
                final_scores[trait] = round(sum(scores) / len(scores))
            else:
                final_scores[trait] = 3

        return final_scores

    def _calculate_mbti(self, scores: Dict) -> str:
        """è®¡ç®—MBTIç±»å‹"""
        try:
            e_i = "E" if scores.get('extraversion', 3) > 3 else "I"
            s_n = "S" if scores.get('openness_to_experience', 3) < 4 else "N"
            t_f = "T" if scores.get('agreeableness', 3) < 3 else "F"
            j_p = "J" if scores.get('conscientiousness', 3) > 3 else "P"

            return f"{e_i}{s_n}{t_f}{j_p}"
        except:
            return "UNKNOWN"

    def _calculate_model_consistency(self, all_model_results: Dict) -> Dict:
        """è®¡ç®—æ¨¡å‹ä¸€è‡´æ€§"""
        consistency = {}

        for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
            model_scores = []

            for model, results in all_model_results.items():
                trait_scores = []
                for result in results:
                    if result.get('success', False) and 'segment_average_scores' in result:
                        score = result['segment_average_scores'].get(trait, 3)
                        trait_scores.append(score)

                if trait_scores:
                    model_avg = sum(trait_scores) / len(trait_scores)
                    model_scores.append(model_avg)

            if len(model_scores) >= 2:
                # è®¡ç®—ä¸€è‡´æ€§ï¼ˆ100%è¡¨ç¤ºå®Œå…¨ä¸€è‡´ï¼‰
                max_score = max(model_scores)
                min_score = min(model_scores)
                consistency[trait] = max(0, 100 - (max_score - min_score) * 20)
            else:
                consistency[trait] = 100

        return consistency

async def test_batch_concurrent_optimizer():
    print("ğŸš€ æµ‹è¯•æ‰¹é‡å¹¶å‘ä¼˜åŒ–å™¨...")

    try:
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = BatchConcurrentOptimizer(
            segment_size=4,  # 4é¢˜ä¸€æ®µ
            max_concurrent=3  # æœ€å¤§3ä¸ªå¹¶å‘
        )

        # æŸ¥æ‰¾æµ‹è¯•æ–‡ä»¶
        results_dir = Path("results/results")
        json_files = list(results_dir.glob("*.json"))

        if not json_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶")
            return

        # é€‰æ‹©æµ‹è¯•æ–‡ä»¶
        test_file = json_files[20] if len(json_files) > 20 else json_files[0]
        print(f"ğŸ“ æµ‹è¯•æ–‡ä»¶: {test_file.name}")

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("batch_optimized_results")
        output_dir.mkdir(exist_ok=True)

        # æ‰§è¡Œä¼˜åŒ–åˆ†æ
        result = await optimizer.analyze_file_optimized(test_file, output_dir)

        if result['success']:
            print(f"\nâœ… æ‰¹é‡å¹¶å‘ä¼˜åŒ–æµ‹è¯•æˆåŠŸ!")

            opt_info = result['optimization_info']
            perf = result['performance_stats']

            print(f"ğŸ“Š ä¼˜åŒ–æ•ˆæœ:")
            print(f"   åˆ†æ®µæ•°é‡: {opt_info['original_segments']} â†’ {opt_info['segments_count']} (å‡å°‘ {opt_info['optimization_ratio']:.1f}å€)")
            print(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.1f} ç§’")
            print(f"   ç¼“å­˜å‘½ä¸­ç‡: {perf['cache_hit_rate']:.1f}%")
            print(f"   å¤„ç†é€Ÿåº¦: {perf['segments_per_second']:.2f} æ®µ/ç§’")
            print(f"   APIè°ƒç”¨èŠ‚çœ: {perf['api_calls_saved']} æ¬¡")

            # ä¼°ç®—æ€»ä½“æå‡
            original_time = opt_info['original_segments'] * len(optimizer.models) * 30  # å‡è®¾åŸæ¥æ¯æ®µ30ç§’
            speedup = original_time / result['processing_time']
            print(f"   ğŸš€ é¢„ä¼°æ•´ä½“åŠ é€Ÿ: {speedup:.1f}å€")

        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {result['error']}")

    except Exception as e:
        print(f"ğŸ’¥ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_batch_concurrent_optimizer())