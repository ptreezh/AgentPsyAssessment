#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¼ºå¥çš„ç»Ÿä¸€è¯„ä¼°ç³»ç»Ÿ - å…·å¤‡æ™ºèƒ½å®¹é”™èƒ½åŠ›
æ”¯æŒå¤šç§æµ‹è¯•æ–‡ä»¶æ ¼å¼ï¼Œæä¾›ä¼˜é›…çš„é”™è¯¯å¤„ç†å’Œå‘åå…¼å®¹
"""

import json
import sys
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class RobustAssessmentSystem:
    """å¼ºå¥çš„ç»Ÿä¸€è¯„ä¼°ç³»ç»Ÿ"""

    def __init__(self):
        self.supported_formats = [
            "traditional_test_bank",    # ä¼ ç»Ÿ test_bank æ ¼å¼
            "unified_questions",       # ç»Ÿä¸€ assessment_questions æ ¼å¼
            "simplified",              # ç®€åŒ–æ ¼å¼
            "custom"                  # è‡ªå®šä¹‰æ ¼å¼
        ]

        self.format_detectors = {
            "traditional_test_bank": self._detect_traditional_format,
            "unified_questions": self._detect_unified_format,
            "simplified": self._detect_simplified_format,
            "custom": self._detect_custom_format
        }

        self.format_processors = {
            "traditional_test_bank": self._process_traditional_format,
            "unified_questions": self._process_unified_format,
            "simplified": self._process_simplified_format,
            "custom": self._process_custom_format
        }

    def detect_format(self, file_path: Union[str, Path]) -> str:
        """æ™ºèƒ½æ£€æµ‹æ–‡ä»¶æ ¼å¼"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = json.load(f)

            logger.info(f"æ£€æµ‹æ–‡ä»¶æ ¼å¼: {file_path}")

            # æŒ‰ä¼˜å…ˆçº§æ£€æµ‹æ ¼å¼
            for format_name, detector in self.format_detectors.items():
                if detector(content):
                    logger.info(f"âœ… æ£€æµ‹åˆ°æ ¼å¼: {format_name}")
                    return format_name

            logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°å·²çŸ¥æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†å™¨")
            return "simplified"

        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            return "simplified"

    def _detect_traditional_format(self, content: Dict) -> bool:
        """æ£€æµ‹ä¼ ç»Ÿ test_bank æ ¼å¼"""
        return "test_bank" in content and isinstance(content.get("test_bank"), list)

    def _detect_unified_format(self, content: Dict) -> bool:
        """æ£€æµ‹ç»Ÿä¸€ assessment_questions æ ¼å¼"""
        return ("assessment_questions" in content and
                isinstance(content.get("assessment_questions"), list)) or \
               ("assessment_metadata" in content and
                "test_info" in content)

    def _detect_simplified_format(self, content: Dict) -> bool:
        """æ£€æµ‹ç®€åŒ–æ ¼å¼"""
        # ç®€åŒ–æ ¼å¼ï¼šåªæœ‰åŸºæœ¬å…ƒæ•°æ®å’Œé—®é¢˜åˆ—è¡¨
        return ("questions" in content and isinstance(content.get("questions"), list)) or \
               ("items" in content and isinstance(content.get("items"), list))

    def _detect_custom_format(self, content: Dict) -> bool:
        """æ£€æµ‹è‡ªå®šä¹‰æ ¼å¼"""
        # ä»»ä½•åŒ…å«é—®é¢˜å†…å®¹çš„æ ¼å¼éƒ½ç®—è‡ªå®šä¹‰
        keys_to_check = ["questions", "items", "test_items", "problems", "scenarios"]
        return any(key in content for key in keys_to_check)

    def process_file(self, file_path: Union[str, Path],
                       personality_params: Optional[Dict] = None) -> Dict[str, Any]:
        """å¤„ç†è¯„ä¼°æ–‡ä»¶ï¼Œå…·æœ‰å®Œæ•´å®¹é”™èƒ½åŠ›"""
        try:
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

            # æ£€æµ‹æ ¼å¼
            format_type = self.detect_format(file_path)

            # ä½¿ç”¨å¯¹åº”çš„å¤„ç†å™¨
            processor = self.format_processors[format_type]
            processed_data = processor(file_path, personality_params)

            # æ·»åŠ å…ƒæ•°æ®
            processed_data["system_info"] = {
                "format_type": format_type,
                "file_path": str(file_path),
                "processing_time": datetime.now().isoformat(),
                "system_version": "1.0.0",
                "robust_mode": True
            }

            logger.info(f"âœ… æˆåŠŸå¤„ç†æ–‡ä»¶: {file_path} (æ ¼å¼: {format_type})")
            return processed_data

        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯ç»“æœè€Œä¸æ˜¯å´©æºƒ
            return self._create_error_result(file_path, str(e), personality_params)

    def _process_traditional_format(self, file_path: Path,
                                   personality_params: Optional[Dict] = None) -> Dict:
        """å¤„ç†ä¼ ç»Ÿ test_bank æ ¼å¼"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = json.load(f)

        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        unified_questions = []

        for item in content.get("test_bank", []):
            unified_question = {
                "question_id": item.get("question_id", f"Q_{len(unified_questions) + 1}"),
                "question": item.get("prompt_for_agent", ""),
                "scenario": item.get("scenario", ""),
                "dimension": item.get("dimension", "general"),
                "evaluation_rubric": item.get("evaluation_rubric", {}),
                "original_format": "traditional"
            }
            unified_questions.append(unified_question)

        return {
            "assessment_metadata": content.get("test_info", {}),
            "assessment_questions": unified_questions,
            "original_content": content
        }

    def _process_unified_format(self, file_path: Path,
                               personality_params: Optional[Dict] = None) -> Dict:
        """å¤„ç†ç»Ÿä¸€ assessment_questions æ ¼å¼"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = json.load(f)

        # å·²ç»æ˜¯ç»Ÿä¸€æ ¼å¼ï¼Œç›´æ¥è¿”å›
        return content

    def _process_simplified_format(self, file_path: Path,
                                  personality_params: Optional[Dict] = None) -> Dict:
        """å¤„ç†ç®€åŒ–æ ¼å¼"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = json.load(f)

        # å°è¯•æå–é—®é¢˜åˆ—è¡¨
        questions_field = None
        for field in ["questions", "items", "test_items", "problems", "scenarios"]:
            if field in content:
                questions_field = field
                break

        if not questions_field:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°é—®é¢˜å­—æ®µï¼Œåˆ›å»ºé»˜è®¤é—®é¢˜")
            return self._create_default_assessment(file_path, personality_params)

        questions = content[questions_field]
        unified_questions = []

        for i, item in enumerate(questions):
            if isinstance(item, str):
                # ç®€å•å­—ç¬¦ä¸²æ ¼å¼
                unified_question = {
                    "question_id": f"Q_{i+1}",
                    "question": item,
                    "dimension": "general",
                    "evaluation_rubric": {
                        "description": "åŸºäºå›ç­”å†…å®¹è¿›è¡Œç»¼åˆè¯„ä¼°"
                    }
                }
            elif isinstance(item, dict):
                # å¯¹è±¡æ ¼å¼
                unified_question = {
                    "question_id": item.get("id", item.get("question_id", f"Q_{i+1}")),
                    "question": item.get("question", item.get("text", item.get("content", ""))),
                    "dimension": item.get("dimension", "general"),
                    "evaluation_rubric": item.get("rubric", item.get("evaluation", {})),
                    "scenario": item.get("scenario", ""),
                    "original_format": "simplified"
                }
            else:
                continue

            unified_questions.append(unified_question)

        return {
            "assessment_metadata": {
                "test_name": f"ç®€åŒ–æ ¼å¼è¯„ä¼° - {file_path.name}",
                "format": "simplified",
                "total_questions": len(unified_questions)
            },
            "assessment_questions": unified_questions,
            "original_content": content
        }

    def _process_custom_format(self, file_path: Path,
                             personality_params: Optional[Dict] = None) -> Dict:
        """å¤„ç†è‡ªå®šä¹‰æ ¼å¼"""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = json.load(f)

        logger.info(f"ğŸ” å¤„ç†è‡ªå®šä¹‰æ ¼å¼æ–‡ä»¶: {file_path.name}")

        # æ™ºèƒ½æå–é—®é¢˜å†…å®¹
        unified_questions = []

        # æŸ¥æ‰¾å¯èƒ½çš„é—®é¢˜å­—æ®µ
        potential_question_fields = []
        for key, value in content.items():
            if key in ["questions", "items", "test_items", "problems", "scenarios"]:
                if isinstance(value, list):
                    potential_question_fields.append((key, value))
                elif isinstance(value, dict):
                    potential_question_fields.append((key, list(value.values())))

        if potential_question_fields:
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„é—®é¢˜å­—æ®µ
            field_name, questions_data = potential_question_fields[0]
            logger.info(f"ğŸ“ ä½¿ç”¨é—®é¢˜å­—æ®µ: {field_name}")

            for i, item in enumerate(questions_data):
                question_text = item.get("question", item.get("text", item.get("content", "")))
                unified_question = {
                    "question_id": item.get("id", item.get("question_id", f"Q_{i+1}")),
                    "question": str(question_text)[0:500],  # é™åˆ¶é•¿åº¦
                    "dimension": item.get("dimension", "general"),
                    "evaluation_rubric": item.get("rubric", item.get("evaluation", {})),
                    "original_field": field_name
                }
                unified_questions.append(unified_question)
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨é—®é¢˜å­—æ®µï¼Œåˆ›å»ºé»˜è®¤è¯„ä¼°")
            return self._create_default_assessment(file_path, personality_params)

        return {
            "assessment_metadata": {
                "test_name": f"è‡ªå®šä¹‰æ ¼å¼è¯„ä¼° - {file_path.name}",
                "format": "custom",
                "field_used": field_name if potential_question_fields else "none",
                "total_questions": len(unified_questions)
            },
            "assessment_questions": unified_questions,
            "original_content": content
        }

    def _create_default_assessment(self, file_path: Path,
                                personality_params: Optional[Dict] = None) -> Dict:
        """åˆ›å»ºé»˜è®¤è¯„ä¼°å†…å®¹"""
        logger.info("ğŸ”„ åˆ›å»ºé»˜è®¤è¯„ä¼°å†…å®¹")

        default_questions = [
            {
                "question_id": "Q1",
                "question": "è¯·æè¿°æ‚¨çš„ä¸ªäººç‰¹ç‚¹å’ŒåŠ¨æœºã€‚",
                "dimension": "general",
                "evaluation_rubric": {
                    "description": "åŸºäºå›ç­”å†…å®¹è¿›è¡Œç»¼åˆè¯„ä¼°"
                }
            },
            {
                "question_id": "Q2",
                "question": "é¢å¯¹æŒ‘æˆ˜æ—¶ï¼Œæ‚¨é€šå¸¸ä¼šå¦‚ä½•åº”å¯¹ï¼Ÿ",
                "dimension": "challenge",
                "evaluation_rubric": {
                    "description": "è¯„ä¼°åº”å¯¹æŒ‘æˆ˜çš„æ–¹å¼å’Œèƒ½åŠ›"
                }
            },
            {
                "question_id": "Q3",
                "question": "æ‚¨è®¤ä¸ºä»€ä¹ˆæ ·çš„äº‹æƒ…æœ€æœ‰æ„ä¹‰ï¼Ÿ",
                "dimension": "values",
                "evaluation_rubric": {
                    "description": "è¯„ä¼°ä»·å€¼è§‚å’Œäººç”Ÿæ„ä¹‰è§‚"
                }
            }
        ]

        return {
            "assessment_metadata": {
                "test_name": f"é»˜è®¤è¯„ä¼° - {file_path.name}",
                "format": "default",
                "total_questions": len(default_questions),
                "auto_generated": True
            },
            "assessment_questions": default_questions,
            "warning": "ä½¿ç”¨é»˜è®¤è¯„ä¼°å†…å®¹ï¼Œå»ºè®®æä¾›å…·ä½“çš„æµ‹è¯•æ–‡ä»¶"
        }

    def _create_error_result(self, file_path: Union[str, Path], error_message: str,
                           personality_params: Optional[Dict] = None) -> Dict:
        """åˆ›å»ºé”™è¯¯ç»“æœ"""
        logger.error(f"ğŸš¨ åˆ›å»ºé”™è¯¯ç»“æœ: {error_message}")

        return {
            "assessment_metadata": {
                "test_name": f"é”™è¯¯å¤„ç† - {Path(file_path).name}",
                "format": "error",
                "error": True,
                "error_message": error_message
            },
            "assessment_questions": [],
            "system_info": {
                "error_occurred": True,
                "error_time": datetime.now().isoformat(),
                "robust_mode": True
            },
            "original_file_path": str(file_path)
        }

    def validate_processed_data(self, data: Dict) -> bool:
        """éªŒè¯å¤„ç†åçš„æ•°æ®"""
        if not isinstance(data, dict):
            logger.error("âŒ å¤„ç†åçš„æ•°æ®ä¸æ˜¯å­—å…¸æ ¼å¼")
            return False

        if "assessment_questions" not in data:
            logger.error("âŒ ç¼ºå°‘ assessment_questions å­—æ®µ")
            return False

        questions = data.get("assessment_questions", [])
        if not isinstance(questions, list):
            logger.error("âŒ assessment_questions ä¸æ˜¯åˆ—è¡¨æ ¼å¼")
            return False

        if len(questions) == 0:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é—®é¢˜")
            return False

        # éªŒè¯æ¯ä¸ªé—®é¢˜å­—æ®µ
        for i, question in enumerate(questions):
            if not isinstance(question, dict):
                logger.error(f"âŒ é—®é¢˜ {i+1} ä¸æ˜¯å­—å…¸æ ¼å¼")
                return False

            if "question_id" not in question:
                logger.warning(f"âš ï¸ é—®é¢˜ {i+1} ç¼ºå°‘ question_id")

            if "question" not in question or not question["question"].strip():
                logger.warning(f"âš ï¸ é—®é¢˜ {i+1} ç¼ºå°‘æˆ–å†…å®¹ä¸ºç©º")

        return True

    def run_assessment(self, file_path: Union[str, Path],
                        personality_params: Optional[Dict] = None) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹å¼ºå¥è¯„ä¼°: {file_path}")

            # å¤„ç†æ–‡ä»¶
            processed_data = self.process_file(file_path, personality_params)

            # éªŒè¯æ•°æ®
            if not self.validate_processed_data(processed_data):
                logger.error("âŒ æ•°æ®éªŒè¯å¤±è´¥")
                return processed_data

            # æ‰§è¡Œè¯„ä¼°ï¼ˆè¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„è¯„ä¼°é€»è¾‘ï¼‰
            logger.info("ğŸ“Š æ‰§è¡Œè¯„ä¼°åˆ†æ...")

            # ç”Ÿæˆè¯„ä¼°ç»“æœ
            assessment_result = {
                "success": True,
                "total_questions": len(processed_data.get("assessment_questions", [])),
                "processed_format": processed_data.get("system_info", {}).get("format_type", "unknown"),
                "file_path": str(file_path),
                "processing_time": processed_data.get("system_info", {}).get("processing_time"),
                "validation_passed": True
            }

            # æ·»åŠ åˆ°å¤„ç†ç»“æœä¸­
            processed_data["assessment_result"] = assessment_result

            logger.info("âœ… è¯„ä¼°å®Œæˆ")
            return processed_data

        except Exception as e:
            logger.error(f"âŒ è¯„ä¼°æµç¨‹å¤±è´¥: {e}")
            return self._create_error_result(file_path, str(e), personality_params)

def main():
    """æ¼”ç¤ºå¼ºå¥è¯„ä¼°ç³»ç»Ÿ"""
    system = RobustAssessmentSystem()

    print("ğŸ›¡ï¸ å¼ºå¥ç»Ÿä¸€è¯„ä¼°ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 50)

    # æµ‹è¯•ä¸åŒæ ¼å¼çš„æ–‡ä»¶
    test_files = [
        "llm_assessment/test_files/ä¸­æ–‡ç‰ˆ/agent-motivation-test.json",  # æ–°æ ¼å¼
        # å¯ä»¥æ·»åŠ æ›´å¤šæµ‹è¯•æ–‡ä»¶
    ]

    for file_path in test_files:
        print(f"\nğŸ“‹ æµ‹è¯•æ–‡ä»¶: {file_path}")
        print("-" * 30)

        result = system.run_assessment(file_path)

        if result.get("assessment_result", {}).get("success", False):
            print("âœ… å¤„ç†æˆåŠŸ")
            print(f"ğŸ“Š æ ¼å¼ç±»å‹: {result.get('system_info', {}).get('format_type', 'unknown')}")
            print(f"ğŸ“ é—®é¢˜æ•°é‡: {result.get('assessment_result', {}).get('total_questions', 0)}")
        else:
            print("âŒ å¤„ç†å¤±è´¥")
            print(f"âš ï¸ é”™è¯¯ä¿¡æ¯: {result.get('assessment_metadata', {}).get('error_message', 'æœªçŸ¥é”™è¯¯')}")

if __name__ == "__main__":
    main()