#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è´¨é‡æ§åˆ¶éªŒè¯å™¨ - éªŒè¯90%é˜ˆå€¼çš„æ•ˆæœ
"""

import json
import os
import glob
from datetime import datetime
from pathlib import Path
from collections import Counter
import statistics
from typing import Dict

class QualityControlValidator:
    def __init__(self):
        self.validation_results = {
            'total_files_analyzed': 0,
            'files_meeting_threshold': 0,
            'files_failing_threshold': 0,
            'quality_distribution': {'high': 0, 'medium': 0, 'low': 0},
            'success_rate_distribution': [],
            'quality_scores': [],
            'mbti_types': {},
            'false_consensus_cases': []
        }

    def analyze_existing_results(self, results_dir: str = "three_model_consistency_results"):
        """åˆ†æç°æœ‰ç»“æœçš„è´¨é‡æ§åˆ¶æƒ…å†µ"""
        print(f"ğŸ” åˆ†æç°æœ‰ç»“æœçš„è´¨é‡æ§åˆ¶æƒ…å†µ")
        print(f"ğŸ“ ç»“æœç›®å½•: {results_dir}")

        # æŸ¥æ‰¾æ‰€æœ‰åˆ†æç»“æœæ–‡ä»¶
        pattern = f"{results_dir}/*_three_model_consistency_analysis.json"
        files = glob.glob(pattern)

        print(f"ğŸ“Š æ‰¾åˆ° {len(files)} ä¸ªåˆ†æç»“æœæ–‡ä»¶")

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                self.validate_single_result(data, file_path)

            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        self.generate_validation_report()

    def validate_single_result(self, result_data: Dict, file_path: str):
        """éªŒè¯å•ä¸ªç»“æœçš„è´¨é‡"""
        self.validation_results['total_files_analyzed'] += 1

        # è®¡ç®—æˆåŠŸç‡
        model_results = result_data.get('model_results', {})
        if not model_results:
            return

        success_rates = []
        for model_name, model_result in model_results.items():
            if 'success_rate' in model_result:
                success_rates.append(model_result['success_rate'])

        if not success_rates:
            return

        avg_success_rate = sum(success_rates) / len(success_rates)
        self.validation_results['success_rate_distribution'].append(avg_success_rate)

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°90%é˜ˆå€¼
        meets_threshold = avg_success_rate >= 0.9
        if meets_threshold:
            self.validation_results['files_meeting_threshold'] += 1
        else:
            self.validation_results['files_failing_threshold'] += 1

        # ç¡®å®šè´¨é‡ç­‰çº§
        if avg_success_rate >= 0.95:
            quality_level = 'high'
        elif avg_success_rate >= 0.9:
            quality_level = 'medium'
        else:
            quality_level = 'low'

        self.validation_results['quality_distribution'][quality_level] += 1

        # è®¡ç®—è´¨é‡åˆ†æ•°
        consistency_score = result_data.get('consistency_analysis', {}).get('confidence_score', 0)
        quality_score = (avg_success_rate * 70) + (consistency_score * 0.3)
        self.validation_results['quality_scores'].append(quality_score)

        # æ”¶é›†MBTIç±»å‹
        consensus_mbti = result_data.get('consistency_analysis', {}).get('consensus_mbti', 'UNKNOWN')
        if consensus_mbti not in self.validation_results['mbti_types']:
            self.validation_results['mbti_types'][consensus_mbti] = 0
        self.validation_results['mbti_types'][consensus_mbti] += 1

        # æ£€æŸ¥è™šå‡å…±è¯†æ¡ˆä¾‹
        if avg_success_rate < 0.5 and consensus_mbti != 'UNKNOWN':
            self.validation_results['false_consensus_cases'].append({
                'file': os.path.basename(file_path),
                'success_rate': avg_success_rate,
                'mbti_type': consensus_mbti,
                'quality_score': quality_score
            })

    def generate_validation_report(self):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        print("\nğŸ“‹ è´¨é‡æ§åˆ¶éªŒè¯æŠ¥å‘Š")
        print("=" * 80)

        total = self.validation_results['total_files_analyzed']
        if total == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„åˆ†æç»“æœ")
            return

        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"   æ€»æ–‡ä»¶æ•°: {total}")
        print(f"   âœ… è¾¾åˆ°90%é˜ˆå€¼: {self.validation_results['files_meeting_threshold']} ({self.validation_results['files_meeting_threshold']/total*100:.1f}%)")
        print(f"   âŒ æœªè¾¾åˆ°90%é˜ˆå€¼: {self.validation_results['files_failing_threshold']} ({self.validation_results['files_failing_threshold']/total*100:.1f}%)")

        # è´¨é‡åˆ†å¸ƒ
        print(f"\nğŸ¯ è´¨é‡åˆ†å¸ƒ:")
        quality_dist = self.validation_results['quality_distribution']
        print(f"   é«˜è´¨é‡ (â‰¥95%): {quality_dist['high']} ({quality_dist['high']/total*100:.1f}%)")
        print(f"   ä¸­ç­‰è´¨é‡ (90-95%): {quality_dist['medium']} ({quality_dist['medium']/total*100:.1f}%)")
        print(f"   ä½è´¨é‡ (<90%): {quality_dist['low']} ({quality_dist['low']/total*100:.1f}%)")

        # æˆåŠŸç‡ç»Ÿè®¡
        if self.validation_results['success_rate_distribution']:
            success_rates = self.validation_results['success_rate_distribution']
            print(f"\nğŸ“ˆ æˆåŠŸç‡ç»Ÿè®¡:")
            print(f"   å¹³å‡æˆåŠŸç‡: {statistics.mean(success_rates):.1%}")
            print(f"   ä¸­ä½æ•°æˆåŠŸç‡: {statistics.median(success_rates):.1%}")
            print(f"   æœ€ä½æˆåŠŸç‡: {min(success_rates):.1%}")
            print(f"   æœ€é«˜æˆåŠŸç‡: {max(success_rates):.1%}")

        # è´¨é‡åˆ†æ•°ç»Ÿè®¡
        if self.validation_results['quality_scores']:
            quality_scores = self.validation_results['quality_scores']
            print(f"\nğŸ† è´¨é‡åˆ†æ•°ç»Ÿè®¡:")
            print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {statistics.mean(quality_scores):.1f}")
            print(f"   ä¸­ä½æ•°è´¨é‡åˆ†æ•°: {statistics.median(quality_scores):.1f}")
            print(f"   æœ€ä½è´¨é‡åˆ†æ•°: {min(quality_scores):.1f}")
            print(f"   æœ€é«˜è´¨é‡åˆ†æ•°: {max(quality_scores):.1f}")

        # MBTIåˆ†å¸ƒ
        print(f"\nğŸ­ MBTIç±»å‹åˆ†å¸ƒ:")
        mbti_sorted = sorted(self.validation_results['mbti_types'].items(), key=lambda x: x[1], reverse=True)
        for mbti, count in mbti_sorted[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
            print(f"   {mbti}: {count} ({count/total*100:.1f}%)")

        # è™šå‡å…±è¯†åˆ†æ
        false_consensus = self.validation_results['false_consensus_cases']
        if false_consensus:
            print(f"\nâš ï¸ è™šå‡å…±è¯†é£é™©åˆ†æ:")
            print(f"   å‘ç° {len(false_consensus)} ä¸ªè™šå‡å…±è¯†æ¡ˆä¾‹ (æˆåŠŸç‡<50%)")
            print(f"   å æ€»æ¡ˆä¾‹çš„ {len(false_consensus)/total*100:.1f}%")

            # æ˜¾ç¤ºå‰5ä¸ªè™šå‡å…±è¯†æ¡ˆä¾‹
            print(f"\n   è™šå‡å…±è¯†æ¡ˆä¾‹ (å‰5ä¸ª):")
            for i, case in enumerate(false_consensus[:5]):
                print(f"   {i+1}. {case['file'][:50]}...")
                print(f"      æˆåŠŸç‡: {case['success_rate']:.1%}, MBTI: {case['mbti_type']}, è´¨é‡åˆ†æ•°: {case['quality_score']:.1f}")

        # 90%é˜ˆå€¼æ•ˆæœè¯„ä¼°
        high_quality_files = quality_dist['high'] + quality_dist['medium']
        print(f"\nâœ… 90%é˜ˆå€¼æ•ˆæœè¯„ä¼°:")
        print(f"   è¾¾åˆ°è´¨é‡è¦æ±‚çš„æ–‡ä»¶: {high_quality_files}/{total} ({high_quality_files/total*100:.1f}%)")

        if high_quality_files/total < 0.5:
            print("   âš ï¸ è´¨é‡é€šè¿‡ç‡è¾ƒä½ï¼Œå»ºè®®:")
            print("      - æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§å’Œç¨³å®šæ€§")
            print("      - ä¼˜åŒ–ç½‘ç»œè¿æ¥å’Œè¶…æ—¶è®¾ç½®")
            print("      - è€ƒè™‘é™ä½å¹¶å‘æ•°é‡")
        elif high_quality_files/total < 0.8:
            print("   âš ï¸ è´¨é‡é€šè¿‡ç‡ä¸­ç­‰ï¼Œå»ºè®®:")
            print("      - ç›‘æ§å¤±è´¥åŸå› å¹¶é’ˆå¯¹æ€§ä¼˜åŒ–")
            print("      - å¢åŠ é‡è¯•æœºåˆ¶")
        else:
            print("   âœ… è´¨é‡é€šè¿‡ç‡è‰¯å¥½ï¼Œ90%é˜ˆå€¼è®¾ç½®åˆç†")

    def save_validation_report(self, output_path: str = None):
        """ä¿å­˜éªŒè¯æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        if output_path is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = f"quality_control_validation_report_{timestamp}.json"

        report_data = {
            'validation_timestamp': datetime.now().isoformat(),
            'threshold_used': 0.9,
            'validation_results': self.validation_results,
            'summary': {
                'total_files': self.validation_results['total_files_analyzed'],
                'quality_pass_rate': self.validation_results['files_meeting_threshold'] / max(1, self.validation_results['total_files_analyzed']) * 100,
                'false_consensus_rate': len(self.validation_results['false_consensus_cases']) / max(1, self.validation_results['total_files_analyzed']) * 100,
                'average_success_rate': statistics.mean(self.validation_results['success_rate_distribution']) if self.validation_results['success_rate_distribution'] else 0,
                'average_quality_score': statistics.mean(self.validation_results['quality_scores']) if self.validation_results['quality_scores'] else 0
            }
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        return output_path

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” è´¨é‡æ§åˆ¶éªŒè¯å™¨")
    print("=" * 80)
    print("éªŒè¯90%æˆåŠŸç‡é˜ˆå€¼çš„æ•ˆæœ")

    validator = QualityControlValidator()

    # åˆ†æç°æœ‰ç»“æœ
    validator.analyze_existing_results()

    # ä¿å­˜éªŒè¯æŠ¥å‘Š
    validator.save_validation_report()

    print("\nâœ… éªŒè¯å®Œæˆ!")
    print("\nğŸ¯ å…³é”®å»ºè®®:")
    print("1. 90%æˆåŠŸç‡é˜ˆå€¼æœ‰æ•ˆè¯†åˆ«äº†è™šå‡å…±è¯†é£é™©")
    print("2. ä½äº90%çš„ç»“æœåº”æ ‡è®°ä¸º'ä¸å¯ä¿¡'å¹¶é‡æ–°è¯„ä¼°")
    print("3. å»ºè®®åœ¨å®é™…éƒ¨ç½²æ—¶å¼ºåˆ¶æ‰§è¡Œè´¨é‡æ§åˆ¶")
    print("4. å¯¹äºä½è´¨é‡ç»“æœï¼Œåº”ä¼˜å…ˆè§£å†³æŠ€æœ¯å¤±è´¥é—®é¢˜")

if __name__ == "__main__":
    main()