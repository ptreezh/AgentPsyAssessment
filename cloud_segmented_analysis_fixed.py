#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç‰ˆäº‘è¯„ä¼°å™¨åˆ†æ®µå¼å¿ƒç†è¯„ä¼°åˆ†æå™¨
ä¿®å¤è¯„åˆ†é€»è¾‘ã€ç½®ä¿¡åº¦è®¡ç®—ã€é”™è¯¯å¤„ç†å’Œæ–‡ä»¶è¾“å‡ºç»“æ„
"""

import json
import sys
import time
from typing import Dict, List, Any, Tuple
from pathlib import Path
from datetime import datetime
import openai

class FixedCloudSegmentedPersonalityAnalyzer:
    """ä¿®å¤ç‰ˆäº‘è¯„ä¼°å™¨åˆ†æ®µå¼äººæ ¼åˆ†æå™¨"""

    def __init__(self, model: str = "qwen-long", api_key: str = None, max_questions_per_segment: int = 2):
        self.model = model
        self.api_key = api_key or "sk-ffd03518254b495b8d27e723cd413fc1"
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.max_questions_per_segment = max_questions_per_segment

        # åˆå§‹åŒ–Big5è¯„åˆ†ç´¯ç§¯å™¨
        self.big_five_traits = {
            'openness_to_experience': {'scores': [], 'evidence': [], 'weight': 0},
            'conscientiousness': {'scores': [], 'evidence': [], 'weight': 0},
            'extraversion': {'scores': [], 'evidence': [], 'weight': 0},
            'agreeableness': {'scores': [], 'evidence': [], 'weight': 0},
            'neuroticism': {'scores': [], 'evidence': [], 'weight': 0}
        }

        self.analysis_log = []
        self.per_question_scores = []
        self.segment_results = []

        # åˆå§‹åŒ–äº‘è¯„ä¼°å™¨å®¢æˆ·ç«¯
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        # æ£€æŸ¥APIè¿æ¥
        self.api_available = self._check_api_connection()

    def _check_api_connection(self) -> bool:
        """æ£€æŸ¥APIè¿æ¥æ˜¯å¦å¯ç”¨"""
        try:
            test_response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=10
            )
            print(f"âœ… APIè¿æ¥æˆåŠŸ: {self.model}")
            return True
        except Exception as e:
            print(f"âŒ APIè¿æ¥å¤±è´¥: {self.model} - {e}")
            return False

    def extract_questions(self, assessment_data: Dict) -> List[Dict]:
        """ä»è¯„ä¼°æ•°æ®ä¸­æå–é—®é¢˜åˆ—è¡¨"""
        if 'assessment_results' in assessment_data:
            assessment_results = assessment_data['assessment_results']

            if isinstance(assessment_results, list):
                if len(assessment_results) > 0 and isinstance(assessment_results[0], dict):
                    if 'question_data' in assessment_results[0]:
                        questions = []
                        for result in assessment_results:
                            if 'question_data' in result:
                                question_data = result['question_data'].copy()
                                if 'conversation_log' in result:
                                    for msg in result['conversation_log']:
                                        if msg.get('role') == 'assistant':
                                            question_data['agent_response'] = msg['content']
                                            break
                                questions.append(question_data)
                        return questions

        if 'questions' in assessment_data:
            return assessment_data['questions']

        print(f"è­¦å‘Š: æ— æ³•ä»æ•°æ®ä¸­æå–é—®é¢˜ï¼Œå¯ç”¨é”®: {list(assessment_data.keys())}")
        return []

    def create_segments(self, questions: List[Dict]) -> List[List[Dict]]:
        """åˆ›å»ºåˆ†æ®µï¼Œæ¯æ®µåŒ…å«æŒ‡å®šæ•°é‡çš„é—®é¢˜"""
        segments = []
        for i in range(0, len(questions), self.max_questions_per_segment):
            segment = questions[i:i + self.max_questions_per_segment]
            if segment:
                segments.append(segment)

        print(f"åˆ›å»º {len(segments)} ä¸ªåˆ†æ®µï¼Œæ¯æ®µæœ€å¤š {self.max_questions_per_segment} ä¸ªé—®é¢˜")
        return segments

    def analyze_segment(self, segment: List[Dict], segment_number: int) -> Dict:
        """åˆ†æå•ä¸ªåˆ†æ®µï¼Œä½¿ç”¨ä¸¥æ ¼çš„1-3-5è¯„åˆ†æ ‡å‡†"""

        system_prompt = f"""You are a personality analyst specialized in Big Five assessment. Analyze {len(segment)} questions and provide Big Five scores using ONLY the 1-3-5 scale.

For each question, assess all 5 traits using EXCLUSIVELY:
- 1: Low/Minimal expression of the trait
- 3: Moderate/Average expression of the trait
- 5: High/Strong expression of the trait

CRITICAL: Your scores must be EXACTLY 1, 3, or 5 - no other values are acceptable!

Scoring guidelines based on evidence quality:
- Direct evidence: Score 1, 3, or 5 based on explicit behavior in the response
- Limited evidence: Score 3 using professional inference when direct evidence is weak
- No evidence: Score 3 using professional judgment when no clear evidence exists

REQUIRED JSON FORMAT:
{{
    "question_scores": [
        {{
            "question_id": "Q1",
            "dimension": "extraversion",
            "big_five_scores": {{
                "openness_to_experience": {{"score": 3, "evidence": "Evidence from response", "quality": "direct"}},
                "conscientiousness": {{"score": 3, "evidence": "Professional inference", "quality": "inferred"}},
                "extraversion": {{"score": 5, "evidence": "Direct evidence", "quality": "direct"}},
                "agreeableness": {{"score": 3, "evidence": "Inference from response", "quality": "inferred"}},
                "neuroticism": {{"score": 1, "evidence": "Evidence from response", "quality": "direct"}}
            }}
        }}
    ]
}}

Return ONLY valid JSON. All traits must have scores 1, 3, or 5 with evidence. No null values."""

        # æ„å»ºç”¨æˆ·è¾“å…¥
        user_content = []
        for i, question in enumerate(segment):
            user_content.append(f"Question {i+1} ({question.get('dimension', 'Unknown')}):")
            user_content.append(f"Scenario: {question['scenario']}")
            response = question.get('agent_response', '')
            user_content.append(f"Response: {response[:500]}...")
            user_content.append(f"Rubric: {question.get('evaluation_rubric', {}).get('description', 'N/A')}")
            user_content.append("---")

        user_prompt = "\n".join(user_content)

        try:
            print(f"  ä½¿ç”¨ {self.model} åˆ†ææ®µ {segment_number} ({len(segment)} é¢˜)")

            # è°ƒç”¨äº‘è¯„ä¼°å™¨
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )

            response_text = response.choices[0].message.content
            print(f"  æ®µ {segment_number} åˆ†ææˆåŠŸ")

            return {
                'system_prompt': system_prompt,
                'user_prompt': user_prompt,
                'segment_number': segment_number,
                'llm_response': response_text,
                'success': True
            }

        except Exception as e:
            print(f"  æ®µ {segment_number} åˆ†æå¤±è´¥: {e}")
            return {
                'system_prompt': system_prompt,
                'user_prompt': user_prompt,
                'segment_number': segment_number,
                'error': str(e),
                'success': False
            }

    def validate_and_fix_score(self, score: Any) -> int:
        """éªŒè¯å¹¶ä¿®å¤è¯„åˆ†ï¼Œç¡®ä¿åªèƒ½æ˜¯1ã€3ã€5"""
        try:
            score = int(score)
            if score in [1, 3, 5]:
                return score
            elif score <= 2:
                return 1
            elif score <= 4:
                return 3
            else:
                return 5
        except (ValueError, TypeError):
            return 3  # é»˜è®¤å€¼

    def accumulate_scores(self, segment_result: Dict) -> None:
        """ç´¯ç§¯åˆ†æ®µåˆ†æç»“æœåˆ°æ€»è¯„åˆ†"""
        if not segment_result.get('success'):
            print(f"  è·³è¿‡å¤±è´¥çš„åˆ†æ®µ {segment_result.get('segment_number', 'Unknown')}")
            return

        try:
            # è§£æLLMå“åº”
            response_text = segment_result['llm_response']

            # å°è¯•æå–JSON
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                json_text = response_text[json_start:json_end].strip()
                response_data = json.loads(json_text)
            else:
                response_data = json.loads(response_text)

            if 'question_scores' not in response_data:
                print(f"  è­¦å‘Š: åˆ†æ®µ {segment_result['segment_number']} å“åº”ä¸­ç¼ºå°‘ question_scores")
                return

            # å¤„ç†æ¯ä¸ªé—®é¢˜çš„è¯„åˆ†
            for question_score in response_data['question_scores']:
                question_id = question_score.get('question_id', f'Q{len(self.per_question_scores)+1}')
                big_five_scores = question_score.get('big_five_scores', {})

                validated_scores = {}
                for trait, score_data in big_five_scores.items():
                    if trait in self.big_five_traits:
                        raw_score = score_data.get('score', 3)
                        score = self.validate_and_fix_score(raw_score)
                        evidence = score_data.get('evidence', '')
                        quality = score_data.get('quality', 'inferred')

                        # è®°å½•ä¿®å¤æƒ…å†µ
                        if raw_score != score:
                            print(f"    ä¿®å¤ {trait} è¯„åˆ†: {raw_score} -> {score}")

                        self.big_five_traits[trait]['scores'].append(score)
                        self.big_five_traits[trait]['evidence'].append(evidence)
                        self.big_five_traits[trait]['weight'] += 1

                        validated_scores[trait] = {
                            'original_score': raw_score,
                            'validated_score': score,
                            'evidence': evidence,
                            'quality': quality
                        }

                # è®°å½•æ¯ä¸ªé—®é¢˜çš„è¯„åˆ†
                self.per_question_scores.append({
                    'question_id': question_id,
                    'segment_number': segment_result['segment_number'],
                    'big_five_scores': validated_scores
                })

            print(f"  æˆåŠŸç´¯ç§¯åˆ†æ®µ {segment_result['segment_number']} çš„è¯„åˆ†")

        except Exception as e:
            print(f"  è§£æåˆ†æ®µ {segment_result.get('segment_number', 'Unknown')} ç»“æœå¤±è´¥: {e}")

    def calculate_final_scores(self) -> Dict:
        """è®¡ç®—æœ€ç»ˆçš„Big5è¯„åˆ†å’Œç½®ä¿¡åº¦"""
        final_scores = {}

        for trait, data in self.big_five_traits.items():
            scores = data['scores']
            weight = data['weight']

            if weight > 0:
                # è®¡ç®—å¹³å‡åˆ†
                avg_score = sum(scores) / len(scores)

                # å››èˆäº”å…¥åˆ°æœ€æ¥è¿‘çš„1ã€3ã€5
                if avg_score <= 2:
                    final_score = 1
                elif avg_score <= 4:
                    final_score = 3
                else:
                    final_score = 5

                # è®¡ç®—ç½®ä¿¡åº¦
                score_distribution = {1: 0, 3: 0, 5: 0}
                for score in scores:
                    score_distribution[score] += 1

                # ç½®ä¿¡åº¦è®¡ç®—ï¼šåŸºäºè¯„åˆ†ä¸€è‡´æ€§
                max_count = max(score_distribution.values())
                consistency = max_count / len(scores)  # è¯„åˆ†ä¸€è‡´æ€§
                confidence = round(consistency * 100, 1)

                final_scores[trait] = {
                    'final_score': final_score,
                    'average_score': round(avg_score, 2),
                    'raw_scores': scores.copy(),
                    'score_distribution': score_distribution,
                    'confidence_percent': confidence,
                    'evidence_count': len(data['evidence']),
                    'weight': weight,
                    'evidence_samples': data['evidence'][:3]
                }
            else:
                final_scores[trait] = {
                    'final_score': 3,  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
                    'average_score': 3.0,
                    'raw_scores': [],
                    'score_distribution': {1: 0, 3: 0, 5: 0},
                    'confidence_percent': 0.0,  # æ— æ•°æ®æ—¶ç½®ä¿¡åº¦ä¸º0
                    'evidence_count': 0,
                    'weight': 0,
                    'evidence_samples': [],
                    'warning': 'No successful segment analysis'
                }

        return final_scores

    def generate_mbti_type(self, final_scores: Dict) -> Dict:
        """åŸºäºBig5è¯„åˆ†ç”ŸæˆMBTIç±»å‹å’Œç½®ä¿¡åº¦"""
        O = final_scores.get('openness_to_experience', {}).get('final_score', 3)
        C = final_scores.get('conscientiousness', {}).get('final_score', 3)
        E = final_scores.get('extraversion', {}).get('final_score', 3)
        A = final_scores.get('agreeableness', {}).get('final_score', 3)
        N = final_scores.get('neuroticism', {}).get('final_score', 3)

        # MBTIè®¡ç®—é€»è¾‘
        # E/I: å¤–å‘æ€§ vs ç¥ç»è´¨
        e_score = E + (5 - N)  # é«˜å¤–å‘æ€§+ä½ç¥ç»è´¨=æ›´å¤–å‘
        i_score = (5 - E) + N
        E_preference = 'E' if e_score > i_score else 'I'
        E_confidence = abs(e_score - i_score) / 8  # ç½®ä¿¡åº¦ 0-1

        # S/N: æ„Ÿè§‰ vs ç›´è§‰ (åŸºäºå¼€æ”¾æ€§)
        S_preference = 'S' if O <= 3 else 'N'
        S_confidence = abs(O - 3) / 2  # è·ç¦»ä¸­é—´å€¼çš„è·ç¦»

        # T/F: æ€è€ƒ vs æƒ…æ„Ÿ (åŸºäºå®œäººæ€§)
        T_preference = 'T' if A <= 3 else 'F'
        T_confidence = abs(A - 3) / 2

        # J/P: åˆ¤æ–­ vs æ„ŸçŸ¥ (åŸºäºå°½è´£æ€§)
        J_preference = 'J' if C > 3 else 'P'
        J_confidence = abs(C - 3) / 2

        mbti_type = f"{E_preference}{S_preference}{T_preference}{J_preference}"

        # æ•´ä½“MBTIç½®ä¿¡åº¦
        overall_confidence = (E_confidence + S_confidence + T_confidence + J_confidence) / 4

        return {
            'type': mbti_type,
            'component_scores': {
                'E/I': {'score': e_score, 'preference': E_preference, 'confidence': round(E_confidence * 100, 1)},
                'S/N': {'score': O, 'preference': S_preference, 'confidence': round(S_confidence * 100, 1)},
                'T/F': {'score': A, 'preference': T_preference, 'confidence': round(T_confidence * 100, 1)},
                'J/P': {'score': C, 'preference': J_preference, 'confidence': round(J_confidence * 100, 1)}
            },
            'overall_confidence': round(overall_confidence * 100, 1),
            'preferences': {
                'extraversion_introversion': E_preference,
                'sensing_intuition': S_preference,
                'thinking_feeling': T_preference,
                'judging_perceiving': J_preference
            }
        }

    def save_separate_files(self, base_filename: str, output_dir: Path,
                           final_scores: Dict, mbti_result: Dict,
                           per_question_scores: List, segment_results: List):
        """åˆ†ç¦»ä¿å­˜ä¸åŒç±»å‹çš„è¾“å‡ºæ–‡ä»¶"""

        output_dir.mkdir(parents=True, exist_ok=True)

        # 1. ä¿å­˜ä¸»è¦æ‘˜è¦æ–‡ä»¶ï¼ˆæœ€ç»ˆè¯„åˆ†å’ŒMBTIï¼‰
        summary_file = output_dir / f"{base_filename}_summary.json"
        summary_data = {
            'file_info': {
                'filename': base_filename,
                'analysis_timestamp': datetime.now().isoformat(),
                'model_used': self.model,
                'api_available': self.api_available
            },
            'big_five_final_scores': {
                trait: {
                    'final_score': data['final_score'],
                    'confidence_percent': data['confidence_percent'],
                    'evidence_count': data['evidence_count']
                } for trait, data in final_scores.items()
            },
            'mbti_assessment': {
                'type': mbti_result['type'],
                'overall_confidence': mbti_result['overall_confidence'],
                'component_confidence': {
                    comp: data['confidence']
                    for comp, data in mbti_result['component_scores'].items()
                }
            },
            'analysis_quality': {
                'total_segments_attempted': len(segment_results),
                'successful_segments': sum(1 for r in segment_results if r.get('success', False)),
                'total_questions_analyzed': len(per_question_scores),
                'successful_analysis_rate': sum(1 for r in segment_results if r.get('success', False)) / len(segment_results) * 100 if segment_results else 0
            }
        }

        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)

        # 2. ä¿å­˜è¯¦ç»†è¯„åˆ†ä¾æ®æ–‡ä»¶
        evidence_file = output_dir / f"{base_filename}_detailed_evidence.json"
        evidence_data = {
            'file_info': summary_data['file_info'],
            'big_five_detailed_scores': final_scores,
            'per_question_analysis': per_question_scores,
            'segment_analysis_log': segment_results
        }

        with open(evidence_file, 'w', encoding='utf-8') as f:
            json.dump(evidence_data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ æ‘˜è¦æ–‡ä»¶: {summary_file.name}")
        print(f"ğŸ“‹ è¯¦ç»†è¯æ®æ–‡ä»¶: {evidence_file.name}")

        return str(summary_file), str(evidence_file)

    def analyze_full_assessment(self, assessment_file: str, output_dir: str = "fixed_results") -> Dict:
        """åˆ†æå®Œæ•´çš„è¯„ä¼°æ–‡ä»¶"""
        print(f"ğŸ” å¼€å§‹åˆ†æ: {assessment_file}")

        # æ£€æŸ¥APIå¯ç”¨æ€§
        if not self.api_available:
            print(f"âŒ APIä¸å¯ç”¨ï¼Œè·³è¿‡åˆ†æ: {assessment_file}")
            return {
                'success': False,
                'error': 'API connection failed',
                'file': assessment_file
            }

        try:
            # åŠ è½½è¯„ä¼°æ•°æ®
            with open(assessment_file, 'r', encoding='utf-8') as f:
                assessment_data = json.load(f)

            # æå–é—®é¢˜
            questions = self.extract_questions(assessment_data)
            if not questions:
                raise ValueError("æ— æ³•ä»è¯„ä¼°æ–‡ä»¶ä¸­æå–é—®é¢˜")

            print(f"ğŸ“Š æå–åˆ° {len(questions)} ä¸ªé—®é¢˜")

            # åˆ›å»ºåˆ†æ®µ
            segments = self.create_segments(questions)

            # åˆ†ææ¯ä¸ªåˆ†æ®µ
            for i, segment in enumerate(segments, 1):
                print(f"ğŸ“ åˆ†æåˆ†æ®µ {i}/{len(segments)}...")
                result = self.analyze_segment(segment, i)
                self.segment_results.append(result)

                # ç´¯ç§¯è¯„åˆ†
                self.accumulate_scores(result)

                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                time.sleep(1)

            # è®¡ç®—æœ€ç»ˆè¯„åˆ†
            final_scores = self.calculate_final_scores()
            mbti_result = self.generate_mbti_type(final_scores)

            # ä¿å­˜åˆ†ç¦»çš„æ–‡ä»¶
            base_filename = Path(assessment_file).stem
            output_path = Path(output_dir)
            summary_file, evidence_file = self.save_separate_files(
                base_filename, output_path / self.model,
                final_scores, mbti_result, self.per_question_scores, self.segment_results
            )

            print(f"âœ… åˆ†æå®Œæˆ: {Path(assessment_file).name}")

            # æ˜¾ç¤ºæ‘˜è¦
            print(f"ğŸ¯ Big5æœ€ç»ˆè¯„åˆ†:")
            for trait, data in final_scores.items():
                score = data['final_score']
                confidence = data['confidence_percent']
                weight = data['weight']
                print(f"  {trait}: {score}/5 (ç½®ä¿¡åº¦: {confidence}%, åŸºäº{weight}ä¸ªè¯æ®)")

            print(f"ğŸ§  MBTIç±»å‹: {mbti_result['type']} (ç½®ä¿¡åº¦: {mbti_result['overall_confidence']}%)")

            # è®¡ç®—æˆåŠŸç‡
            successful_segments = sum(1 for r in self.segment_results if r.get('success', False))
            success_rate = successful_segments / len(self.segment_results) * 100

            return {
                'success': True,
                'file': assessment_file,
                'summary_file': summary_file,
                'evidence_file': evidence_file,
                'big_five_scores': {trait: data['final_score'] for trait, data in final_scores.items()},
                'mbti_type': mbti_result['type'],
                'analysis_quality': {
                    'success_rate': success_rate,
                    'successful_segments': successful_segments,
                    'total_segments': len(self.segment_results)
                }
            }

        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {assessment_file} - {e}")
            return {
                'success': False,
                'error': str(e),
                'file': assessment_file
            }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description='ä¿®å¤ç‰ˆäº‘è¯„ä¼°å™¨åˆ†æ®µå¼Big5åˆ†æ')
    parser.add_argument('input_file', help='è¾“å…¥è¯„ä¼°æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', default='qwen-long', choices=['qwen-long', 'qwen-max'],
                       help='ä½¿ç”¨çš„äº‘æ¨¡å‹')
    parser.add_argument('--output', default='fixed_cloud_results', help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    # åˆ›å»ºåˆ†æå™¨
    analyzer = FixedCloudSegmentedPersonalityAnalyzer(model=args.model)

    # æ‰§è¡Œåˆ†æ
    result = analyzer.analyze_full_assessment(args.input_file, args.output)

    if result['success']:
        print(f"\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆ!")
        print(f"ğŸ“Š Big5: {result['big_five_scores']}")
        print(f"ğŸ§  MBTI: {result['mbti_type']}")
        print(f"ğŸ“ˆ åˆ†æè´¨é‡: {result['analysis_quality']['success_rate']:.1f}% åˆ†æ®µæˆåŠŸ")
    else:
        print(f"\nğŸ’¥ åˆ†æå¤±è´¥: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()