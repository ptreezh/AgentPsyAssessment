# 可信评估实施计划 (严格按照用户要求重构版)

## 核心原则
1. **独立评估**: 每个评估器独立对原始测评报告进行分段评分
2. **题目级评分**: 每个评估器对每道题(共50题)分别给出1-10分评分和依据
3. **无后续分析**: 评估器只评分，不进行大五或MBTI分析
4. **一致性验证**: 三份评分完成后验证一致性和分歧
5. **分歧处理**: 不一致则增加两个评估器重新评分
6. **多数一致**: 去掉极值后看是否形成多数一致
7. **可信度要求**: 全部题目评分可信后才进行后续分析
8. **真实调用**: 确保评估器真实调用LLM，失败则更换模型

## 实施流程

### 第一阶段：基础设施准备
1. 为每个原始测评报告创建独立评估目录
2. 为每个评估器创建独立工作空间
3. 复制原始测评报告到各评估器工作目录

### 第二阶段：三评估器独立评分
#### 评估器配置
- 评估器1: Ollama Mistral (本地模型)
- 评估器2: Ollama Llama3 (本地模型)  
- 评估器3: Ollama Gemma (本地模型)

#### 评分要求 (评估器只做评分，不做分析)
1. 每个评估器独立对50道题逐题评分
2. 每道题评分范围: 1-10分
3. 每道题必须提供评分依据(直接证据/推断证据)
4. 每个评估器生成独立的题目评分文件
5. **严禁**进行大五人格或MBTI类型分析
6. 确保真实LLM调用，失败则更换模型

### 第三阶段：一致性检验与分歧识别
#### 检验标准
- **高一致性**: 三评估器评分差异≤1分 → 可信
- **中等一致性**: 三评估器评分差异2分 → 需要增加评估器
- **低一致性**: 三评估器评分差异≥3分 → 需要增加评估器

#### 检验流程
1. 对比三个评估器对每道题的评分
2. 识别一致性不足的题目(差异≥2分)
3. 记录分歧题目清单

### 第四阶段：分歧题目处理 (增加两个评估器)
#### 处理流程
1. 对分歧题目增加两个额外评估器:
   - 额外评估器4: Ollama Phi3 (本地模型)
   - 额外评估器5: Ollama Qwen2 (本地模型)
2. 五个评估器对分歧题目重新评分
3. 去掉最高分和最低分后看是否形成多数一致

#### 多数一致判断标准
- 去掉极值后，剩余3个评分中至少2个相同或差异≤1分 → 多数一致
- 去掉极值后，剩余评分仍存在较大差异(≥2分) → 仍需增加评估器

### 第五阶段：进一步分歧处理 (再增加两个评估器)
#### 处理条件
如果5个评估器仍存在分歧(去掉极值后仍无多数一致)

#### 处理流程
1. 再增加两个额外评估器:
   - 额外评估器6: Ollama DeepSeek (本地模型)
   - 额外评估器7: Ollama Yi (本地模型)
2. 七个评估器对仍分歧的题目重新评分
3. 重复多数一致判断过程

### 第六阶段：可信度验证
#### 验证标准
- 每道题必须有至少3个有效评分
- 每道题必须形成多数一致(至少3个评分相同或差异≤1分)
- 所有50道题目均通过一致性检验

#### 验证流程
1. 确认所有题目均已通过多数一致检验
2. 生成最终可信题目评分清单
3. 标记未通过检验的题目及原因

## 评估器真实性保障

### 调用验证措施
1. 记录每次LLM调用的完整请求和响应
2. 验证响应内容的独特性和逻辑性
3. 检查是否存在模板化或重复性回答
4. 记录调用时间和资源消耗

### 失败处理机制
1. **调用失败**: 立即更换同类型其他模型重新评估
2. **响应异常**: 更换不同架构模型重新评估
3. **一致性不足**: 增加评估器数量直到形成多数一致
4. **质量不达标**: 重新训练提示或更换评估器

## 评估器配置扩展

### 本地模型评估器
1. Ollama Mistral系列
2. Ollama Llama3系列
3. Ollama Gemma系列
4. Ollama Phi系列
5. Ollama Qwen系列
6. Ollama DeepSeek系列
7. Ollama Yi系列

### 云模型评估器
1. 阿里云百炼平台模型
2. 腾讯混元模型
3. 百度文心一言模型
4. 讯飞星火模型
5. 通义千问API模型
6. Claude API模型
7. GPT系列API模型

## 质量控制措施

### 评分质量控制
1. 每道题评分必须附带具体证据
2. 证据必须来自原始测评报告内容
3. 禁止无依据的推测性评分
4. 评分差异必须有合理解释

### 一致性控制
1. 题目级一致性检验
2. 评估器间差异分析
3. 多数一致形成机制
4. 分歧处理递进流程

### 可信度控制
1. 可信题目必须≥45题(90%)
2. 剩余题目必须说明不可信原因
3. 整体一致性系数计算
4. 可信度评级(1-10分)

## 目录结构
```
trustworthy_assessment_system/
├── assessment_reports/
│   ├── [测评报告名称]_assessment/
│   │   ├── evaluator_1/
│   │   │   ├── question_scores.json
│   │   │   └── llm_call_logs/
│   │   ├── evaluator_2/
│   │   ├── evaluator_3/
│   │   ├── additional_evaluators/
│   │   │   ├── evaluator_4/
│   │   │   ├── evaluator_5/
│   │   │   ├── evaluator_6/
│   │   │   └── evaluator_7/
│   │   └── consistency_verification/
│   │       ├── round_1_disagreements.json
│   │       ├── round_2_disagreements.json
│   │       ├── round_3_disagreements.json
│   │       └── final_verified_scores.json
└── README.md
```

## 后续分析阶段 (评分全部可信后)
只有在所有题目评分通过可信度验证后，才进入下一阶段:
1. 基于可信题目评分进行大五人格分析
2. 基于大五人格分析推断MBTI类型
3. 进行认知功能分析

## 成功标准
1. 每个评估器真实调用LLM完成独立评分
2. 所有50道题目均形成多数一致
3. 可信题目≥45题(90%)
4. 生成最终可信题目评分清单
5. 可进行后续大五和MBTI分析

## 失败处理
1. 评估器调用失败: 立即更换模型重新评分
2. 一致性不足: 按流程增加评估器直到形成多数一致
3. 可信度不达标: 标记为不可信并说明原因
4. 无法完成评估: 记录失败原因并重新安排