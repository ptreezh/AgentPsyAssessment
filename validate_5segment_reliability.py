#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éªŒè¯5é¢˜åˆ†æ®µæ–¹æ¡ˆçš„ä¿¡åº¦å’Œå¤šæ¨¡å‹ä¸€è‡´æ€§
ä¸¥æ ¼éµå¾ª1-3-5è¯„åˆ†æ ‡å‡†
"""

import sys
import json
import time
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime

# è®¾ç½®ç¯å¢ƒå˜é‡
import os
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['DASHSCOPE_API_KEY'] = 'sk-ded837735b3c44599a9bc138da561c27'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def load_test_data() -> List[Dict]:
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    test_file = "results/results/asses_deepseek_r1_70b_agent_big_five_50_complete2_a10_e0_t0_0_09271.json"

    try:
        with open(test_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        questions = []
        if 'assessment_results' in data and isinstance(data['assessment_results'], list):
            for item in data['assessment_results'][:10]:  # åªå–å‰10é¢˜ç”¨äºéªŒè¯
                if isinstance(item, dict) and 'question_data' in item:
                    question_data = item['question_data']
                    if isinstance(question_data, dict):
                        question_text = question_data.get('prompt_for_agent', question_data.get('mapped_ipip_concept', ''))

                        answer_text = ''
                        if 'extracted_response' in item and item['extracted_response']:
                            answer_text = item['extracted_response']
                        elif 'conversation_log' in item and isinstance(item['conversation_log'], list):
                            for msg in item['conversation_log']:
                                if isinstance(msg, dict) and msg.get('role') == 'assistant':
                                    answer_text = msg.get('content', '')
                                    break

                        if question_text and answer_text:
                            questions.append({
                                'question': question_text,
                                'answer': answer_text
                            })
        return questions

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return []

def create_strict_prompt(segment: List[Dict], segment_num: int) -> str:
    """åˆ›å»ºä¸¥æ ¼çš„1-3-5è¯„åˆ†æç¤º"""

    prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**ä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°å›ç­”è€…å±•ç°çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**é‡è¦æé†’ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…ï¼Œä¸è¦å›ç­”é—®å·é—®é¢˜
- âŒ ä¸è¦æ··æ·†è§’è‰²ï¼Œä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä¸“æ³¨äºåˆ†æå›ç­”ä¸­çš„äººæ ¼ç‰¹å¾
- âœ… å¿½ç•¥è§’è‰²æ‰®æ¼”å†…å®¹ï¼Œä¸“æ³¨å®é™…è¡Œä¸ºå€¾å‘

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ç¨‹åº¦
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æº
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼ˆå¿…é¡»ä½¿ç”¨ï¼‰ï¼š**
- **1åˆ†**ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- **3åˆ†**ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- **5åˆ†**ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼Œç¦æ­¢ä½¿ç”¨2ã€4ç­‰å…¶ä»–æ•°å€¼ï¼**

**ç¬¬{segment_num}æ®µé—®å·å†…å®¹ï¼ˆ{len(segment)}é¢˜ï¼‰ï¼š**
"""

    for i, item in enumerate(segment, 1):
        prompt += f"""
**é—®é¢˜ {i}:**
{item['question']}

**å›ç­” {i}:**
{item['answer']}

---
"""

    prompt += """
**è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š**
```json
{
  "success": true,
  "segment_number": åˆ†æ®µç¼–å·,
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“è¯æ®å¼•ç”¨",
    "conscientiousness": "å…·ä½“è¯æ®å¼•ç”¨",
    "extraversion": "å…·ä½“è¯æ®å¼•ç”¨",
    "agreeableness": "å…·ä½“è¯æ®å¼•ç”¨",
    "neuroticism": "å…·ä½“è¯æ®å¼•ç”¨"
  },
  "confidence": "high/medium/low"
}
```

**å†æ¬¡æé†’ï¼šæ¯ä¸ªè¯„åˆ†å¿…é¡»æ˜¯1ã€3æˆ–5ï¼Œä¸èƒ½ä½¿ç”¨å…¶ä»–æ•°å€¼ï¼**
"""

    return prompt

def analyze_with_model(model_name: str, api_key: str, base_url: str, segment: List[Dict], segment_num: int) -> Dict:
    """ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æåˆ†æ®µ"""
    try:
        import openai
        client = openai.OpenAI(api_key=api_key, base_url=base_url)

        prompt = create_strict_prompt(segment, segment_num)

        print(f"  ğŸ” ä½¿ç”¨ {model_name} åˆ†ææ®µ{segment_num}...")

        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=2000,
            temperature=0.1
        )

        content = response.choices[0].message.content
        print(f"  ğŸ“ å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")

        # è§£æJSON
        try:
            # å…ˆå°è¯•æå–```json```åŒ…è£¹çš„å†…å®¹
            import re
            json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                print(f"  ğŸ” æ‰¾åˆ°```json```åŒ…è£¹çš„å†…å®¹")
                try:
                    result = json.loads(json_str)
                    print(f"  ğŸ“„ ```json```å†…å®¹è§£ææˆåŠŸ")
                except json.JSONDecodeError as e2:
                    print(f"  âŒ ```json```å†…å®¹è§£æå¤±è´¥: {str(e2)[:100]}...")
                    raise Exception("JSONè§£æå¤±è´¥")
            else:
                # å¦‚æœæ²¡æœ‰```json```ï¼Œå°è¯•ç›´æ¥è§£æ
                try:
                    result = json.loads(content)
                    print(f"  ğŸ“„ ç›´æ¥JSONè§£ææˆåŠŸ")
                except json.JSONDecodeError as e:
                    print(f"  âš ï¸ ç›´æ¥JSONè§£æå¤±è´¥: {str(e)[:100]}...")
                    # å°è¯•æå–ä»»ä½•JSONæ ¼å¼
                    json_match = re.search(r'\\{.*?\\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                        print(f"  ğŸ” æ‰¾åˆ°JSONç‰‡æ®µ: {json_str[:200]}...")
                        try:
                            result = json.loads(json_str)
                            print(f"  ğŸ“„ ç‰‡æ®µJSONè§£ææˆåŠŸ")
                        except json.JSONDecodeError as e2:
                            print(f"  âŒ ç‰‡æ®µJSONä¹Ÿè§£æå¤±è´¥: {str(e2)[:100]}...")
                            print(f"  ğŸ“ å®Œæ•´å“åº”å†…å®¹: {content[:500]}...")
                            raise Exception("æ— æ³•è§£æJSON")
                    else:
                        print(f"  âŒ æœªæ‰¾åˆ°JSONæ ¼å¼")
                        print(f"  ğŸ“ å®Œæ•´å“åº”å†…å®¹: {content[:500]}...")
                        raise Exception("æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„JSON")

        except Exception as e:
            print(f"  âŒ JSONè§£æå¼‚å¸¸: {e}")
            raise e

        # éªŒè¯è¯„åˆ†æ ‡å‡†
        if 'scores' in result:
            scores = result['scores']
            invalid_scores = []
            for trait, score in scores.items():
                if score not in [1, 3, 5]:
                    invalid_scores.append(f"{trait}:{score}")

            if invalid_scores:
                print(f"  âš ï¸ å‘ç°æ— æ•ˆè¯„åˆ†: {invalid_scores}")
                # å°†æ— æ•ˆè¯„åˆ†ä¿®æ­£ä¸ºæœ€æ¥è¿‘çš„æœ‰æ•ˆè¯„åˆ†
                for trait, score in scores.items():
                    if score not in [1, 3, 5]:
                        if score < 2:
                            scores[trait] = 1
                        elif score > 4:
                            scores[trait] = 5
                        else:
                            scores[trait] = 3
                print(f"  ğŸ”§ ä¿®æ­£åè¯„åˆ†: {scores}")

        result['model'] = model_name
        result['segment_number'] = segment_num
        result['raw_response_length'] = len(content)

        print(f"  âœ… {model_name} åˆ†ææˆåŠŸ")
        return result

    except Exception as e:
        print(f"  âŒ {model_name} åˆ†æå¤±è´¥: {e}")
        return {
            'success': False,
            'model': model_name,
            'segment_number': segment_num,
            'error': str(e)
        }

def calculate_model_consistency(results: List[Dict]) -> Dict:
    """è®¡ç®—æ¨¡å‹ä¸€è‡´æ€§"""
    if not results:
        return {'consistency_score': 0, 'details': {}}

    # æ”¶é›†æ‰€æœ‰æˆåŠŸçš„ç»“æœ
    successful_results = [r for r in results if r.get('success', False) and 'scores' in r]

    if len(successful_results) < 2:
        return {'consistency_score': 0, 'details': {'error': 'æœ‰æ•ˆç»“æœä¸è¶³2ä¸ª'}}

    # è®¡ç®—æ¯ä¸ªç»´åº¦çš„ä¸€è‡´æ€§
    traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
    consistency_details = {}

    for trait in traits:
        scores = [r['scores'][trait] for r in successful_results]
        unique_scores = set(scores)
        consistency_details[trait] = {
            'scores': scores,
            'unique_count': len(unique_scores),
            'most_common': max(set(scores), key=scores.count),
            'is_consistent': len(unique_scores) == 1
        }

    # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§
    consistent_traits = sum(1 for detail in consistency_details.values() if detail['is_consistent'])
    total_traits = len(traits)
    consistency_score = (consistent_traits / total_traits) * 100 if total_traits > 0 else 0

    return {
        'consistency_score': consistency_score,
        'consistent_traits': consistent_traits,
        'total_traits': total_traits,
        'details': consistency_details,
        'successful_models': len(successful_results)
    }

def validate_5segment_reliability():
    """éªŒè¯5é¢˜åˆ†æ®µæ–¹æ¡ˆçš„ä¿¡åº¦"""
    print("ğŸ” 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¿¡åº¦éªŒè¯")
    print("=" * 60)
    print("âœ“ ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†")
    print("âœ“ å¤šæ¨¡å‹ä¸€è‡´æ€§éªŒè¯")
    print("âœ“ ä¿¡åº¦è¯„ä¼°")
    print()

    # åŠ è½½æµ‹è¯•æ•°æ®
    questions = load_test_data()
    if len(questions) < 10:
        print("âŒ æµ‹è¯•æ•°æ®ä¸è¶³")
        return

    print(f"ğŸ“‹ åŠ è½½äº† {len(questions)} ä¸ªé—®é¢˜")
    print()

    # æµ‹è¯•æ¨¡å‹é…ç½®
    models = [
        {"name": "qwen-long", "api_key": "sk-ded837735b3c44599a9bc138da561c27", "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"},
        {"name": "qwen-max", "api_key": "sk-ded837735b3c44599a9bc138da561c27", "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1"},
    ]

    # åˆ†æ®µæµ‹è¯•
    segments = [
        questions[:5],   # åˆ†æ®µ1
        questions[5:10]  # åˆ†æ®µ2
    ]

    all_results = []

    for seg_num, segment in enumerate(segments, 1):
        print(f"ğŸ§ª æµ‹è¯•åˆ†æ®µ {seg_num} ({len(segment)}é¢˜)")
        print("-" * 40)

        segment_results = []

        for model_config in models:
            result = analyze_with_model(
                model_config["name"],
                model_config["api_key"],
                model_config["base_url"],
                segment,
                seg_num
            )
            segment_results.append(result)
            time.sleep(2)  # é¿å…APIé™åˆ¶

        # è®¡ç®—è¯¥åˆ†æ®µçš„ä¸€è‡´æ€§
        consistency = calculate_model_consistency(segment_results)
        print(f"  ğŸ“Š åˆ†æ®µ{seg_num}ä¸€è‡´æ€§: {consistency['consistency_score']:.1f}%")

        if consistency.get('details') and isinstance(consistency['details'], dict):
            for trait, detail in consistency['details'].items():
                if isinstance(detail, dict) and detail.get('is_consistent'):
                    print(f"    âœ… {trait}: {detail['scores']} (ä¸€è‡´)")
                elif isinstance(detail, dict):
                    print(f"    âš ï¸ {trait}: {detail['scores']} (ä¸ä¸€è‡´)")

        all_results.extend(segment_results)
        print()

    # æ€»ä½“ä¿¡åº¦è¯„ä¼°
    print("ğŸ¯ æ€»ä½“ä¿¡åº¦è¯„ä¼°")
    print("=" * 40)

    overall_consistency = calculate_model_consistency(all_results)
    print(f"ğŸ“Š æ€»ä½“ä¸€è‡´æ€§: {overall_consistency['consistency_score']:.1f}%")
    print(f"ğŸ“ˆ ä¸€è‡´ç»´åº¦: {overall_consistency['consistent_traits']}/{overall_consistency['total_traits']}")
    print(f"ğŸ¤– æˆåŠŸæ¨¡å‹æ•°: {overall_consistency['successful_models']}")

    # è¯„åˆ†æ ‡å‡†éªŒè¯
    print()
    print("ğŸ“‹ è¯„åˆ†æ ‡å‡†éªŒè¯")
    print("-" * 40)

    valid_scores = 0
    total_scores = 0

    for result in all_results:
        if result.get('success', False) and 'scores' in result:
            for trait, score in result['scores'].items():
                total_scores += 1
                if score in [1, 3, 5]:
                    valid_scores += 1

    score_compliance = (valid_scores / total_scores * 100) if total_scores > 0 else 0
    print(f"âœ… 1-3-5è¯„åˆ†æ ‡å‡†ç¬¦åˆç‡: {score_compliance:.1f}%")
    print(f"ğŸ“Š æœ‰æ•ˆè¯„åˆ†: {valid_scores}/{total_scores}")

    # æœ€ç»ˆè¯„çº§
    print()
    print("ğŸ† ä¿¡åº¦è¯„çº§")
    print("-" * 40)

    if overall_consistency['consistency_score'] >= 80 and score_compliance >= 90:
        rating = "ä¼˜ç§€"
        recommendation = "âœ… æ¨èä½¿ç”¨5é¢˜åˆ†æ®µæ–¹æ¡ˆ"
    elif overall_consistency['consistency_score'] >= 60 and score_compliance >= 80:
        rating = "è‰¯å¥½"
        recommendation = "âš ï¸ å¯ä»¥ä½¿ç”¨ï¼Œéœ€ä¼˜åŒ–"
    else:
        rating = "éœ€è¦æ”¹è¿›"
        recommendation = "âŒ ä¸æ¨èï¼Œéœ€è¦ä¿®å¤"

    print(f"ğŸ“Š ä¿¡åº¦ç­‰çº§: {rating}")
    print(f"ğŸ’¡ å»ºè®®: {recommendation}")

    # ä¿å­˜éªŒè¯ç»“æœ
    validation_result = {
        "validation_date": datetime.now().isoformat(),
        "segment_size": 5,
        "test_questions": len(questions),
        "models_tested": [m["name"] for m in models],
        "overall_consistency": overall_consistency,
        "score_compliance": score_compliance,
        "rating": rating,
        "recommendation": recommendation,
        "all_results": all_results
    }

    with open("5segment_reliability_validation.json", "w", encoding="utf-8") as f:
        json.dump(validation_result, f, ensure_ascii=False, indent=2)

    print(f"ğŸ“„ éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: 5segment_reliability_validation.json")

if __name__ == "__main__":
    validate_5segment_reliability()