#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¯ä¿¡è¯„ä¼°åˆ†æå™¨
ä½¿ç”¨æ–°å®ç°çš„åˆ†æ®µè¯„åˆ†ç³»ç»Ÿå¤„ç†æ‰€æœ‰åŸå§‹æµ‹è¯„æŠ¥å‘Š
"""
import sys
import os
import json
import glob
from pathlib import Path
from datetime import datetime
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from segmented_scoring_evaluator import SegmentedScoringEvaluator
from report_manager import ReportManager


def run_batch_analysis(input_dir="results/readonly-original", output_dir="segmented_scoring_results", max_files=None, segment_size=2):
    """
    æ‰¹é‡è¿è¡Œåˆ†æ®µè¯„åˆ†åˆ†æ
    :param input_dir: è¾“å…¥ç›®å½•è·¯å¾„
    :param output_dir: è¾“å‡ºç›®å½•è·¯å¾„
    :param max_files: æœ€å¤§å¤„ç†æ–‡ä»¶æ•°ï¼ˆå¯é€‰ï¼‰
    :param segment_size: åˆ†æ®µå¤§å°ï¼ˆæ¯æ®µé¢˜æ•°ï¼Œé»˜è®¤2é¢˜ï¼‰
    """
    print("="*60)
    print("ğŸš€ æ‰¹é‡å¯ä¿¡è¯„ä¼°åˆ†æå™¨")
    print("="*60)
    print("ğŸ“‹ ç³»ç»Ÿé…ç½®:")
    print("   ğŸ¦™ Ollamaæœ¬åœ°æ¨¡å‹: ä½œä¸ºä¸»è¦è¯„ä¼°å™¨")
    print("   â˜ï¸  OpenRouter API: ç”±äºAPIå¯†é’¥å¤±æ•ˆå·²åœç”¨")
    print(f"   ğŸ“ è¯„ä¼°æ–¹æ³•: {segment_size}é¢˜åˆ†æ®µç‹¬ç«‹è¯„ä¼°")
    print("   ğŸ§  åŒ…å«äººæ ¼åˆ†æ: å¤§äº” + MBTI")
    print("   ğŸ“Š åŒ…å«ä¿¡åº¦éªŒè¯: Cronbach's Alpha å’Œ è¯„ä¼°è€…é—´ä¿¡åº¦")
    print("   ğŸ¤– ä¸»è¦æ¨¡å‹: qwen3:4b, gemma2:2b, llama3.2:3b")
    print()
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not os.path.exists(input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è·å–æ‰€æœ‰JSONæ–‡ä»¶
    json_pattern = os.path.join(input_dir, "*.json")
    all_files = glob.glob(json_pattern)
    
    if not all_files:
        print(f"âŒ åœ¨ {input_dir} ä¸­æœªæ‰¾åˆ°JSONæ–‡ä»¶")
        return
    
    # é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    files_to_process = all_files[:max_files] if max_files else all_files
    
    print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š æ‰¾åˆ° {len(all_files)} ä¸ªJSONæ–‡ä»¶ï¼Œå°†å¤„ç† {len(files_to_process)} ä¸ª")
    print(f"ğŸ“ åˆ†æ®µå¤§å°: {segment_size}é¢˜/æ®µ")
    print()
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨ï¼ˆé…ç½®ä¸ºä¼˜å…ˆä½¿ç”¨Ollamaæ¨¡å‹ï¼Œä½¿ç”¨æŒ‡å®šåˆ†æ®µå¤§å°ï¼‰
    evaluator = SegmentedScoringEvaluator(use_ollama_first=True, segment_size=segment_size)
    
    # åˆå§‹åŒ–æŠ¥å‘Šç®¡ç†å™¨
    report_manager = ReportManager()
    
    # ç»Ÿè®¡ä¿¡æ¯
    processed_count = 0
    success_count = 0
    failed_count = 0
    total_consistency = 0
    total_reliability = 0
    passed_reliability_count = 0
    completed_mbti_count = 0  # å®ŒæˆMBTIåˆ†æçš„æ–‡ä»¶æ•°é‡
    
    start_time = time.time()
    
    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for i, file_path in enumerate(files_to_process, 1):
        filename = os.path.basename(file_path)
        print(f"ğŸ“ˆ [{i}/{len(files_to_process)}] å¤„ç†: {filename}")
        
        try:
            # æ‰§è¡Œè¯„ä¼°ï¼ˆä½¿ç”¨æŒ‡å®šçš„åˆ†æ®µå¤§å°ï¼‰
            result = evaluator.evaluate_file_with_multiple_models(file_path, output_dir, segment_size=segment_size)
            
            if result['success']:
                processed_count += 1
                success_count += 1
                
                consistency_score = result.get('consistency_score', 0)
                reliability_score = result.get('reliability_score', 0)
                reliability_passed = result.get('reliability_passed', False)
                
                total_consistency += consistency_score
                total_reliability += reliability_score
                
                if reliability_passed:
                    passed_reliability_count += 1
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«äººæ ¼åˆ†æç»“æœ
                if 'personality_analysis' in result:
                    mbti_type = result['personality_analysis']['mbti_analysis']['mbti_type']
                    print(f"   ğŸ§  MBTIç±»å‹: {mbti_type}")
                    completed_mbti_count += 1
                
                print(f"   âœ… ä¸€è‡´æ€§: {consistency_score:.2f}%")
                print(f"   âœ… ä¿¡åº¦: {reliability_score:.2f}%")
                print(f"   âœ… ä¿¡åº¦éªŒè¯: {'é€šè¿‡' if reliability_passed else 'æœªé€šè¿‡'}")
                print(f"   ğŸ’¾ ç»“æœå·²ä¿å­˜: {result['output_path']}")
                
                # æ ‡è®°æŠ¥å‘Šä¸ºå·²å®Œæˆå¹¶ç§»åŠ¨æ–‡ä»¶
                completion_result = report_manager.mark_report_complete(file_path, result)
                if completion_result['success']:
                    print(f"   ğŸ“ åŸå§‹æ–‡ä»¶å·²ç§»è‡³: {completion_result['original_moved_to']}")
                else:
                    print(f"   âš ï¸  æ–‡ä»¶ç§»åŠ¨å¤±è´¥: {completion_result.get('error', 'Unknown error')}")
                    
            else:
                processed_count += 1
                failed_count += 1
                error_msg = result.get('error', 'Unknown error')
                print(f"   âŒ å¤„ç†å¤±è´¥: {error_msg}")
                
        except Exception as e:
            processed_count += 1
            failed_count += 1
            print(f"   âŒ å¼‚å¸¸: {str(e)}")
            import traceback
            traceback.print_exc()
            continue
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # æ‰“å°æ±‡æ€»ç»Ÿè®¡
    print(f"\n" + "="*60)
    print(f"ğŸ“Š æ‰¹é‡åˆ†æå®ŒæˆæŠ¥å‘Š")
    print(f"â° å¼€å§‹æ—¶é—´: {datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â° ç»“æŸæ—¶é—´: {datetime.fromtimestamp(end_time).strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.2f} ç§’")
    print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {len(files_to_process)}")
    print(f"âœ… æˆåŠŸå¤„ç†: {success_count}")
    print(f"âŒ å¤„ç†å¤±è´¥: {failed_count}")
    print(f"ğŸ¯ æˆåŠŸç‡: {(success_count/len(files_to_process))*100:.1f}%" if len(files_to_process) > 0 else "N/A")
    print(f"ğŸ§  å®Œæˆäººæ ¼åˆ†æ: {completed_mbti_count}")
    print(f"ğŸ“ åˆ†æ®µå¤§å°: {segment_size}é¢˜/æ®µ")
    
    if success_count > 0:
        avg_consistency = total_consistency / success_count
        avg_reliability = total_reliability / success_count
        
        print(f"ğŸ“ˆ å¹³å‡ä¸€è‡´æ€§: {avg_consistency:.2f}%")
        print(f"âœ… ä¿¡åº¦éªŒè¯é€šè¿‡ç‡: {passed_reliability_count}/{success_count} ({(passed_reliability_count/success_count)*100:.1f}%)")
        print(f"ğŸ“ˆ å¹³å‡ä¿¡åº¦: {avg_reliability:.2f}%")
    
    # æ‰“å°å®ŒæˆæŠ¥å‘Šç»Ÿè®¡
    completion_stats = report_manager.check_completed_reports()
    print(f"ğŸ“‹ å®Œæˆæ–‡ä»¶ç»Ÿè®¡:")
    print(f"   åŸå§‹æ–‡ä»¶ç§»åŠ¨æ•°: {completion_stats['completed_original_count']}")
    print(f"   è¯„ä¼°ç»“æœæ–‡ä»¶æ•°: {completion_stats['completed_evaluated_count']}")
    
    print(f"ğŸ’¾ ç»“æœä¿å­˜åœ¨: {output_dir}")
    print("="*60)


def main():
    """
    ä¸»å‡½æ•°
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='æ‰¹é‡å¯ä¿¡è¯„ä¼°åˆ†æå™¨')
    parser.add_argument('--input_dir', type=str, default='results/readonly-original',
                        help='è¾“å…¥ç›®å½•è·¯å¾„ (é»˜è®¤: results/readonly-original)')
    parser.add_argument('--output_dir', type=str, default='segmented_scoring_results',
                        help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: segmented_scoring_results)')
    parser.add_argument('--max_files', type=int, 
                        help='æœ€å¤§å¤„ç†æ–‡ä»¶æ•° (å¯é€‰)')
    parser.add_argument('--segment_size', type=int, default=2,
                        help='åˆ†æ®µå¤§å°ï¼ˆæ¯æ®µé¢˜æ•°ï¼Œé»˜è®¤2é¢˜ï¼‰')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œæ‰¹é‡åˆ†æ
    run_batch_analysis(args.input_dir, args.output_dir, args.max_files, args.segment_size)


if __name__ == "__main__":
    main()