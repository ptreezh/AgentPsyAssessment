# Ollama æœ¬åœ°è¯„ä¼°å™¨ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

ç°åœ¨æ”¯æŒä½¿ç”¨æœ¬åœ°Ollamaå¤§æ¨¡å‹ä½œä¸ºè¯„ä¼°å™¨ï¼Œæ— éœ€APIå¯†é’¥ï¼Œå®Œå…¨åœ¨æœ¬åœ°è¿è¡Œã€‚

## æ”¯æŒçš„æ¨¡å‹

### é»˜è®¤é…ç½®çš„æ¨¡å‹
1. **ollama_llama3** - `llama3:latest`
   - Meta Llama 3 æ¨¡å‹
   - é€šç”¨æ€§å¼ºï¼Œé€‚åˆå¤§å¤šæ•°è¯„ä¼°ä»»åŠ¡

2. **ollama_qwen3** - `qwen3:8b`
   - é˜¿é‡Œäº‘é€šä¹‰åƒé—®3
   - ä¸­æ–‡ä¼˜åŒ–ï¼Œ8Bå‚æ•°ç‰ˆæœ¬

3. **ollama_mistral** - `mistral-nemo:latest`
   - Mistral NeMo æ¨¡å‹
   - é«˜æ€§èƒ½æ¨ç†æ¨¡å‹

## å®‰è£…å’Œè®¾ç½®

### 1. å®‰è£…Ollama

**Windows**:
```bash
# ä¸‹è½½å¹¶å®‰è£…Ollama
# è®¿é—® https://ollama.ai/download ä¸‹è½½Windowsç‰ˆæœ¬
```

**Linux**:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**macOS**:
```bash
# ä½¿ç”¨Homebrew
brew install ollama
```

### 2. å¯åŠ¨OllamaæœåŠ¡
```bash
ollama serve
```

### 3. ä¸‹è½½æ¨¡å‹
```bash
# ä¸‹è½½Llama3
ollama pull llama3:latest

# ä¸‹è½½Qwen3
ollama pull qwen3:8b

# ä¸‹è½½Mistral NeMo
ollama pull mistral-nemo:latest
```

### 4. éªŒè¯å®‰è£…
```bash
# æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
python shared_analysis/ollama_evaluator.py

# æµ‹è¯•é…ç½®
python -c "from shared_analysis.ollama_evaluator import test_ollama_setup; test_ollama_setup()"
```

## é…ç½®æ–‡ä»¶

é…ç½®æ–‡ä»¶ä½ç½®: `config/ollama_config.json`

```json
{
  "ollama": {
    "base_url": "http://localhost:11434",
    "timeout": 120,
    "models": {
      "llama3": {
        "name": "llama3:latest",
        "temperature": 0.1,
        "max_tokens": 1024
      },
      "qwen3": {
        "name": "qwen3:8b",
        "temperature": 0.1,
        "max_tokens": 1024
      },
      "mistral": {
        "name": "mistral-nemo:latest",
        "temperature": 0.1,
        "max_tokens": 1024
      }
    }
  },
  "evaluators": {
    "ollama_llama3": {
      "provider": "ollama",
      "model": "llama3",
      "description": "Llama3 æœ¬åœ°è¯„ä¼°å™¨"
    },
    "ollama_qwen3": {
      "provider": "ollama",
      "model": "qwen3",
      "description": "Qwen3 æœ¬åœ°è¯„ä¼°å™¨"
    },
    "ollama_mistral": {
      "provider": "ollama",
      "model": "mistral",
      "description": "Mistral NeMo æœ¬åœ°è¯„ä¼°å™¨"
    }
  }
}
```

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨
```bash
# ä½¿ç”¨é»˜è®¤çš„Llama3è¯„ä¼°å™¨
python shared_analysis/analyze_results.py test_file.json

# ä½¿ç”¨Qwen3è¯„ä¼°å™¨
python shared_analysis/analyze_results.py test_file.json --evaluators ollama_qwen3

# ä½¿ç”¨Mistralè¯„ä¼°å™¨
python shared_analysis/analyze_results.py test_file.json --evaluators ollama_mistral
```

### å¤šä¸ªè¯„ä¼°å™¨å¯¹æ¯”
```bash
# ä½¿ç”¨æ‰€æœ‰æœ¬åœ°è¯„ä¼°å™¨
python shared_analysis/analyze_results.py test_file.json --evaluators ollama_llama3 ollama_qwen3 ollama_mistral

# æ··åˆä½¿ç”¨æœ¬åœ°å’Œäº‘ç«¯è¯„ä¼°å™¨
python shared_analysis/analyze_results.py test_file.json --evaluators ollama_llama3 gpt
```

### æ‰¹é‡å¤„ç†
```bash
# å¤„ç†å¤šä¸ªæ–‡ä»¶
for file in results/*.json; do
    python shared_analysis/analyze_results.py "$file" --evaluators ollama_llama3
done
```

## æ·»åŠ æ–°çš„Ollamaæ¨¡å‹

### 1. ä¸‹è½½æ–°æ¨¡å‹
```bash
# ä¸‹è½½æ–°æ¨¡å‹
ollama pull new_model:tag
```

### 2. æ›´æ–°é…ç½®æ–‡ä»¶
åœ¨ `config/ollama_config.json` ä¸­æ·»åŠ ï¼š

```json
{
  "ollama": {
    "models": {
      "new_model": {
        "name": "new_model:tag",
        "temperature": 0.1,
        "max_tokens": 1024,
        "description": "æ–°æ¨¡å‹æè¿°"
      }
    }
  },
  "evaluators": {
    "ollama_new_model": {
      "provider": "ollama",
      "model": "new_model",
      "description": "æ–°æ¨¡å‹è¯„ä¼°å™¨"
    }
  }
}
```

### 3. æ›´æ–°EVALUATOR_PROVIDERSåˆ—è¡¨
åœ¨ `shared_analysis/analyze_results.py` ä¸­ï¼š
```python
EVALUATOR_PROVIDERS = ["gpt", "claude", "gemini", "deepseek", "glm", "qwen", "ollama_llama3", "ollama_qwen3", "ollama_mistral", "ollama_new_model"]
```

## æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹é€‰æ‹©
- **å¿«é€Ÿè¯„ä¼°**: ä½¿ç”¨ `qwen3:8b` (8Bå‚æ•°)
- **å¹³è¡¡è¯„ä¼°**: ä½¿ç”¨ `llama3:latest` (8Bå‚æ•°)
- **é«˜è´¨é‡è¯„ä¼°**: ä½¿ç”¨ `mistral-nemo:latest` (12Bå‚æ•°)

### 2. å‚æ•°è°ƒæ•´
åœ¨é…ç½®æ–‡ä»¶ä¸­è°ƒæ•´ï¼š
```json
{
  "temperature": 0.1,    # é™ä½æ¸©åº¦æé«˜ä¸€è‡´æ€§
  "max_tokens": 512,    # å‡å°‘tokenæ•°é‡æé«˜é€Ÿåº¦
  "timeout": 60          # å‡å°‘è¶…æ—¶æ—¶é—´
}
```

### 3. å¹¶è¡Œå¤„ç†
```bash
# ä½¿ç”¨å¤šä¸ªè¯„ä¼°å™¨å¹¶è¡Œå¤„ç†
python shared_analysis/analyze_results.py test_file.json --evaluators ollama_llama3 ollama_qwen3
```

## æ•…éšœæ’é™¤

### 1. è¿æ¥é—®é¢˜
```bash
# æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
ollama ps

# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
curl http://localhost:11434/api/tags
```

### 2. æ¨¡å‹é—®é¢˜
```bash
# åˆ—å‡ºå·²ä¸‹è½½çš„æ¨¡å‹
ollama list

# é‡æ–°ä¸‹è½½æ¨¡å‹
ollama pull llama3:latest
```

### 3. é…ç½®é—®é¢˜
```bash
# éªŒè¯é…ç½®æ–‡ä»¶
python -c "import json; print(json.load(open('config/ollama_config.json')))"

# æµ‹è¯•å•ä¸ªè¯„ä¼°å™¨
python -c "
from shared_analysis.ollama_evaluator import create_ollama_evaluator
evaluator = create_ollama_evaluator('ollama_llama3')
print('Evaluator created:', evaluator is not None)
"
```

## ä¼˜åŠ¿

### âœ… æœ¬åœ°è¯„ä¼°å™¨ä¼˜åŠ¿
1. **å®Œå…¨ç¦»çº¿**: æ— éœ€ç½‘ç»œè¿æ¥
2. **æ— æˆæœ¬**: æ— APIè´¹ç”¨
3. **æ•°æ®å®‰å…¨**: æ•°æ®ä¸ç¦»å¼€æœ¬åœ°
4. **è‡ªå®šä¹‰**: å¯ä½¿ç”¨ä»»ä½•æœ¬åœ°æ¨¡å‹
5. **å¿«é€Ÿå“åº”**: æœ¬åœ°å¤„ç†æ— å»¶è¿Ÿ
6. **å¯æ§æ€§**: å®Œå…¨æ§åˆ¶è¯„ä¼°è¿‡ç¨‹

### ğŸ”„ ä¸äº‘ç«¯è¯„ä¼°å™¨å¯¹æ¯”
| ç‰¹æ€§ | æœ¬åœ°Ollama | äº‘ç«¯API |
|------|-----------|--------|
| æˆæœ¬ | å…è´¹ | æŒ‰ä½¿ç”¨ä»˜è´¹ |
| é€Ÿåº¦ | é€‚ä¸­ | ä¾èµ–ç½‘ç»œ |
| è´¨é‡ | ä¾èµ–æ¨¡å‹ | é€šå¸¸è¾ƒé«˜ |
| éšç§ | å®Œå…¨æœ¬åœ° | æ•°æ®ä¸Šä¼  |
| å¯ç”¨æ€§ | ä¾èµ–æœ¬åœ° | é€šå¸¸ç¨³å®š |

## æœ€ä½³å®è·µ

1. **é¦–æ¬¡ä½¿ç”¨**: å…ˆç”¨ `ollama_llama3` æµ‹è¯•
2. **æ‰¹é‡å¤„ç†**: ä½¿ç”¨å¤šä¸ªè¯„ä¼°å™¨æé«˜å‡†ç¡®æ€§
3. **ä¸­æ–‡å†…å®¹**: ä¼˜å…ˆä½¿ç”¨ `ollama_qwen3`
4. **å¤æ‚åˆ†æ**: ä½¿ç”¨ `ollama_mistral`
5. **å®šæœŸæ›´æ–°**: ä¿æŒæ¨¡å‹ä¸ºæœ€æ–°ç‰ˆæœ¬

ç°åœ¨ä½ å¯ä»¥å®Œå…¨åœ¨æœ¬åœ°è¿›è¡ŒAIè¯„ä¼°ï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨APIæœåŠ¡ï¼