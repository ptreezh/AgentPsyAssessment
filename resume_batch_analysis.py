#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–­ç‚¹ç»§ç»­æ‰¹é‡åˆ†æ - ä½¿ç”¨ä¿®å¤åçš„ç³»ç»Ÿ
"""

import sys
import os
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['DASHSCOPE_API_KEY'] = 'sk-3f16ac9d87e34ca88bf3925c3651624f'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def resume_batch_analysis():
    print("ğŸš€ æ–­ç‚¹ç»§ç»­æ‰¹é‡åˆ†æï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰...")

    try:
        from batch_four_model_analysis import BatchFourModelAnalyzer

        # åˆ›å»ºåˆ†æå™¨
        analyzer = BatchFourModelAnalyzer(
            models=["qwen-max", "deepseek-v3.2-exp", "Moonshot-Kimi-K2-Instruct"]
        )

        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {', '.join(analyzer.models)}")
        print(f"ğŸ”‘ APIå¯†é’¥å·²è®¾ç½®")

        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        results_dir = Path("results/results")
        if not results_dir.exists():
            print("âŒ resultsç›®å½•ä¸å­˜åœ¨")
            return

        json_files = list(results_dir.glob("*.json"))
        if not json_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°JSONæ–‡ä»¶")
            return

        print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} ä¸ªæ–‡ä»¶")

        # æ£€æŸ¥æ–­ç‚¹ä¿¡æ¯
        if analyzer.load_progress():
            print(f"ğŸ“‚ å‘ç°æ–­ç‚¹ç»§ç»­ä¿¡æ¯:")
            print(f"   å·²å®Œæˆ: {len(analyzer.completed_files)} ä¸ªæ–‡ä»¶")
            print(f"   å¤±è´¥: {len(analyzer.failed_files)} ä¸ªæ–‡ä»¶")
            if analyzer.start_time:
                start_dt = analyzer.start_time.replace('T', ' ')[:19]
                print(f"   å¼€å§‹æ—¶é—´: {start_dt}")
        else:
            print("ğŸ“‚ æœªå‘ç°æ–­ç‚¹ä¿¡æ¯ï¼Œä»å¤´å¼€å§‹")

        # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶
        remaining_files = [f for f in json_files if str(f) not in analyzer.completed_files and str(f) not in analyzer.failed_files]

        print(f"ğŸ“Š å‰©ä½™å¾…å¤„ç†æ–‡ä»¶: {len(remaining_files)} ä¸ª")

        if not remaining_files:
            print("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤„ç†å®Œæˆ")
            return

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path("four_model_results")
        output_dir.mkdir(exist_ok=True)

        # æ‰§è¡Œæ‰¹é‡åˆ†æ
        def progress_callback(completed, total, result):
            success_rate = sum(1 for r in analyzer.results if r.get('success', False)) / len(analyzer.results) * 100 if analyzer.results else 0
            confidences = [r.get('confidence_analysis', {}).get('overall_confidence', 0) for r in analyzer.results if r.get('success', False)]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0
            completed_in_batch = len(analyzer.completed_files) + len(analyzer.failed_files)
            print(f"ğŸ“ˆ æ€»è¿›åº¦: {completed_in_batch}/{total} ({completed_in_batch/total*100:.1f}%) - æˆåŠŸç‡: {success_rate:.1f}% - å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.1f}%")

        print(f"\nğŸ¯ å¼€å§‹æ–­ç‚¹ç»§ç»­åˆ†æ...")
        print(f"â±ï¸  ä½¿ç”¨2ç§’å»¶è¿Ÿæé«˜å¤„ç†é€Ÿåº¦")

        results = analyzer.analyze_batch(
            remaining_files,
            output_dir,
            progress_callback=progress_callback,
            delay_between_files=2
        )

        # ç”Ÿæˆæœ€ç»ˆæ±‡æ€»
        successful = sum(1 for r in results if r['success'])
        print(f"\nğŸ“Š æ‰¹é‡åˆ†æå®Œæˆ:")
        print(f"âœ… æˆåŠŸ: {successful}/{len(results)}")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")

        if successful > 0:
            # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
            analyzer.generate_summary_report(results, output_dir)
            print(f"ğŸ“‹ æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {output_dir}/four_model_batch_summary.md")

            # ç”Ÿæˆå½“å‰è¿›åº¦æ±‡æ€»
            os.system("python generate_current_summary.py")

    except Exception as e:
        print(f"ğŸ’¥ æ‰¹é‡åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    resume_batch_analysis()