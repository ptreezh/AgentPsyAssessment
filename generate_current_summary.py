#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºç°æœ‰ç»“æœç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
"""

import json
from pathlib import Path
from datetime import datetime

def generate_current_summary():
    print("ğŸ“Š åŸºäºç°æœ‰ç»“æœç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")

    # ç»“æœç›®å½•
    results_dir = Path("four_model_results/multi_model_results")
    if not results_dir.exists():
        print("âŒ ç»“æœç›®å½•ä¸å­˜åœ¨")
        return

    # æ”¶é›†æ‰€æœ‰æˆåŠŸçš„ç»“æœ
    successful_results = []
    failed_results = []

    # æŸ¥æ‰¾æ‰€æœ‰summaryæ–‡ä»¶
    summary_files = list(results_dir.glob("*/*summary.json"))
    print(f"ğŸ“ æ‰¾åˆ° {len(summary_files)} ä¸ªç»“æœæ–‡ä»¶")

    for summary_file in summary_files:
        try:
            with open(summary_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸï¼ˆåŸºäºåˆ†æè´¨é‡ï¼‰
            analysis_quality = data.get('analysis_quality', {})
            success_rate = analysis_quality.get('success_rate', 0)

            if success_rate > 0:  # æœ‰æˆåŠŸçš„åˆ†æ®µ
                big5_scores = data.get('big5_final_scores', {})
                mbti_type = data.get('mbti_type', 'N/A')
                mbti_confidence = data.get('mbti_confidence', 0)
                model_used = data.get('analysis_info', {}).get('model_used', 'Unknown')
                filename = data.get('analysis_info', {}).get('filename', 'Unknown')

                # æå–Big5è¯„åˆ†
                big5_simple = {}
                for trait, scores in big5_scores.items():
                    big5_simple[trait] = scores.get('final_score', 3)

                successful_results.append({
                    'filename': filename,
                    'model': model_used,
                    'big5_scores': big5_simple,
                    'mbti_type': mbti_type,
                    'mbti_confidence': mbti_confidence,
                    'success_rate': success_rate,
                    'analysis_time': data.get('analysis_info', {}).get('analysis_timestamp', '')
                })
            else:
                failed_results.append({
                    'filename': data.get('analysis_info', {}).get('filename', 'Unknown'),
                    'model': data.get('analysis_info', {}).get('model_used', 'Unknown'),
                    'error': 'åˆ†æå¤±è´¥ (æˆåŠŸç‡0%)'
                })

        except Exception as e:
            failed_results.append({
                'filename': summary_file.name,
                'error': f'è¯»å–å¤±è´¥: {e}'
            })

    print(f"âœ… æˆåŠŸç»“æœ: {len(successful_results)}")
    print(f"âŒ å¤±è´¥ç»“æœ: {len(failed_results)}")

    if not successful_results:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„ç»“æœå¯ä»¥åˆ†æ")
        return

    # æŒ‰æ–‡ä»¶åˆ†ç»„ç»“æœ
    file_results = {}
    for result in successful_results:
        filename = result['filename']
        if filename not in file_results:
            file_results[filename] = []
        file_results[filename].append(result)

    # ç»Ÿè®¡åˆ†æ
    print(f"\nğŸ“ˆ åˆ†æç»Ÿè®¡:")
    print(f"   æˆåŠŸåˆ†æçš„æ–‡ä»¶æ•°: {len(file_results)}")
    print(f"   ä½¿ç”¨çš„æ¨¡å‹: {list(set(r['model'] for r in successful_results))}")

    # Big5è¯„åˆ†åˆ†å¸ƒ
    big5_stats = {}
    mbti_stats = {}

    for filename, results in file_results.items():
        if len(results) >= 2:  # åªç»Ÿè®¡æœ‰å¤šæ¨¡å‹ç»“æœçš„æ–‡ä»¶
            # ç»Ÿè®¡MBTI
            mbti_types = [r['mbti_type'] for r in results]
            for mbti in mbti_types:
                mbti_stats[mbti] = mbti_stats.get(mbti, 0) + 1

            # ç»Ÿè®¡Big5 (ä½¿ç”¨ç¬¬ä¸€ä¸ªæˆåŠŸæ¨¡å‹çš„ç»“æœ)
            first_result = results[0]
            for trait, score in first_result['big5_scores'].items():
                if trait not in big5_stats:
                    big5_stats[trait] = {1: 0, 3: 0, 5: 0}
                big5_stats[trait][score] += 1

    print(f"\nğŸ¯ MBTIç±»å‹åˆ†å¸ƒ:")
    for mbti, count in sorted(mbti_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = count / sum(mbti_stats.values()) * 100
        print(f"   {mbti}: {count} ({percentage:.1f}%)")

    print(f"\nğŸ“Š Big5è¯„åˆ†åˆ†å¸ƒ:")
    trait_names = {
        'openness_to_experience': 'å¼€æ”¾æ€§ (O)',
        'conscientiousness': 'å°½è´£æ€§ (C)',
        'extraversion': 'å¤–å‘æ€§ (E)',
        'agreeableness': 'å®œäººæ€§ (A)',
        'neuroticism': 'ç¥ç»è´¨ (N)'
    }

    for trait, scores in big5_stats.items():
        trait_name = trait_names.get(trait, trait)
        total = sum(scores.values())
        print(f"\n   {trait_name}:")
        for score in [1, 3, 5]:
            if scores[score] > 0:
                percentage = scores[score] / total * 100
                print(f"     {score}åˆ†: {scores[score]} ({percentage:.1f}%)")

    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    summary_report = {
        'summary': {
            'total_files_analyzed': len(file_results),
            'successful_analyses': len(successful_results),
            'failed_analyses': len(failed_results),
            'models_used': list(set(r['model'] for r in successful_results)),
            'analysis_timestamp': datetime.now().isoformat(),
            'data_source': 'four_model_results/multi_model_results'
        },
        'statistics': {
            'mbti_distribution': mbti_stats,
            'big5_distribution': big5_stats
        },
        'detailed_results': file_results
    }

    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    output_file = Path("current_analysis_summary.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summary_report, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {output_file}")

    # ç”ŸæˆMarkdownæŠ¥å‘Š
    generate_markdown_report(summary_report)

def generate_markdown_report(summary_data):
    """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
    summary = summary_data['summary']
    mbti_stats = summary_data['statistics']['mbti_distribution']
    big5_stats = summary_data['statistics']['big5_distribution']
    detailed_results = summary_data['detailed_results']

    trait_names = {
        'openness_to_experience': 'å¼€æ”¾æ€§ (O)',
        'conscientiousness': 'å°½è´£æ€§ (C)',
        'extraversion': 'å¤–å‘æ€§ (E)',
        'agreeableness': 'å®œäººæ€§ (A)',
        'neuroticism': 'ç¥ç»è´¨ (N)'
    }

    md_content = f"""# å¿ƒç†è¯„ä¼°åˆ†ææ±‡æ€»æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯

- **åˆ†ææ—¶é—´:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **åˆ†ææ–‡ä»¶æ•°:** {summary['total_files_analyzed']}
- **æˆåŠŸåˆ†ææ•°:** {summary['successful_analyses']}
- **å¤±è´¥åˆ†ææ•°:** {summary['failed_analyses']}
- **ä½¿ç”¨æ¨¡å‹:** {', '.join(summary['models_used'])}
- **æ•°æ®æ¥æº:** {summary['data_source']}

## MBTIç±»å‹åˆ†å¸ƒ

"""

    total_mbti = sum(mbti_stats.values())
    for mbti, count in sorted(mbti_stats.items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_mbti * 100 if total_mbti > 0 else 0
        md_content += f"- **{mbti}:** {count} ({percentage:.1f}%)\n"

    md_content += "\n## Big5è¯„åˆ†åˆ†å¸ƒ\n\n"

    for trait, scores in big5_stats.items():
        trait_name = trait_names.get(trait, trait)
        total = sum(scores.values())
        md_content += f"### {trait_name}\n"
        for score in [1, 3, 5]:
            if scores.get(score, 0) > 0:
                percentage = scores[score] / total * 100 if total > 0 else 0
                md_content += f"- **{score}åˆ†:** {scores[score]} ({percentage:.1f}%)\n"
        md_content += "\n"

    md_content += "## è¯¦ç»†åˆ†æç»“æœ\n\n"

    for filename, results in detailed_results.items():
        md_content += f"### {filename}\n\n"

        if len(results) >= 2:
            # å¤šæ¨¡å‹ç»“æœå¯¹æ¯”
            md_content += "**å¤šæ¨¡å‹å¯¹æ¯”:**\n\n"
            md_content += "| æ¨¡å‹ | MBTI | ç½®ä¿¡åº¦ | Big5 (O,C,E,A,N) | æˆåŠŸç‡ |\n"
            md_content += "|------|------|--------|------------------|--------|\n"

            for result in results:
                big5_str = ",".join(str(result['big5_scores'][t]) for t in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'])
                md_content += f"| {result['model']} | {result['mbti_type']} | {result['mbti_confidence']:.1f}% | {big5_str} | {result['success_rate']:.1f}% |\n"

            # è®¡ç®—ä¸€è‡´æ€§
            mbti_types = [r['mbti_type'] for r in results]
            mbti_agreement = len(set(mbti_types)) == 1
            md_content += f"\n**MBTIä¸€è‡´æ€§:** {'âœ… ä¸€è‡´' if mbti_agreement else 'âŒ ä¸ä¸€è‡´'}\n"
        else:
            # å•æ¨¡å‹ç»“æœ
            result = results[0]
            big5_str = ",".join(str(result['big5_scores'][t]) for t in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism'])
            md_content += f"- **æ¨¡å‹:** {result['model']}\n"
            md_content += f"- **MBTI:** {result['mbti_type']} (ç½®ä¿¡åº¦: {result['mbti_confidence']:.1f}%)\n"
            md_content += f"- **Big5:** {big5_str}\n"
            md_content += f"- **æˆåŠŸç‡:** {result['success_rate']:.1f}%\n"

        md_content += "\n"

    # ä¿å­˜MarkdownæŠ¥å‘Š
    md_file = Path("current_analysis_report.md")
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(md_content)

    print(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜: {md_file}")

if __name__ == "__main__":
    generate_current_summary()