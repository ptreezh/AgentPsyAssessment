#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€é¢˜å¯¹æ¯”åˆ†æ - 50é“é¢˜çš„è¯¦ç»†è¯„åˆ†å·®å¼‚åˆ†æ
å¯¹æ¯”2é¢˜åˆ†æ®µå’Œ5é¢˜åˆ†æ®µå¯¹æ¯é“é¢˜çš„è¯„åˆ†å·®å¼‚
"""

import sys
import os
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
import statistics

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['DASHSCOPE_API_KEY'] = 'sk-ded837735b3c44599a9bc138da561c27'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class QuestionByQuestionComparator:
    def __init__(self, model: str = "qwen-long"):
        self.model = model
        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

    def load_2segment_per_question_scores(self, analysis_file: str) -> Dict:
        """åŠ è½½2é¢˜åˆ†æ®µçš„æ¯é¢˜è¯„åˆ†"""
        print(f"ğŸ“‚ åŠ è½½2é¢˜åˆ†æ®µé€é¢˜è¯„åˆ†: {analysis_file}")

        try:
            with open(analysis_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–æ¯ä¸ªåˆ†æ®µçš„è¯¦ç»†è¯„åˆ†
            segment_scores = []
            if 'segment_analyses' in data:
                for segment in data['segment_analyses']:
                    if 'big_five_scores' in segment:
                        segment_scores.append(segment['big_five_scores'])

            print(f"  âœ… åŠ è½½äº† {len(segment_scores)} ä¸ªåˆ†æ®µçš„è¯„åˆ†")
            return {
                'success': True,
                'segment_scores': segment_scores,
                'total_segments': len(segment_scores),
                'questions_per_segment': 2
            }

        except Exception as e:
            print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def load_test_data_with_mapping(self, data_file: str) -> Tuple[List[Dict], Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®å¹¶å»ºç«‹é—®é¢˜-åˆ†æ®µæ˜ å°„å…³ç³»"""
        print(f"ğŸ“‹ åŠ è½½æµ‹è¯„æ•°æ®: {data_file}")

        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            questions = []
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for i, item in enumerate(data['assessment_results']):
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', '')
                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']

                            if question_text and answer_text:
                                questions.append({
                                    'index': i + 1,
                                    'question': question_text,
                                    'answer': answer_text,
                                    'original_segment_2': (i // 2) + 1,  # 2é¢˜åˆ†æ®µç´¢å¼•
                                    'original_segment_5': (i // 5) + 1   # 5é¢˜åˆ†æ®µç´¢å¼•
                                })

            print(f"  ğŸ“Š æˆåŠŸæå– {len(questions)} ä¸ªé—®é¢˜")

            # å»ºç«‹åˆ†æ®µæ˜ å°„
            segment_2_mapping = {}
            segment_5_mapping = {}

            for q in questions:
                seg_2 = q['original_segment_2']
                seg_5 = q['original_segment_5']

                if seg_2 not in segment_2_mapping:
                    segment_2_mapping[seg_2] = []
                if seg_5 not in segment_5_mapping:
                    segment_5_mapping[seg_5] = []

                segment_2_mapping[seg_2].append(q)
                segment_5_mapping[seg_5].append(q)

            return questions, {
                'segment_2_mapping': segment_2_mapping,
                'segment_5_mapping': segment_5_mapping
            }

        except Exception as e:
            print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return [], {}

    def analyze_5segment_per_question(self, questions: List[Dict], mapping: Dict) -> Dict:
        """åˆ†æ5é¢˜åˆ†æ®µçš„æ¯é¢˜è¯„åˆ†"""
        print(f"\nğŸ” å¼€å§‹5é¢˜åˆ†æ®µé€é¢˜åˆ†æ")

        segment_5_mapping = mapping['segment_5_mapping']
        all_question_scores = []

        import openai
        client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        # åˆ†ææ¯ä¸ª5é¢˜åˆ†æ®µ
        for segment_num, segment_questions in segment_5_mapping.items():
            print(f"  ğŸ“ åˆ†æ5é¢˜åˆ†æ®µ {segment_num}/{len(segment_5_mapping)} (é¢˜ {segment_questions[0]['index']}-{segment_questions[-1]['index']})")

            # ä¸ºåˆ†æ®µä¸­çš„æ¯ä¸ªé¢˜å•ç‹¬åˆ†æ
            for i, question in enumerate(segment_questions):
                print(f"    ğŸ” åˆ†æé¢˜ {question['index']}...")

                prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚åˆ†æä»¥ä¸‹å•ä¸ªé—®é¢˜çš„å›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ã€‚

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- 1åˆ†ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- 3åˆ†ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- 5åˆ†ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼**

é—®é¢˜ {question['index']}:
{question['question']}

å›ç­”:
{question['answer']}

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "success": true,
  "scores": {{
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  }}
}}
"""

                try:
                    response = client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ã€‚"},
                            {"role": "user", "content": prompt}
                        ],
                        max_tokens=1000,
                        temperature=0.1
                    )

                    content = response.choices[0].message.content

                    # è§£æJSON
                    try:
                        import re
                        json_match = re.search(r'\{.*\}', content, re.DOTALL)
                        if json_match:
                            json_str = json_match.group(0)
                            result = json.loads(json_str)
                        else:
                            result = json.loads(content)
                    except json.JSONDecodeError:
                        print(f"      âŒ JSONè§£æå¤±è´¥")
                        continue

                    # éªŒè¯è¯„åˆ†æ ‡å‡†
                    if 'scores' in result and result['scores']:
                        invalid_scores = []
                        for trait, score in result['scores'].items():
                            if score not in [1, 3, 5]:
                                invalid_scores.append(f"{trait}:{score}")
                                # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                                if score < 2:
                                    result['scores'][trait] = 1
                                elif score > 4:
                                    result['scores'][trait] = 5
                                else:
                                    result['scores'][trait] = 3

                        if invalid_scores:
                            print(f"      âš ï¸ ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")

                        question_score = {
                            'question_index': question['index'],
                            'segment_5': segment_num,
                            'scores': result['scores'],
                            'segment_position': i + 1  # åœ¨åˆ†æ®µä¸­çš„ä½ç½®
                        }
                        all_question_scores.append(question_score)
                        print(f"      âœ… é¢˜{question['index']}: {result['scores']}")
                    else:
                        print(f"      âŒ æ— æœ‰æ•ˆè¯„åˆ†")

                except Exception as e:
                    print(f"      âŒ åˆ†æå¤±è´¥: {e}")

                time.sleep(1)  # APIé™åˆ¶

        print(f"  ğŸ“Š å®Œæˆ {len(all_question_scores)} é“é¢˜çš„5é¢˜åˆ†æ®µåˆ†æ")
        return {
            'success': True,
            'question_scores': all_question_scores,
            'total_questions': len(all_question_scores)
        }

    def reconstruct_2segment_per_question_scores(self, questions: List[Dict], segment_2_scores: List[Dict]) -> Dict:
        """é‡æ„2é¢˜åˆ†æ®µçš„æ¯é¢˜è¯„åˆ†"""
        print(f"\nğŸ”§ é‡æ„2é¢˜åˆ†æ®µé€é¢˜è¯„åˆ†")

        question_scores_2segment = []

        for i, question in enumerate(questions):
            segment_2_index = question['original_segment_2'] - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•

            if segment_2_index < len(segment_2_scores):
                segment_score = segment_2_scores[segment_2_index]
                question_score = {
                    'question_index': question['index'],
                    'segment_2': question['original_segment_2'],
                    'scores': segment_score.copy(),  # ä½¿ç”¨å‰¯æœ¬é¿å…ä¿®æ”¹åŸæ•°æ®
                    'segment_position': (i % 2) + 1  # åœ¨åˆ†æ®µä¸­çš„ä½ç½®
                }
                question_scores_2segment.append(question_score)

        print(f"  ğŸ“Š é‡æ„äº† {len(question_scores_2segment)} é“é¢˜çš„2é¢˜åˆ†æ®µè¯„åˆ†")
        return {
            'success': True,
            'question_scores': question_scores_2segment,
            'total_questions': len(question_scores_2segment)
        }

    def calculate_question_by_question_differences(self, scores_2segment: List[Dict], scores_5segment: List[Dict]) -> Dict:
        """è®¡ç®—é€é¢˜è¯„åˆ†å·®å¼‚"""
        print(f"\nğŸ“ˆ è®¡ç®—é€é¢˜è¯„åˆ†å·®å¼‚åˆ†æ")

        # åˆ›å»ºé—®é¢˜ç´¢å¼•æ˜ å°„
        scores_5_by_index = {q['question_index']: q for q in scores_5segment}
        scores_2_by_index = {q['question_index']: q for q in scores_2segment}

        question_differences = []
        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']

        for question_idx in range(1, 51):  # 1-50é¢˜
            if question_idx in scores_2_by_index and question_idx in scores_5_by_index:
                q2_scores = scores_2_by_index[question_idx]['scores']
                q5_scores = scores_5_by_index[question_idx]['scores']

                question_diff = {
                    'question_index': question_idx,
                    'segment_2': scores_2_by_index[question_idx]['segment_2'],
                    'segment_5': scores_5_by_index[question_idx]['segment_5'],
                    'trait_differences': {},
                    'overall_difference': 0,
                    'max_difference': 0,
                    'consistent_traits': 0,
                    'inconsistent_traits': 0
                }

                total_diff = 0
                max_diff = 0
                consistent_count = 0

                for trait in traits:
                    score_2 = q2_scores.get(trait, 3)
                    score_5 = q5_scores.get(trait, 3)
                    difference = abs(score_2 - score_5)

                    question_diff['trait_differences'][trait] = {
                        'score_2segment': score_2,
                        'score_5segment': score_5,
                        'difference': difference,
                        'consistent': difference == 0
                    }

                    total_diff += difference
                    max_diff = max(max_diff, difference)
                    if difference == 0:
                        consistent_count += 1

                question_diff['overall_difference'] = total_diff
                question_diff['max_difference'] = max_diff
                question_diff['consistent_traits'] = consistent_count
                question_diff['inconsistent_traits'] = 5 - consistent_count

                # åˆ†ç±»å·®å¼‚ç­‰çº§
                if max_diff == 0:
                    question_diff['difference_level'] = 'å®Œå…¨ä¸€è‡´'
                elif max_diff <= 2:
                    question_diff['difference_level'] = 'è½»å¾®å·®å¼‚'
                elif max_diff <= 4:
                    question_diff['difference_level'] = 'ä¸­ç­‰å·®å¼‚'
                else:
                    question_diff['difference_level'] = 'æ˜¾è‘—å·®å¼‚'

                question_differences.append(question_diff)

        print(f"  ğŸ“Š åˆ†æäº† {len(question_differences)} é“é¢˜çš„å·®å¼‚")
        return question_differences

    def generate_detailed_difference_report(self, question_differences: List[Dict]) -> Dict:
        """ç”Ÿæˆè¯¦ç»†å·®å¼‚æŠ¥å‘Š"""
        print(f"\nğŸ“‹ ç”Ÿæˆè¯¦ç»†å·®å¼‚åˆ†ææŠ¥å‘Š")

        # ç»Ÿè®¡åˆ†æ
        total_questions = len(question_differences)
        difference_levels = {'å®Œå…¨ä¸€è‡´': 0, 'è½»å¾®å·®å¼‚': 0, 'ä¸­ç­‰å·®å¼‚': 0, 'æ˜¾è‘—å·®å¼‚': 0}
        trait_consistency = {trait: {'consistent': 0, 'total': 0} for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']}

        overall_differences = []
        max_differences = []
        consistent_counts = []

        for q_diff in question_differences:
            difference_levels[q_diff['difference_level']] += 1
            overall_differences.append(q_diff['overall_difference'])
            max_differences.append(q_diff['max_difference'])
            consistent_counts.append(q_diff['consistent_traits'])

            # ç»Ÿè®¡ç‰¹è´¨ä¸€è‡´æ€§
            for trait, trait_diff in q_diff['trait_differences'].items():
                trait_consistency[trait]['total'] += 1
                if trait_diff['consistent']:
                    trait_consistency[trait]['consistent'] += 1

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        avg_overall_diff = statistics.mean(overall_differences) if overall_differences else 0
        avg_max_diff = statistics.mean(max_differences) if max_differences else 0
        avg_consistent_traits = statistics.mean(consistent_counts) if consistent_counts else 0

        print(f"ğŸ“Š å·®å¼‚åˆ†å¸ƒ:")
        for level, count in difference_levels.items():
            percentage = (count / total_questions) * 100 if total_questions > 0 else 0
            print(f"  {level}: {count}é¢˜ ({percentage:.1f}%)")

        print(f"\nğŸ¯ æ€»ä½“ç»Ÿè®¡:")
        print(f"  å¹³å‡æ€»ä½“å·®å¼‚: {avg_overall_diff:.2f}")
        print(f"  å¹³å‡æœ€å¤§å·®å¼‚: {avg_max_diff:.2f}")
        print(f"  å¹³å‡ä¸€è‡´ç‰¹è´¨æ•°: {avg_consistent_traits:.1f}/5")

        print(f"\nğŸ“‹ ç‰¹è´¨ä¸€è‡´æ€§åˆ†æ:")
        for trait, stats in trait_consistency.items():
            consistency_rate = (stats['consistent'] / stats['total']) * 100 if stats['total'] > 0 else 0
            print(f"  {trait}: {stats['consistent']}/{stats['total']} ({consistency_rate:.1f}%)")

        # æ‰¾å‡ºå·®å¼‚æœ€å¤§çš„é—®é¢˜
        most_different_questions = sorted(question_differences, key=lambda x: x['max_difference'], reverse=True)[:5]
        most_consistent_questions = sorted(question_differences, key=lambda x: x['consistent_traits'], reverse=True)[:5]

        print(f"\nğŸ” å·®å¼‚æœ€å¤§çš„5é¢˜:")
        for i, q in enumerate(most_different_questions, 1):
            print(f"  {i}. é¢˜{q['question_index']}: æœ€å¤§å·®å¼‚{q['max_difference']}, ä¸€è‡´ç‰¹è´¨{q['consistent_traits']}/5")

        print(f"\nâœ… æœ€ä¸€è‡´çš„5é¢˜:")
        for i, q in enumerate(most_consistent_questions, 1):
            print(f"  {i}. é¢˜{q['question_index']}: {q['consistent_traits']}/5ç‰¹è´¨ä¸€è‡´")

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        detailed_report = {
            "analysis_info": {
                "total_questions_analyzed": total_questions,
                "analysis_date": datetime.now().isoformat(),
                "model_used": self.model
            },
            "difference_distribution": difference_levels,
            "overall_statistics": {
                "average_overall_difference": avg_overall_diff,
                "average_max_difference": avg_max_diff,
                "average_consistent_traits": avg_consistent_traits,
                "perfectly_consistent_questions": difference_levels['å®Œå…¨ä¸€è‡´'],
                "perfect_consistency_rate": (difference_levels['å®Œå…¨ä¸€è‡´'] / total_questions) * 100
            },
            "trait_consistency": {
                trait: {
                    "consistent_count": stats['consistent'],
                    "total_count": stats['total'],
                    "consistency_rate": (stats['consistent'] / stats['total']) * 100 if stats['total'] > 0 else 0
                }
                for trait, stats in trait_consistency.items()
            },
            "question_level_differences": question_differences,
            "most_different_questions": most_different_questions,
            "most_consistent_questions": most_consistent_questions,
            "conclusions": self._generate_conclusions(avg_overall_diff, avg_consistent_traits, difference_levels)
        }

        return detailed_report

    def _generate_conclusions(self, avg_overall_diff: float, avg_consistent_traits: float, difference_levels: Dict) -> Dict:
        """ç”Ÿæˆç»“è®º"""
        perfect_consistency_rate = (difference_levels['å®Œå…¨ä¸€è‡´'] / sum(difference_levels.values())) * 100

        if perfect_consistency_rate >= 80:
            reliability_level = "ä¼˜ç§€"
            recommendation = "âœ… 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¸2é¢˜åˆ†æ®µé«˜åº¦ä¸€è‡´ï¼Œæ¨èä½¿ç”¨"
        elif perfect_consistency_rate >= 60:
            reliability_level = "è‰¯å¥½"
            recommendation = "âœ… 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¸2é¢˜åˆ†æ®µåŸºæœ¬ä¸€è‡´ï¼Œå¯ä»¥ä½¿ç”¨"
        elif perfect_consistency_rate >= 40:
            reliability_level = "ä¸­ç­‰"
            recommendation = "âš ï¸ 5é¢˜åˆ†æ®µæ–¹æ¡ˆå­˜åœ¨ä¸€äº›å·®å¼‚ï¼Œéœ€è¦è°¨æ…ä½¿ç”¨"
        else:
            reliability_level = "éœ€è¦æ”¹è¿›"
            recommendation = "âŒ 5é¢˜åˆ†æ®µæ–¹æ¡ˆå·®å¼‚è¾ƒå¤§ï¼Œä¸å»ºè®®ä½¿ç”¨"

        return {
            "reliability_level": reliability_level,
            "recommendation": recommendation,
            "perfect_consistency_rate": perfect_consistency_rate,
            "average_consistency_per_question": avg_consistent_traits,
            "key_finding": f"å¹³å‡æ¯é¢˜æœ‰{avg_consistent_traits:.1f}/5ä¸ªç‰¹è´¨è¯„åˆ†ä¸€è‡´"
        }

    def complete_question_by_question_analysis(self, data_file: str, analysis_2segment_file: str) -> Dict:
        """å®Œæ•´çš„é€é¢˜å¯¹æ¯”åˆ†æ"""
        print("ğŸš€ å¼€å§‹å®Œæ•´50é¢˜é€é¢˜å¯¹æ¯”åˆ†æ")
        print("=" * 70)

        # 1. åŠ è½½2é¢˜åˆ†æ®µè¯„åˆ†
        result_2segment = self.load_2segment_per_question_scores(analysis_2segment_file)
        if not result_2segment['success']:
            return {'success': False, 'error': 'æ— æ³•åŠ è½½2é¢˜åˆ†æ®µè¯„åˆ†'}

        # 2. åŠ è½½æµ‹è¯„æ•°æ®
        questions, mapping = self.load_test_data_with_mapping(data_file)
        if len(questions) < 50:
            return {'success': False, 'error': f'é—®é¢˜æ•°é‡ä¸è¶³: {len(questions)} < 50'}

        # 3. åˆ†æ5é¢˜åˆ†æ®µæ¯é¢˜è¯„åˆ†
        result_5segment = self.analyze_5segment_per_question(questions, mapping)
        if not result_5segment['success']:
            return {'success': False, 'error': '5é¢˜åˆ†æ®µåˆ†æå¤±è´¥'}

        # 4. é‡æ„2é¢˜åˆ†æ®µæ¯é¢˜è¯„åˆ†
        result_2segment_reconstructed = self.reconstruct_2segment_per_question_scores(questions, result_2segment['segment_scores'])

        # 5. è®¡ç®—é€é¢˜å·®å¼‚
        question_differences = self.calculate_question_by_question_differences(
            result_2segment_reconstructed['question_scores'],
            result_5segment['question_scores']
        )

        # 6. ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        detailed_report = self.generate_detailed_difference_report(question_differences)

        # 7. ä¿å­˜ç»“æœ
        output_filename = f"question_by_question_comparison_{Path(data_file).stem}.json"
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(detailed_report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ è¯¦ç»†å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {output_filename}")

        return {
            'success': True,
            'data_file': data_file,
            'analysis_2segment_file': analysis_2segment_file,
            'detailed_report': detailed_report
        }

def main():
    """ä¸»å‡½æ•°"""
    comparator = QuestionByQuestionComparator(model="qwen-long")

    # é€‰æ‹©æµ‹è¯•æ–‡ä»¶
    data_file = "results/results/asses_deepseek_r1_70b_agent_big_five_50_complete2_a10_e0_t0_0_09271.json"
    analysis_2segment_file = "asses_deepseek_r1_70b_agent_big_five_50_complete2_a10_e0_t0_0_09271_qwen-long_segmented_analysis.json"

    print(f"ğŸ¯ é€‰æ‹©æµ‹è¯•æ–‡ä»¶:")
    print(f"  æ•°æ®æ–‡ä»¶: {data_file}")
    print(f"  2é¢˜åˆ†æ®µåˆ†æ: {analysis_2segment_file}")

    # æ‰§è¡Œé€é¢˜å¯¹æ¯”åˆ†æ
    result = comparator.complete_question_by_question_analysis(data_file, analysis_2segment_file)

    if result['success']:
        report = result['detailed_report']
        print(f"\nğŸ‰ é€é¢˜å¯¹æ¯”åˆ†æå®Œæˆ!")
        print(f"  ğŸ“Š å®Œå…¨ä¸€è‡´é¢˜æ•°: {report['difference_distribution']['å®Œå…¨ä¸€è‡´']}/50")
        print(f"  ğŸ“ˆ å®Œç¾ä¸€è‡´æ€§ç‡: {report['overall_statistics']['perfect_consistency_rate']:.1f}%")
        print(f"  ğŸ¯ å¯é æ€§ç­‰çº§: {report['conclusions']['reliability_level']}")
        print(f"  ğŸ’¡ å»ºè®®: {report['conclusions']['recommendation']}")
    else:
        print(f"\nâŒ é€é¢˜å¯¹æ¯”åˆ†æå¤±è´¥: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()