#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çœŸæ­£çš„æ‰¹é‡å¤„ç†è„šæœ¬
æ”¯æŒæ–­ç‚¹ç»­è·‘å’Œè¿›åº¦è·Ÿè¸ª
"""

import sys
import os
import json
import argparse
from pathlib import Path
from datetime import datetime
import time
import pickle

# æ·»åŠ åŒ…ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from single_report_pipeline import TransparentPipeline


class RealBatchProcessor:
    """çœŸæ­£çš„æ‰¹é‡å¤„ç†å™¨ - æ”¯æŒæ–­ç‚¹ç»­è·‘"""
    
    def __init__(self, input_dir: str, output_dir: str, checkpoint_interval: int = 5):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            input_dir: è¾“å…¥ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            checkpoint_interval: æ£€æŸ¥ç‚¹é—´éš”
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.checkpoint_interval = checkpoint_interval
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥ç‚¹æ–‡ä»¶
        self.checkpoint_file = self.output_dir / "checkpoint.pkl"
        self.results_file = self.output_dir / "batch_results.json"
        self.progress_file = self.output_dir / "progress.txt"
        
        # åˆ›å»ºæµæ°´çº¿å®ä¾‹
        self.pipeline = TransparentPipeline()
        
        # çŠ¶æ€å˜é‡
        self.processed_files = set()
        self.results = []
        self.start_time = datetime.now()
        self.total_files = 0
        self.current_index = 0
    
    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if self.checkpoint_file.exists():
            try:
                with open(self.checkpoint_file, 'rb') as f:
                    data = pickle.load(f)
                
                self.processed_files = set(data.get('processed_files', []))
                self.results = data.get('results', [])
                self.start_time = data.get('start_time', datetime.now())
                self.total_files = data.get('total_files', 0)
                self.current_index = data.get('current_index', 0)
                
                print(f"âœ… å·²åŠ è½½æ£€æŸ¥ç‚¹: {len(self.processed_files)} ä¸ªæ–‡ä»¶å·²å¤„ç†")
                return True
            except Exception as e:
                print(f"âš ï¸  åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                return False
        else:
            print("â„¹ï¸  æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œå¼€å§‹å…¨æ–°å¤„ç†")
            return False
    
    def save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        data = {
            'processed_files': list(self.processed_files),
            'results': self.results,
            'start_time': self.start_time,
            'total_files': self.total_files,
            'current_index': self.current_index
        }
        
        try:
            with open(self.checkpoint_file, 'wb') as f:
                pickle.dump(data, f)
            print("âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
            return False
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        data = {
            'processing_info': {
                'start_time': self.start_time.isoformat(),
                'end_time': datetime.now().isoformat(),
                'total_files': self.total_files,
                'processed_files': len(self.processed_files),
                'duration_seconds': (datetime.now() - self.start_time).total_seconds()
            },
            'results': self.results
        }
        
        try:
            with open(self.results_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
            return False
    
    def save_progress(self, message: str):
        """ä¿å­˜è¿›åº¦ä¿¡æ¯"""
        try:
            with open(self.progress_file, 'a', encoding='utf-8') as f:
                f.write(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\n")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜è¿›åº¦ä¿¡æ¯å¤±è´¥: {e}")
    
    def find_json_files(self) -> list:
        """æŸ¥æ‰¾JSONæ–‡ä»¶"""
        json_files = list(self.input_dir.glob("*.json"))
        json_files.sort()
        return json_files
    
    def process_single_file(self, file_path: Path) -> dict:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœ
        """
        print(f"ğŸ” å¤„ç†æ–‡ä»¶: {file_path.name}")
        self.save_progress(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path.name}")
        
        try:
            result = self.pipeline.process_single_report(str(file_path))
            
            if result and result.get('success', False):
                print(f"  âœ… å¤„ç†å®Œæˆ: {file_path.name}")
                print(f"    å¤§äº”äººæ ¼: {result.get('big5_scores', {})}")
                print(f"    MBTIç±»å‹: {result.get('mbti_type', 'Unknown')}")
                self.save_progress(f"å¤„ç†å®Œæˆ: {file_path.name} - æˆåŠŸ")
                return result
            else:
                print(f"  âŒ å¤„ç†å¤±è´¥: {file_path.name}")
                error_msg = result.get('error', 'Unknown error') if result else 'No result'
                print(f"    é”™è¯¯: {error_msg}")
                self.save_progress(f"å¤„ç†å®Œæˆ: {file_path.name} - å¤±è´¥ - {error_msg}")
                return {
                    'success': False,
                    'file_path': str(file_path),
                    'error': error_msg
                }
                
        except Exception as e:
            print(f"  ğŸ’¥ å¤„ç†å¼‚å¸¸: {file_path.name} - {e}")
            import traceback
            traceback.print_exc()
            self.save_progress(f"å¤„ç†å®Œæˆ: {file_path.name} - å¼‚å¸¸ - {str(e)}")
            return {
                'success': False,
                'file_path': str(file_path),
                'error': str(e)
            }
    
    def run_batch_processing(self, limit: int = None, resume: bool = True):
        """
        è¿è¡Œæ‰¹é‡å¤„ç†
        
        Args:
            limit: é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡
            resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        """
        print("ğŸš€ çœŸæ­£çš„æ‰¹é‡å¤„ç†è„šæœ¬ - æ”¯æŒæ–­ç‚¹ç»­è·‘")
        print("="*80)
        print(f"è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"æ£€æŸ¥ç‚¹é—´éš”: æ¯ {self.checkpoint_interval} ä¸ªæ–‡ä»¶")
        print()
        
        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if resume:
            self.load_checkpoint()
        
        # æŸ¥æ‰¾æ–‡ä»¶
        print("ğŸ“‚ æŸ¥æ‰¾æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶...")
        json_files = self.find_json_files()
        
        if not json_files:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
            return False
        
        self.total_files = len(json_files)
        if limit:
            json_files = json_files[:limit]
            self.total_files = len(json_files)
        
        print(f"  æ‰¾åˆ° {len(json_files)} ä¸ªæµ‹è¯„æŠ¥å‘Šæ–‡ä»¶")
        print(f"  å·²å¤„ç†: {len(self.processed_files)} ä¸ª")
        print(f"  å‰©ä½™: {len(json_files) - len(self.processed_files)} ä¸ª")
        print()
        
        # ä»æ£€æŸ¥ç‚¹ä½ç½®å¼€å§‹å¤„ç†
        start_index = 0
        if resume and self.current_index < len(json_files):
            start_index = self.current_index
        
        print(f"â–¶ï¸  ä»ç¬¬ {start_index + 1} ä¸ªæ–‡ä»¶å¼€å§‹å¤„ç†")
        print()
        
        # å¤„ç†æ–‡ä»¶
        processed_count = 0
        success_count = 0
        failed_count = 0
        
        for i in range(start_index, len(json_files)):
            file_path = json_files[i]
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if str(file_path) in self.processed_files:
                print(f"â­ï¸  è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}")
                continue
            
            # å¤„ç†æ–‡ä»¶
            result = self.process_single_file(file_path)
            
            # æ›´æ–°çŠ¶æ€
            self.processed_files.add(str(file_path))
            self.results.append(result)
            self.current_index = i + 1
            
            if result.get('success', False):
                success_count += 1
            else:
                failed_count += 1
            
            processed_count += 1
            
            # æ˜¾ç¤ºè¿›åº¦
            if processed_count % 10 == 0:
                print(f"  ğŸ“Š è¿›åº¦: {processed_count} ä¸ªæ–‡ä»¶å·²å¤„ç† "
                      f"(æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count})")
                self.save_progress(f"è¿›åº¦: {processed_count} ä¸ªæ–‡ä»¶å·²å¤„ç† "
                                 f"(æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count})")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if processed_count % self.checkpoint_interval == 0:
                print(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹...")
                self.save_checkpoint()
                self.save_results()
                self.save_progress(f"ä¿å­˜æ£€æŸ¥ç‚¹: å¤„ç†äº† {processed_count} ä¸ªæ–‡ä»¶")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIè¿‡è½½
            time.sleep(1)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        print(f"\nğŸ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print("="*80)
        print(f"æ€»æ–‡ä»¶æ•°: {len(json_files)}")
        print(f"å·²å¤„ç†æ•°: {processed_count}")
        print(f"æˆåŠŸå¤„ç†: {success_count}")
        print(f"å¤„ç†å¤±è´¥: {failed_count}")
        print(f"æˆåŠŸç‡: {success_count/processed_count*100:.1f}%" if processed_count > 0 else "N/A")
        
        self.save_checkpoint()
        self.save_results()
        self.save_progress(f"æ‰¹é‡å¤„ç†å®Œæˆ: æ€»æ–‡ä»¶æ•° {len(json_files)}, "
                         f"æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        
        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {self.results_file}")
        print(f"ğŸ” å¦‚éœ€ç»§ç»­å¤„ç†å‰©ä½™æ–‡ä»¶ï¼Œè¯·é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
        
        return True


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='çœŸæ­£çš„æ‰¹é‡å¤„ç†è„šæœ¬ - æ”¯æŒæ–­ç‚¹ç»­è·‘')
    parser.add_argument('--input-dir', default='../results/readonly-original',
                       help='è¾“å…¥ç›®å½• (é»˜è®¤: ../results/readonly-original)')
    parser.add_argument('--output-dir', default='../results/batch-processing-results',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: ../results/batch-processing-results)')
    parser.add_argument('--limit', type=int,
                       help='é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡')
    parser.add_argument('--checkpoint-interval', type=int, default=5,
                       help='æ£€æŸ¥ç‚¹é—´éš” (é»˜è®¤: æ¯5ä¸ªæ–‡ä»¶)')
    parser.add_argument('--no-resume', action='store_true',
                       help='ä¸ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œé‡æ–°å¼€å§‹')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæ‰¹é‡å¤„ç†å™¨
    processor = RealBatchProcessor(
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        checkpoint_interval=args.checkpoint_interval
    )
    
    # è¿è¡Œæ‰¹é‡å¤„ç†
    success = processor.run_batch_processing(
        limit=args.limit,
        resume=not args.no_resume
    )
    
    if success:
        print("\nğŸ‰ æ‰¹é‡å¤„ç†æˆåŠŸå®Œæˆ!")
        return 0
    else:
        print("\nâŒ æ‰¹é‡å¤„ç†å¤±è´¥!")
        return 1


if __name__ == "__main__":
    sys.exit(main())