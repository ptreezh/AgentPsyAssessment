#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ollamaå¹¶è¡Œæ‰¹é‡åˆ†æå™¨
ä½¿ç”¨ä¸‰ä¸ªOllamaæ¨¡å‹ä»åå¾€å‰åˆ†ææ‰€æœ‰æµ‹è¯„æŠ¥å‘Š
5é¢˜åˆ†æ®µï¼ˆæ¯æ®µ5é¢˜ï¼Œæ¯ä¸ªæµ‹è¯„æŠ¥å‘Šåˆ†10æ®µï¼‰ï¼Œä¸‰æ¨¡å‹ç‹¬ç«‹å¹¶è¡Œè¯„ä¼°
é¿å…ä¸ç°æœ‰äº‘æ¨¡å‹è¿›ç¨‹å†²çª
"""

import sys
import os
import json
import subprocess
import time
import threading
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import statistics
import concurrent.futures
import glob
import math

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['PYTHONIOENCODING'] = 'utf-8'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# å¯¼å…¥å·²ç»éªŒè¯çš„TDDåˆ†æå™¨
from ollama_tdd_5segment_analyzer import OllamaTDD5SegmentAnalyzer

class OllamaParallelBatchAnalyzer:
    def __init__(self, input_dir: str, output_dir: str, num_processes: int = 3):
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.num_processes = num_processes

        # ä½¿ç”¨å·²ç»éªŒè¯çš„TDDåˆ†æå™¨
        self.analyzer = OllamaTDD5SegmentAnalyzer()

        # ä¸‰ä¸ªOllamaæ¨¡å‹é…ç½®
        self.models = [
            {"name": "deepseek-v3.1:671b-cloud", "description": "DeepSeek 671Bäº‘æ¨¡å‹"},
            {"name": "gpt-oss:120b-cloud", "description": "GPT OSS 120Bäº‘æ¨¡å‹"},
            {"name": "qwen3-coder:480b-cloud", "description": "Qwen3 Coder 480Bäº‘æ¨¡å‹"}
        ]

        # å¤„ç†ç»Ÿè®¡
        self.stats = {
            "total_files": 0,
            "processed_files": 0,
            "failed_files": 0,
            "total_segments": 0,
            "successful_segments": 0,
            "total_processing_time": 0,
            "start_time": None,
            "files_processed_per_model": {model["name"]: 0 for model in self.models}
        }

        # è¿›ç¨‹é”ï¼Œé¿å…å†²çª
        self.process_locks = {model["name"]: threading.Lock() for model in self.models}

    def get_all_files_sorted_reverse(self) -> List[str]:
        """è·å–æ‰€æœ‰æ–‡ä»¶ï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—ï¼ˆä»æœ€æ–°åˆ°æœ€æ—§ï¼‰"""
        file_pattern = os.path.join(self.input_dir, "*.json")
        files = glob.glob(file_pattern)

        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—ï¼Œä»æœ€æ–°çš„å¼€å§‹
        files.sort(key=lambda x: os.path.getmtime(x), reverse=True)

        print(f"ğŸ“ æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ï¼ˆä»æœ€æ–°å¼€å§‹æ’åºï¼‰")
        return files

    def split_files_for_processes(self, files: List[str]) -> List[List[str]]:
        """å°†æ–‡ä»¶åˆ†é…ç»™ä¸åŒè¿›ç¨‹"""
        total_files = len(files)
        batch_size = math.ceil(total_files / self.num_processes)

        batches = []
        for i in range(0, total_files, batch_size):
            batch = files[i:i + batch_size]
            batches.append(batch)

        print(f"ğŸ“¦ åˆ†é…ç»™ {len(batches)} ä¸ªè¿›ç¨‹:")
        for i, batch in enumerate(batches, 1):
            print(f"   è¿›ç¨‹ {i}: {len(batch)} ä¸ªæ–‡ä»¶")

        return batches

    def check_existing_results(self, output_dir: str) -> set:
        """æ£€æŸ¥å·²ç»å¤„ç†è¿‡çš„æ–‡ä»¶"""
        if not os.path.exists(output_dir):
            return set()

        existing_files = set()
        pattern = os.path.join(output_dir, "*_ollama_tdd_5segment_analysis.json")
        for file_path in glob.glob(pattern):
            # æå–åŸå§‹æ–‡ä»¶å
            original_name = Path(file_path).stem.replace("_ollama_tdd_5segment_analysis", "")
            existing_files.add(original_name)

        print(f"ğŸ” å‘ç° {len(existing_files)} ä¸ªå·²å¤„ç†çš„æ–‡ä»¶")
        return existing_files

    def analyze_single_file(self, file_path: str, process_id: int) -> Dict:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        start_time = time.time()
        file_name = Path(file_path).name

        print(f"ğŸ”„ [è¿›ç¨‹{process_id}] å¼€å§‹åˆ†æ: {file_name}")

        try:
            # åˆ›å»ºè¿›ç¨‹ç‰¹å®šçš„è¾“å‡ºç›®å½•
            process_output_dir = os.path.join(self.output_dir, f"process_{process_id}")
            os.makedirs(process_output_dir, exist_ok=True)

            # ä½¿ç”¨TDDåˆ†æå™¨åˆ†ææ–‡ä»¶
            result = self.analyzer.analyze_file_with_three_models(file_path, process_output_dir)

            processing_time = time.time() - start_time

            if result['success']:
                print(f"   âœ… [è¿›ç¨‹{process_id}] {file_name} - æˆåŠŸ (ç”¨æ—¶{processing_time:.1f}s)")
                print(f"      ä¸€è‡´æ€§: {result['consistency_analysis'].get('consensus_mbti', 'UNKNOWN')}")

                return {
                    'success': True,
                    'file_path': file_path,
                    'output_path': result['output_path'],
                    'processing_time': processing_time,
                    'consistency_analysis': result['consistency_analysis'],
                    'process_id': process_id
                }
            else:
                print(f"   âŒ [è¿›ç¨‹{process_id}] {file_name} - å¤±è´¥: {result.get('error', 'Unknown error')}")

                return {
                    'success': False,
                    'file_path': file_path,
                    'error': result.get('error', 'Unknown error'),
                    'processing_time': processing_time,
                    'process_id': process_id
                }

        except Exception as e:
            print(f"   âš ï¸ [è¿›ç¨‹{process_id}] {file_name} - å¼‚å¸¸: {str(e)}")

            return {
                'success': False,
                'file_path': file_path,
                'error': f"å¤„ç†å¼‚å¸¸: {str(e)}",
                'processing_time': time.time() - start_time,
                'process_id': process_id
            }

    def process_batch_files(self, batch_files: List[str], process_id: int) -> List[Dict]:
        """å¤„ç†ä¸€æ‰¹æ–‡ä»¶"""
        results = []

        print(f"ğŸš€ [è¿›ç¨‹{process_id}] å¼€å§‹å¤„ç† {len(batch_files)} ä¸ªæ–‡ä»¶")

        for i, file_path in enumerate(batch_files, 1):
            result = self.analyze_single_file(file_path, process_id)
            results.append(result)

            # æ›´æ–°ç»Ÿè®¡
            with self.stats_lock:
                self.stats["processed_files"] += 1
                if result['success']:
                    self.stats["successful_segments"] += 1
                else:
                    self.stats["failed_files"] += 1

        print(f"âœ… [è¿›ç¨‹{process_id}] å®Œæˆå¤„ç†ï¼ŒæˆåŠŸç‡: {sum(1 for r in results if r['success']) / len(results):.1%}")
        return results

    def generate_progress_report(self) -> Dict:
        """ç”Ÿæˆè¿›åº¦æŠ¥å‘Š"""
        current_time = datetime.now()
        elapsed_time = (current_time - self.stats["start_time"]).total_seconds() if self.stats["start_time"] else 0

        # è®¡ç®—é¢„ä¼°å®Œæˆæ—¶é—´
        if self.stats["processed_files"] > 0:
            avg_time_per_file = elapsed_time / self.stats["processed_files"]
            remaining_files = self.stats["total_files"] - self.stats["processed_files"]
            estimated_remaining_time = remaining_files * avg_time_per_file / self.num_processes
            estimated_completion = current_time + timedelta(seconds=estimated_remaining_time)
        else:
            estimated_remaining_time = 0
            estimated_completion = current_time

        return {
            "timestamp": current_time.isoformat(),
            "progress": {
                "total_files": self.stats["total_files"],
                "processed_files": self.stats["processed_files"],
                "failed_files": self.stats["failed_files"],
                "success_rate": self.stats["processed_files"] / self.stats["total_files"] if self.stats["total_files"] > 0 else 0,
                "completion_percentage": (self.stats["processed_files"] / self.stats["total_files"]) * 100 if self.stats["total_files"] > 0 else 0
            },
            "performance": {
                "elapsed_time": elapsed_time,
                "avg_time_per_file": elapsed_time / self.stats["processed_files"] if self.stats["processed_files"] > 0 else 0,
                "files_per_hour": (self.stats["processed_files"] / elapsed_time * 3600) if elapsed_time > 0 else 0,
                "estimated_remaining_time": estimated_remaining_time,
                "estimated_completion_time": estimated_completion.isoformat()
            },
            "model_stats": self.stats["files_processed_per_model"]
        }

    def run_parallel_analysis(self):
        """è¿è¡Œå¹¶è¡Œåˆ†æ"""
        print("ğŸš€ å¯åŠ¨Ollamaå¹¶è¡Œæ‰¹é‡åˆ†æå™¨")
        print("=" * 60)
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {self.input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ğŸ”§ å¹¶è¡Œè¿›ç¨‹æ•°: {self.num_processes}")
        print(f"ğŸŒ Ollamaæ¨¡å‹: {', '.join([m['name'] for m in self.models])}")
        print(f"ğŸ“Š åˆ†ææ–¹å¼: 5é¢˜åˆ†æ®µï¼ˆæ¯æ®µ5é¢˜ï¼Œ10æ®µ/æ–‡ä»¶ï¼‰")
        print(f"ğŸ”„ å¤„ç†é¡ºåº: ä»æœ€æ–°åˆ°æœ€æ—§")
        print()

        # åˆå§‹åŒ–ç»Ÿè®¡
        self.stats["start_time"] = datetime.now()

        # è·å–æ‰€æœ‰æ–‡ä»¶
        all_files = self.get_all_files_sorted_reverse()
        self.stats["total_files"] = len(all_files)

        if not all_files:
            print("âŒ æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶")
            return

        # æ£€æŸ¥å·²å¤„ç†çš„æ–‡ä»¶
        existing_files = self.check_existing_results(self.output_dir)

        # è¿‡æ»¤æœªå¤„ç†çš„æ–‡ä»¶
        unprocessed_files = []
        for file_path in all_files:
            file_name = Path(file_path).stem
            if file_name not in existing_files:
                unprocessed_files.append(file_path)

        print(f"ğŸ“‹ å¾…å¤„ç†æ–‡ä»¶: {len(unprocessed_files)} (è·³è¿‡å·²å¤„ç†çš„ {len(existing_files)} ä¸ªæ–‡ä»¶)")

        if not unprocessed_files:
            print("âœ… æ‰€æœ‰æ–‡ä»¶éƒ½å·²å¤„ç†å®Œæˆ")
            return

        # åˆ†é…æ–‡ä»¶ç»™ä¸åŒè¿›ç¨‹
        file_batches = self.split_files_for_processes(unprocessed_files)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)

        print(f"\nğŸš€ å¯åŠ¨ {len(file_batches)} ä¸ªå¹¶è¡Œè¿›ç¨‹...")
        print("=" * 60)

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_processes) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡çš„å¤„ç†ä»»åŠ¡
            future_to_batch = {}

            for i, batch_files in enumerate(file_batches, 1):
                print(f"ğŸŒ æäº¤è¿›ç¨‹ {i} çš„ä»»åŠ¡ ({len(batch_files)} ä¸ªæ–‡ä»¶)")
                future = executor.submit(self.process_batch_files, batch_files, i)
                future_to_batch[future] = i

            # æ”¶é›†ç»“æœ
            all_results = []
            completed_batches = 0

            for future in concurrent.futures.as_completed(future_to_batch):
                batch_id = future_to_batch[future]
                completed_batches += 1

                try:
                    batch_results = future.result()
                    all_results.extend(batch_results)

                    print(f"âœ… è¿›ç¨‹ {batch_id} å®Œæˆ (ç¬¬ {completed_batches}/{len(file_batches)} ä¸ªè¿›ç¨‹)")

                    # ç”Ÿæˆè¿›åº¦æŠ¥å‘Š
                    progress_report = self.generate_progress_report()
                    print(f"ğŸ“Š è¿›åº¦: {progress_report['progress']['completion_percentage']:.1f}% ({progress_report['progress']['processed_files']}/{progress_report['progress']['total_files']} æ–‡ä»¶)")
                    print(f"â±ï¸ å¤„ç†é€Ÿåº¦: {progress_report['performance']['files_per_hour']:.1f} æ–‡ä»¶/å°æ—¶")
                    print(f"â° é¢„è®¡å‰©ä½™æ—¶é—´: {timedelta(seconds=int(progress_report['performance']['estimated_remaining_time']))")

                    # ä¿å­˜è¿›åº¦æŠ¥å‘Š
                    progress_file = os.path.join(self.output_dir, "progress_report.json")
                    with open(progress_file, 'w', encoding='utf-8') as f:
                        json.dump(progress_report, f, ensure_ascii=False, indent=2)

                except Exception as e:
                    print(f"âŒ è¿›ç¨‹ {batch_id} å¤±è´¥: {e}")

        # æœ€ç»ˆç»Ÿè®¡
        final_stats = self.generate_progress_report()

        print("\n" + "=" * 60)
        print("ğŸ‰ Ollamaå¹¶è¡Œæ‰¹é‡åˆ†æå®Œæˆ!")
        print("=" * 60)

        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ€»æ–‡ä»¶æ•°: {final_stats['progress']['total_files']}")
        print(f"   æˆåŠŸå¤„ç†: {final_stats['progress']['processed_files']}")
        print(f"   å¤„ç†å¤±è´¥: {final_stats['progress']['failed_files']}")
        print(f"   æˆåŠŸç‡: {final_stats['progress']['success_rate']:.1%}")
        print(f"   æ€»è€—æ—¶: {timedelta(seconds=int(final_stats['performance']['elapsed_time']))}")
        print(f"   å¹³å‡é€Ÿåº¦: {final_stats['performance']['files_per_hour']:.1f} æ–‡ä»¶/å°æ—¶")

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "analysis_summary": {
                "completion_time": datetime.now().isoformat(),
                "input_directory": self.input_dir,
                "output_directory": self.output_dir,
                "models_used": [{"name": m["name"], "description": m["description"]} for m in self.models],
                "analysis_method": "5é¢˜åˆ†æ®µï¼Œä¸‰æ¨¡å‹å¹¶è¡Œ",
                "processing_order": "ä»æœ€æ–°åˆ°æœ€æ—§",
                "progress_stats": final_stats,
                "all_results": all_results
            }
        }

        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        final_report_file = os.path.join(self.output_dir, "final_batch_analysis_report.json")
        with open(final_report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜: {final_report_file}")

        # ç»Ÿè®¡ä¸€è‡´æ€§ç»“æœ
        successful_results = [r for r in all_results if r['success']]
        if successful_results:
            consensus_types = {}
            for result in successful_results:
                mbti_type = result.get('consistency_analysis', {}).get('consensus_mbti', 'UNKNOWN')
                if mbti_type != 'UNKNOWN':
                    consensus_types[mbti_type] = consensus_types.get(mbti_type, 0) + 1

            if consensus_types:
                print(f"\nğŸ¯ MBTIç±»å‹åˆ†å¸ƒ:")
                for mbti_type, count in sorted(consensus_types.items(), key=lambda x: x[1], reverse=True):
                    print(f"   {mbti_type}: {count} ä¸ªæ–‡ä»¶")

        return final_report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Ollamaå¹¶è¡Œæ‰¹é‡åˆ†æå™¨")
    print("ä½¿ç”¨ä¸‰ä¸ªOllamaæ¨¡å‹ä»åå¾€å‰åˆ†ææ‰€æœ‰æµ‹è¯„æŠ¥å‘Š")
    print("5é¢˜åˆ†æ®µï¼Œä¸‰æ¨¡å‹å¹¶è¡Œï¼Œé¿å…ä¸ç°æœ‰äº‘æ¨¡å‹è¿›ç¨‹å†²çª")
    print("=" * 80)

    # æ£€æŸ¥è¾“å…¥ç›®å½•
    input_dir = "results/results"
    if not os.path.exists(input_dir):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = "ollama_parallel_batch_results"

    # åˆ›å»ºåˆ†æå™¨
    analyzer = OllamaParallelBatchAnalyzer(
        input_dir=input_dir,
        output_dir=output_dir,
        num_processes=3  # ä¸‰ä¸ªå¹¶è¡Œè¿›ç¨‹ï¼Œå¯¹åº”ä¸‰ä¸ªæ¨¡å‹
    )

    # è¿è¡Œå¹¶è¡Œåˆ†æ
    try:
        analyzer.run_parallel_analysis()
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­åˆ†æ")
        print(f"ğŸ“Š å½“å‰è¿›åº¦: {len([f for f in glob.glob(os.path.join(output_dir, '*')) if f.endswith('.json')])} ä¸ªç»“æœæ–‡ä»¶")
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    print("\nâœ… Ollamaå¹¶è¡Œæ‰¹é‡åˆ†æå™¨è¿è¡Œå®Œæˆ")

if __name__ == "__main__":
    main()