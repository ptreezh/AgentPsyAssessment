#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº‘è¯„ä¼°å™¨åˆ†æ®µå¼å¿ƒç†è¯„ä¼°åˆ†æå™¨
ä½¿ç”¨Qwenäº‘æ¨¡å‹è¿›è¡Œåˆ†æ®µå¤„ç†ï¼Œé€æ­¥ç´¯ç§¯è¯„åˆ†ï¼Œç¡®ä¿åˆ†æå®Œæ•´æ€§å’Œå‡†ç¡®æ€§
ä¸¥æ ¼éµå¾ª1-3-5è¯„åˆ†æ ‡å‡†
"""

import json
import sys
import time
from typing import Dict, List, Any, Tuple
from pathlib import Path
from datetime import datetime
import openai

class CloudSegmentedPersonalityAnalyzer:
    """äº‘è¯„ä¼°å™¨åˆ†æ®µå¼äººæ ¼åˆ†æå™¨"""

    def __init__(self, model: str = "qwen-long", api_key: str = None, max_questions_per_segment: int = 2):
        self.model = model
        self.api_key = api_key or "sk-ffd03518254b495b8d27e723cd413fc1"
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.max_questions_per_segment = max_questions_per_segment

        # åˆå§‹åŒ–Big5è¯„åˆ†ç´¯ç§¯å™¨
        self.big_five_traits = {
            'openness_to_experience': {'scores': [], 'evidence': [], 'weight': 0},
            'conscientiousness': {'scores': [], 'evidence': [], 'weight': 0},
            'extraversion': {'scores': [], 'evidence': [], 'weight': 0},
            'agreeableness': {'scores': [], 'evidence': [], 'weight': 0},
            'neuroticism': {'scores': [], 'evidence': [], 'weight': 0}
        }

        self.analysis_log = []
        self.per_question_scores = []

        # åˆå§‹åŒ–äº‘è¯„ä¼°å™¨å®¢æˆ·ç«¯
        self.client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

    def extract_questions(self, assessment_data: Dict) -> List[Dict]:
        """ä»è¯„ä¼°æ•°æ®ä¸­æå–é—®é¢˜åˆ—è¡¨"""
        if 'assessment_results' in assessment_data:
            assessment_results = assessment_data['assessment_results']

            if isinstance(assessment_results, list):
                if len(assessment_results) > 0 and isinstance(assessment_results[0], dict):
                    if 'question_data' in assessment_results[0]:
                        questions = []
                        for result in assessment_results:
                            if 'question_data' in result:
                                question_data = result['question_data'].copy()
                                # æ·»åŠ agent_responseä»conversation_logä¸­æå–
                                if 'conversation_log' in result:
                                    for msg in result['conversation_log']:
                                        if msg.get('role') == 'assistant':
                                            question_data['agent_response'] = msg['content']
                                            break
                                questions.append(question_data)
                        return questions

        if 'questions' in assessment_data:
            return assessment_data['questions']

        print(f"è­¦å‘Š: æ— æ³•ä»æ•°æ®ä¸­æå–é—®é¢˜ï¼Œå¯ç”¨é”®: {list(assessment_data.keys())}")
        return []

    def create_segments(self, questions: List[Dict]) -> List[List[Dict]]:
        """åˆ›å»ºåˆ†æ®µï¼Œæ¯æ®µåŒ…å«æŒ‡å®šæ•°é‡çš„é—®é¢˜"""
        segments = []
        for i in range(0, len(questions), self.max_questions_per_segment):
            segment = questions[i:i + self.max_questions_per_segment]
            if segment:  # ç¡®ä¿æ®µä¸ä¸ºç©º
                segments.append(segment)

        print(f"åˆ›å»º {len(segments)} ä¸ªåˆ†æ®µï¼Œæ¯æ®µæœ€å¤š {self.max_questions_per_segment} ä¸ªé—®é¢˜")
        return segments

    def analyze_segment(self, segment: List[Dict], segment_number: int) -> Dict:
        """åˆ†æå•ä¸ªåˆ†æ®µï¼Œä½¿ç”¨ä¸¥æ ¼çš„1-3-5è¯„åˆ†æ ‡å‡†"""

        system_prompt = f"""You are a personality analyst specialized in Big Five assessment. Analyze {len(segment)} questions and provide Big Five scores using ONLY the 1-3-5 scale.

For each question, assess all 5 traits using EXCLUSIVELY:
- 1: Low/Minimal expression of the trait
- 3: Moderate/Average expression of the trait
- 5: High/Strong expression of the trait

Scoring guidelines based on evidence quality:
- Direct evidence: Score 1, 3, or 5 based on explicit behavior in the response
- Limited evidence: Score 3 using professional inference when direct evidence is weak
- No evidence: Score 3 using professional judgment when no clear evidence exists

REQUIRED JSON FORMAT:
{{
    "question_scores": [
        {{
            "question_id": "Q1",
            "dimension": "extraversion",
            "big_five_scores": {{
                "openness_to_experience": {{"score": 3, "evidence": "Evidence from response", "quality": "direct"}},
                "conscientiousness": {{"score": 3, "evidence": "Professional inference", "quality": "inferred"}},
                "extraversion": {{"score": 5, "evidence": "Direct evidence", "quality": "direct"}},
                "agreeableness": {{"score": 3, "evidence": "Inference from response", "quality": "inferred"}},
                "neuroticism": {{"score": 1, "evidence": "Evidence from response", "quality": "direct"}}
            }}
        }}
    ]
}}

Return ONLY valid JSON. All traits must have scores 1, 3, or 5 with evidence. No null values."""

        # æ„å»ºç”¨æˆ·è¾“å…¥
        user_content = []
        for i, question in enumerate(segment):
            user_content.append(f"Question {i+1} ({question.get('dimension', 'Unknown')}):")
            user_content.append(f"Scenario: {question['scenario']}")
            response = question.get('agent_response', '')
            user_content.append(f"Response: {response[:500]}...")
            user_content.append(f"Rubric: {question.get('evaluation_rubric', {}).get('description', 'N/A')}")
            user_content.append("---")

        user_prompt = "\n".join(user_content)

        try:
            print(f"  ä½¿ç”¨ {self.model} åˆ†ææ®µ {segment_number} ({len(segment)} é¢˜)")

            # è°ƒç”¨äº‘è¯„ä¼°å™¨
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.1,
                max_tokens=2000
            )

            response_text = response.choices[0].message.content
            print(f"  æ®µ {segment_number} åˆ†ææˆåŠŸ")

            return {
                'system_prompt': system_prompt,
                'user_prompt': user_prompt,
                'segment_number': segment_number,
                'llm_response': response_text,
                'success': True
            }

        except Exception as e:
            print(f"  æ®µ {segment_number} åˆ†æå¤±è´¥: {e}")
            return {
                'system_prompt': system_prompt,
                'user_prompt': user_prompt,
                'segment_number': segment_number,
                'error': str(e),
                'success': False
            }

    def accumulate_scores(self, segment_result: Dict) -> None:
        """ç´¯ç§¯åˆ†æ®µåˆ†æç»“æœåˆ°æ€»è¯„åˆ†"""
        if not segment_result.get('success'):
            print(f"  è·³è¿‡å¤±è´¥çš„åˆ†æ®µ {segment_result.get('segment_number', 'Unknown')}")
            return

        try:
            # è§£æLLMå“åº”
            response_text = segment_result['llm_response']

            # å°è¯•æå–JSON
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                json_text = response_text[json_start:json_end].strip()
                response_data = json.loads(json_text)
            else:
                response_data = json.loads(response_text)

            if 'question_scores' not in response_data:
                print(f"  è­¦å‘Š: åˆ†æ®µ {segment_result['segment_number']} å“åº”ä¸­ç¼ºå°‘ question_scores")
                return

            # å¤„ç†æ¯ä¸ªé—®é¢˜çš„è¯„åˆ†
            for question_score in response_data['question_scores']:
                question_id = question_score.get('question_id', f'Q{len(self.per_question_scores)+1}')
                big_five_scores = question_score.get('big_five_scores', {})

                # ç´¯ç§¯æ¯ä¸ªç‰¹è´¨çš„è¯„åˆ†
                for trait, score_data in big_five_scores.items():
                    if trait in self.big_five_traits:
                        score = score_data.get('score', 3)  # é»˜è®¤3åˆ†
                        evidence = score_data.get('evidence', '')
                        quality = score_data.get('quality', 'inferred')

                        # éªŒè¯è¯„åˆ†æ˜¯å¦ä¸º1ã€3ã€5
                        if score not in [1, 3, 5]:
                            print(f"  è­¦å‘Š: {trait} è¯„åˆ† {score} ä¸æ˜¯1ã€3ã€5ï¼Œå·²ä¿®æ­£ä¸º3")
                            score = 3

                        self.big_five_traits[trait]['scores'].append(score)
                        self.big_five_traits[trait]['evidence'].append(evidence)
                        self.big_five_traits[trait]['weight'] += 1

                # è®°å½•æ¯ä¸ªé—®é¢˜çš„è¯„åˆ†
                self.per_question_scores.append({
                    'question_id': question_id,
                    'big_five_scores': big_five_scores.copy(),
                    'segment_number': segment_result['segment_number']
                })

            print(f"  æˆåŠŸç´¯ç§¯åˆ†æ®µ {segment_result['segment_number']} çš„è¯„åˆ†")

        except Exception as e:
            print(f"  è§£æåˆ†æ®µ {segment_result.get('segment_number', 'Unknown')} ç»“æœå¤±è´¥: {e}")

    def calculate_final_scores(self) -> Dict:
        """è®¡ç®—æœ€ç»ˆçš„Big5è¯„åˆ†"""
        final_scores = {}

        for trait, data in self.big_five_traits.items():
            scores = data['scores']
            weight = data['weight']

            if weight > 0:
                # è®¡ç®—å¹³å‡åˆ†ï¼Œç„¶åå››èˆäº”å…¥åˆ°1ã€3ã€5
                avg_score = sum(scores) / len(scores)

                # å››èˆäº”å…¥åˆ°æœ€æ¥è¿‘çš„1ã€3ã€5
                if avg_score <= 2:
                    final_score = 1
                elif avg_score <= 4:
                    final_score = 3
                else:
                    final_score = 5

                final_scores[trait] = {
                    'final_score': final_score,
                    'average_score': round(avg_score, 2),
                    'raw_scores': scores.copy(),
                    'evidence_count': len(data['evidence']),
                    'weight': weight,
                    'evidence_samples': data['evidence'][:3]  # å‰3ä¸ªè¯æ®æ ·æœ¬
                }
            else:
                final_scores[trait] = {
                    'final_score': 3,  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
                    'average_score': 3.0,
                    'raw_scores': [],
                    'evidence_count': 0,
                    'weight': 0,
                    'evidence_samples': []
                }

        return final_scores

    def generate_mbti_type(self, final_scores: Dict) -> Dict:
        """åŸºäºBig5è¯„åˆ†ç”ŸæˆMBTIç±»å‹"""
        O = final_scores.get('openness_to_experience', {}).get('final_score', 3)
        C = final_scores.get('conscientiousness', {}).get('final_score', 3)
        E = final_scores.get('extraversion', {}).get('final_score', 3)
        A = final_scores.get('agreeableness', {}).get('final_score', 3)
        N = final_scores.get('neuroticism', {}).get('final_score', 3)

        # MBTIè®¡ç®—é€»è¾‘
        # E/I: å¤–å‘æ€§ vs ç¥ç»è´¨
        e_score = E + (5 - N)  # é«˜å¤–å‘æ€§+ä½ç¥ç»è´¨=æ›´å¤–å‘
        i_score = (5 - E) + N
        E_preference = 'E' if e_score > i_score else 'I'

        # S/N: æ„Ÿè§‰ vs ç›´è§‰ (åŸºäºå¼€æ”¾æ€§)
        S_preference = 'S' if O <= 3 else 'N'

        # T/F: æ€è€ƒ vs æƒ…æ„Ÿ (åŸºäºå®œäººæ€§)
        T_preference = 'T' if A <= 3 else 'F'

        # J/P: åˆ¤æ–­ vs æ„ŸçŸ¥ (åŸºäºå°½è´£æ€§)
        J_preference = 'J' if C > 3 else 'P'

        mbti_type = f"{E_preference}{S_preference}{T_preference}{J_preference}"

        return {
            'type': mbti_type,
            'scores': {'E/I': e_score, 'S/N': O, 'T/F': A, 'J/P': C},
            'preferences': {
                'extraversion_introversion': E_preference,
                'sensing_intuition': S_preference,
                'thinking_feeling': T_preference,
                'judging_perceiving': J_preference
            }
        }

    def analyze_full_assessment(self, assessment_file: str) -> Dict:
        """åˆ†æå®Œæ•´çš„è¯„ä¼°æ–‡ä»¶"""
        print(f"ğŸ” å¼€å§‹åˆ†æ: {assessment_file}")

        # åŠ è½½è¯„ä¼°æ•°æ®
        with open(assessment_file, 'r', encoding='utf-8') as f:
            assessment_data = json.load(f)

        # æå–é—®é¢˜
        questions = self.extract_questions(assessment_data)
        if not questions:
            raise ValueError("æ— æ³•ä»è¯„ä¼°æ–‡ä»¶ä¸­æå–é—®é¢˜")

        print(f"ğŸ“Š æå–åˆ° {len(questions)} ä¸ªé—®é¢˜")

        # åˆ›å»ºåˆ†æ®µ
        segments = self.create_segments(questions)

        # åˆ†ææ¯ä¸ªåˆ†æ®µ
        segment_results = []
        for i, segment in enumerate(segments, 1):
            print(f"ğŸ“ åˆ†æåˆ†æ®µ {i}/{len(segments)}...")
            result = self.analyze_segment(segment, i)
            segment_results.append(result)

            # ç´¯ç§¯è¯„åˆ†
            self.accumulate_scores(result)

            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            time.sleep(1)

        # è®¡ç®—æœ€ç»ˆè¯„åˆ†
        final_scores = self.calculate_final_scores()
        mbti_result = self.generate_mbti_type(final_scores)

        # ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š
        analysis_report = {
            'file_info': {
                'filename': Path(assessment_file).name,
                'total_questions': len(questions),
                'segments_count': len(segments),
                'questions_per_segment': self.max_questions_per_segment,
                'model_used': self.model
            },
            'big_five_final_scores': final_scores,
            'mbti_assessment': mbti_result,
            'detailed_analysis': {
                'per_question_scores': self.per_question_scores,
                'segment_results': segment_results,
                'analysis_log': self.analysis_log
            },
            'metadata': {
                'analysis_timestamp': datetime.now().isoformat(),
                'scoring_standard': '1-3-5 scale (1=Low, 3=Moderate, 5=High)',
                'analysis_method': 'Segmented cumulative analysis'
            }
        }

        print(f"âœ… åˆ†æå®Œæˆ: {Path(assessment_file).name}")
        return analysis_report

def main():
    """ä¸»å‡½æ•° - æµ‹è¯•äº‘è¯„ä¼°å™¨åˆ†æ®µåˆ†æ"""
    import argparse

    parser = argparse.ArgumentParser(description='äº‘è¯„ä¼°å™¨åˆ†æ®µå¼Big5åˆ†æ')
    parser.add_argument('input_file', help='è¾“å…¥è¯„ä¼°æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', default='qwen-long', choices=['qwen-long', 'qwen-max'],
                       help='ä½¿ç”¨çš„äº‘æ¨¡å‹')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')

    args = parser.parse_args()

    # åˆ›å»ºåˆ†æå™¨
    analyzer = CloudSegmentedPersonalityAnalyzer(model=args.model)

    try:
        # æ‰§è¡Œåˆ†æ
        result = analyzer.analyze_full_assessment(args.input_file)

        # ä¿å­˜ç»“æœ
        output_file = args.output or f"{Path(args.input_file).stem}_{args.model}_segmented_analysis.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"ğŸ“„ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_file}")

        # æ˜¾ç¤ºæ‘˜è¦
        print("\nğŸ¯ Big5æœ€ç»ˆè¯„åˆ† (1-3-5åˆ¶):")
        for trait, data in result['big_five_final_scores'].items():
            score = data['final_score']
            weight = data['weight']
            print(f"  {trait}: {score}/5 (åŸºäº{weight}ä¸ªè¯æ®)")

        print(f"\nğŸ§  MBTIç±»å‹: {result['mbti_assessment']['type']}")

    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()