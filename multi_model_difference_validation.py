#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤šæ¨¡å‹å·®å¼‚éªŒè¯åˆ†æ
é‡ç‚¹æ£€æŸ¥å·®å¼‚æœ€å¤§çš„é¢˜ç›®åœ¨ä¸‰ä¸ªäº‘è¯„ä¼°å™¨ä¹‹é—´çš„ä¸€è‡´æ€§
å¯¹æ¯”2é¢˜åˆ†æ®µvs5é¢˜åˆ†æ®µæ–¹æ¡ˆçš„è¯„åˆ†ä¾æ®
"""

import sys
import os
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
import statistics

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['DASHSCOPE_API_KEY'] = 'sk-ded837735b3c44599a9bc138da561c27'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class MultiModelDifferenceValidator:
    def __init__(self):
        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

        # ä¸‰ä¸ªæ¨¡å‹é…ç½®
        self.models = [
            {"name": "qwen-long", "description": "é€šä¹‰åƒé—®é•¿æ–‡æœ¬æ¨¡å‹"},
            {"name": "qwen-max", "description": "é€šä¹‰åƒé—®æœ€å¼ºæ¨¡å‹"},
            {"name": "qwen-turbo", "description": "é€šä¹‰åƒé—®å¿«é€Ÿæ¨¡å‹"}
        ]

        # ä»é€é¢˜åˆ†æä¸­è¯†åˆ«çš„å·®å¼‚æœ€å¤§çš„é¢˜ç›®
        self.most_different_questions = [25, 9, 1, 10, 3, 4, 15, 18, 20, 36]  # å‰10ä¸ªæœ€ä¸ä¸€è‡´çš„é¢˜ç›®

    def load_test_data(self, data_file: str) -> Dict:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“‹ åŠ è½½æµ‹è¯„æ•°æ®: {data_file}")

        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            questions = {}
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for i, item in enumerate(data['assessment_results']):
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', '')
                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']

                            if question_text and answer_text:
                                questions[i + 1] = {  # 1-based indexing
                                    'question': question_text,
                                    'answer': answer_text,
                                    'question_index': i + 1,
                                    'original_segment_2': (i // 2) + 1,
                                    'original_segment_5': (i // 5) + 1
                                }

            print(f"  ğŸ“Š æˆåŠŸæå– {len(questions)} ä¸ªé—®é¢˜")
            return questions

        except Exception as e:
            print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return {}

    def create_analysis_prompt(self, question: Dict, segment_size: int, segment_context: str = "") -> str:
        """åˆ›å»ºåˆ†ææç¤º"""
        context_info = ""
        if segment_context:
            context_info = f"\n**åˆ†æ®µä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š**\n{segment_context}"

        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚åˆ†æä»¥ä¸‹{segment_size}é¢˜åˆ†æ®µçš„é—®å·å›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ã€‚

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- 1åˆ†ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- 3åˆ†ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- 5åˆ†ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼**

{context_info}

é—®é¢˜ {question['question_index']}:
{question['question']}

å›ç­”:
{question['answer']}

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "success": true,
  "analysis_summary": "ç®€è¦åˆ†æè¯´æ˜å’Œè¯„åˆ†ä¾æ®",
  "scores": {{
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  }},
  "evidence": {{
    "openness_to_experience": "å…·ä½“è¯„åˆ†ä¾æ®",
    "conscientiousness": "å…·ä½“è¯„åˆ†ä¾æ®",
    "extraversion": "å…·ä½“è¯„åˆ†ä¾æ®",
    "agreeableness": "å…·ä½“è¯„åˆ†ä¾æ®",
    "neuroticism": "å…·ä½“è¯„åˆ†ä¾æ®"
  }},
  "confidence": "high/medium/low"
}}
"""
        return prompt

    def analyze_question_with_model(self, model_config: Dict, question: Dict, segment_size: int, segment_context: str = "") -> Dict:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æå•ä¸ªé—®é¢˜"""
        try:
            import openai
            client = openai.OpenAI(
                api_key=self.api_key,
                base_url=self.base_url
            )

            prompt = self.create_analysis_prompt(question, segment_size, segment_context)

            response = client.chat.completions.create(
                model=model_config['name'],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ï¼Œå¹¶æä¾›è¯¦ç»†çš„è¯„åˆ†ä¾æ®ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.1
            )

            content = response.choices[0].message.content

            # è§£æJSON
            try:
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                    result = json.loads(json_str)
                else:
                    result = json.loads(content)
            except json.JSONDecodeError:
                return {
                    'success': False,
                    'model': model_config['name'],
                    'error': 'JSONè§£æå¤±è´¥',
                    'raw_response': content[:500]
                }

            # éªŒè¯è¯„åˆ†æ ‡å‡†
            if 'scores' in result and result['scores']:
                invalid_scores = []
                for trait, score in result['scores'].items():
                    if score not in [1, 3, 5]:
                        invalid_scores.append(f"{trait}:{score}")
                        # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                        if score < 2:
                            result['scores'][trait] = 1
                        elif score > 4:
                            result['scores'][trait] = 5
                        else:
                            result['scores'][trait] = 3

                if invalid_scores:
                    print(f"      âš ï¸ å‘ç°å¹¶ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")

            result['model'] = model_config['name']
            result['model_description'] = model_config['description']
            result['question_index'] = question['question_index']
            result['segment_size'] = segment_size

            return result

        except Exception as e:
            return {
                'success': False,
                'model': model_config['name'],
                'error': str(e),
                'question_index': question['question_index']
            }

    def get_segment_context(self, questions: Dict, question_index: int, segment_size: int) -> str:
        """è·å–åˆ†æ®µä¸Šä¸‹æ–‡ä¿¡æ¯"""
        segment_num = (question_index - 1) // segment_size + 1
        start_idx = (segment_num - 1) * segment_size + 1
        end_idx = min(segment_num * segment_size, len(questions))

        context_questions = []
        for i in range(start_idx, end_idx + 1):
            if i in questions:
                context_questions.append(f"é—®é¢˜{i}: {questions[i]['question'][:100]}...")

        return "\n".join(context_questions)

    def multi_model_validate_differences(self, questions: Dict) -> Dict:
        """å¤šæ¨¡å‹éªŒè¯å·®å¼‚é¢˜ç›®"""
        print(f"ğŸ”¬ å¤šæ¨¡å‹å·®å¼‚éªŒè¯åˆ†æ")
        print(f"ğŸ“‹ é‡ç‚¹éªŒè¯é¢˜ç›®: {self.most_different_questions}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {[m['name'] for m in self.models]}")
        print("=" * 60)

        validation_results = {}

        for question_index in self.most_different_questions:
            if question_index not in questions:
                print(f"  âŒ é¢˜{question_index} æ•°æ®ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue

            question = questions[question_index]
            print(f"\nğŸ“ éªŒè¯é¢˜ {question_index} (å·®å¼‚é¢˜ç›®)")
            print("-" * 40)

            # è·å–2é¢˜åˆ†æ®µä¸Šä¸‹æ–‡
            context_2seg = self.get_segment_context(questions, question_index, 2)
            # è·å–5é¢˜åˆ†æ®µä¸Šä¸‹æ–‡
            context_5seg = self.get_segment_context(questions, question_index, 5)

            question_results = {
                'question_index': question_index,
                'question_text': question['question'][:100] + "...",
                'answer_preview': question['answer'][:100] + "...",
                'segment_2': question['original_segment_2'],
                'segment_5': question['original_segment_5'],
                'models_2segment': {},
                'models_5segment': {}
            }

            # 2é¢˜åˆ†æ®µå¤šæ¨¡å‹åˆ†æ
            print(f"  ğŸ” 2é¢˜åˆ†æ®µå¤šæ¨¡å‹åˆ†æ...")
            for model_config in self.models:
                print(f"    åˆ†ææ¨¡å‹: {model_config['name']}...")
                result = self.analyze_question_with_model(model_config, question, 2, context_2seg)
                question_results['models_2segment'][model_config['name']] = result
                time.sleep(1)  # APIé™åˆ¶

            # 5é¢˜åˆ†æ®µå¤šæ¨¡å‹åˆ†æ
            print(f"  ğŸ” 5é¢˜åˆ†æ®µå¤šæ¨¡å‹åˆ†æ...")
            for model_config in self.models:
                print(f"    åˆ†ææ¨¡å‹: {model_config['name']}...")
                result = self.analyze_question_with_model(model_config, question, 5, context_5seg)
                question_results['models_5segment'][model_config['name']] = result
                time.sleep(1)  # APIé™åˆ¶

            validation_results[question_index] = question_results

        return validation_results

    def analyze_model_consistency(self, validation_results: Dict) -> Dict:
        """åˆ†ææ¨¡å‹é—´ä¸€è‡´æ€§"""
        print(f"\nğŸ“Š æ¨¡å‹ä¸€è‡´æ€§åˆ†æ")
        print("=" * 50)

        consistency_analysis = {
            'questions': {},
            'segment_2_analysis': {},
            'segment_5_analysis': {},
            'cross_segment_comparison': {}
        }

        # åˆ†ææ¯ä¸ªé¢˜ç›®çš„æ¨¡å‹ä¸€è‡´æ€§
        for question_index, results in validation_results.items():
            print(f"\nğŸ“‹ é¢˜{question_index} ä¸€è‡´æ€§åˆ†æ:")

            # 2é¢˜åˆ†æ®µæ¨¡å‹ä¸€è‡´æ€§
            seg_2_models = {}
            for model_name, result in results['models_2segment'].items():
                if result['success'] and 'scores' in result:
                    seg_2_models[model_name] = result['scores']

            # 5é¢˜åˆ†æ®µæ¨¡å‹ä¸€è‡´æ€§
            seg_5_models = {}
            for model_name, result in results['models_5segment'].items():
                if result['success'] and 'scores' in result:
                    seg_5_models[model_name] = result['scores']

            question_consistency = {
                'segment_2_models': seg_2_models,
                'segment_5_models': seg_5_models,
                'segment_2_consistency': self._calculate_model_consistency(seg_2_models),
                'segment_5_consistency': self._calculate_model_consistency(seg_5_models),
                'cross_segment_consistency': self._calculate_cross_segment_consistency(seg_2_models, seg_5_models)
            }

            # æ˜¾ç¤ºä¸€è‡´æ€§ç»“æœ
            print(f"  2é¢˜åˆ†æ®µä¸€è‡´æ€§: {question_consistency['segment_2_consistency']['consistency_rate']:.1f}%")
            print(f"  5é¢˜åˆ†æ®µä¸€è‡´æ€§: {question_consistency['segment_5_consistency']['consistency_rate']:.1f}%")
            print(f"  è·¨åˆ†æ®µä¸€è‡´æ€§: {question_consistency['cross_segment_consistency']['consistency_rate']:.1f}%")

            consistency_analysis['questions'][question_index] = question_consistency

        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§
        all_seg2_consistency = [q['segment_2_consistency']['consistency_rate'] for q in consistency_analysis['questions'].values()]
        all_seg5_consistency = [q['segment_5_consistency']['consistency_rate'] for q in consistency_analysis['questions'].values()]
        all_cross_consistency = [q['cross_segment_consistency']['consistency_rate'] for q in consistency_analysis['questions'].values()]

        consistency_analysis['segment_2_analysis'] = {
            'average_consistency': statistics.mean(all_seg2_consistency),
            'consistency_scores': all_seg2_consistency,
            'min_consistency': min(all_seg2_consistency),
            'max_consistency': max(all_seg2_consistency)
        }

        consistency_analysis['segment_5_analysis'] = {
            'average_consistency': statistics.mean(all_seg5_consistency),
            'consistency_scores': all_seg5_consistency,
            'min_consistency': min(all_seg5_consistency),
            'max_consistency': max(all_seg5_consistency)
        }

        consistency_analysis['cross_segment_comparison'] = {
            'average_consistency': statistics.mean(all_cross_consistency),
            'consistency_scores': all_cross_consistency,
            'min_consistency': min(all_cross_consistency),
            'max_consistency': max(all_cross_consistency)
        }

        print(f"\nğŸ¯ æ€»ä½“ä¸€è‡´æ€§ç»Ÿè®¡:")
        print(f"  2é¢˜åˆ†æ®µæ¨¡å‹ä¸€è‡´æ€§: {consistency_analysis['segment_2_analysis']['average_consistency']:.1f}%")
        print(f"  5é¢˜åˆ†æ®µæ¨¡å‹ä¸€è‡´æ€§: {consistency_analysis['segment_5_analysis']['average_consistency']:.1f}%")
        print(f"  è·¨åˆ†æ®µä¸€è‡´æ€§: {consistency_analysis['cross_segment_comparison']['average_consistency']:.1f}%")

        return consistency_analysis

    def _calculate_model_consistency(self, model_scores: Dict) -> Dict:
        """è®¡ç®—æ¨¡å‹é—´ä¸€è‡´æ€§"""
        if len(model_scores) < 2:
            return {'consistency_rate': 0, 'details': {}}

        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        model_names = list(model_scores.keys())

        consistency_details = {}
        total_consistency = 0
        total_traits = len(traits)

        for trait in traits:
            scores = [model_scores[model][trait] for model in model_names if trait in model_scores[model]]
            if len(scores) >= 2:
                unique_scores = set(scores)
                consistency_details[trait] = {
                    'scores': scores,
                    'unique_count': len(unique_scores),
                    'all_same': len(unique_scores) == 1,
                    'model_names': model_names
                }
                if consistency_details[trait]['all_same']:
                    total_consistency += 1

        consistency_rate = (total_consistency / total_traits) * 100 if total_traits > 0 else 0

        return {
            'consistency_rate': consistency_rate,
            'details': consistency_details,
            'total_consistent_traits': total_consistency,
            'total_traits': total_traits
        }

    def _calculate_cross_segment_consistency(self, seg_2_models: Dict, seg_5_models: Dict) -> Dict:
        """è®¡ç®—è·¨åˆ†æ®µä¸€è‡´æ€§"""
        # å–å„æ¨¡å‹åœ¨ä¸åŒåˆ†æ®µä¸‹çš„å¹³å‡å€¼è¿›è¡Œå¯¹æ¯”
        common_models = set(seg_2_models.keys()) & set(seg_5_models.keys())

        if len(common_models) < 1:
            return {'consistency_rate': 0, 'details': {}}

        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']
        cross_consistency = {}
        total_consistency = 0

        for trait in traits:
            seg_2_scores = []
            seg_5_scores = []

            for model in common_models:
                if trait in seg_2_models[model]:
                    seg_2_scores.append(seg_2_models[model][trait])
                if trait in seg_5_models[model]:
                    seg_5_scores.append(seg_5_models[model][trait])

            if seg_2_scores and seg_5_scores:
                avg_2 = statistics.mean(seg_2_scores)
                avg_5 = statistics.mean(seg_5_scores)
                diff = abs(avg_2 - avg_5)

                cross_consistency[trait] = {
                    'segment_2_avg': avg_2,
                    'segment_5_avg': avg_5,
                    'difference': diff,
                    'consistent': diff < 1  # å…è®¸0.5çš„è¯¯å·®
                }

                if cross_consistency[trait]['consistent']:
                    total_consistency += 1

        consistency_rate = (total_consistency / len(traits)) * 100 if traits else 0

        return {
            'consistency_rate': consistency_rate,
            'details': cross_consistency,
            'total_consistent_traits': total_consistency,
            'total_traits': len(traits)
        }

    def analyze_scoring_rationale(self, validation_results: Dict) -> Dict:
        """åˆ†æè¯„åˆ†ä¾æ®"""
        print(f"\nğŸ“ è¯„åˆ†ä¾æ®åˆ†æ")
        print("=" * 50)

        rationale_analysis = {
            'questions': {},
            'common_rationale_patterns': {},
            'segment_comparison': {}
        }

        # æå–è¯„åˆ†ä¾æ®
        common_evidence = {
            'openness_to_experience': [],
            'conscientiousness': [],
            'extraversion': [],
            'agreeableness': [],
            'neuroticism': []
        }

        for question_index, results in validation_results.items():
            print(f"\nğŸ“‹ é¢˜{question_index} è¯„åˆ†ä¾æ®:")

            question_rationale = {
                'segment_2_rationales': {},
                'segment_5_rationales': {},
                'rationale_comparison': {}
            }

            # æå–2é¢˜åˆ†æ®µè¯„åˆ†ä¾æ®
            for model_name, result in results['models_2segment'].items():
                if result['success'] and 'evidence' in result:
                    question_rationale['segment_2_rationales'][model_name] = result['evidence']
                    # æ”¶é›†è¯æ®æ¨¡å¼
                    for trait, evidence in result['evidence'].items():
                        if evidence:
                            common_evidence[trait].append(f"é¢˜{question_index}-2-{model_name}: {evidence}")

            # æå–5é¢˜åˆ†æ®µè¯„åˆ†ä¾æ®
            for model_name, result in results['models_5segment'].items():
                if result['success'] and 'evidence' in result:
                    question_rationale['segment_5_rationales'][model_name] = result['evidence']
                    # æ”¶é›†è¯æ®æ¨¡å¼
                    for trait, evidence in result['evidence'].items():
                        if evidence:
                            common_evidence[trait].append(f"é¢˜{question_index}-5-{model_name}: {evidence}")

            # åˆ†æè¯„åˆ†ä¾æ®å·®å¼‚
            rationale_analysis['questions'][question_index] = question_rationale

        # åˆ†æå¸¸è§çš„è¯„åˆ†ä¾æ®æ¨¡å¼
        print(f"\nğŸ” è¯„åˆ†ä¾æ®æ¨¡å¼åˆ†æ:")
        for trait, evidence_list in common_evidence.items():
            print(f"\n{trait} ç‰¹è´¨çš„è¯„åˆ†ä¾æ®æ¨¡å¼:")

            # ç»Ÿè®¡å¸¸è§å…³é”®è¯
            evidence_keywords = {}
            for evidence in evidence_list:
                words = evidence.lower().split()
                for word in words:
                    if len(word) > 2:  # è¿‡æ»¤çŸ­è¯
                        evidence_keywords[word] = evidence_keywords.get(word, 0) + 1

            # æ˜¾ç¤ºé«˜é¢‘å…³é”®è¯
            top_keywords = sorted(evidence_keywords.items(), key=lambda x: x[1], reverse=True)[:5]
            for word, count in top_keywords:
                print(f"  '{word}': {count}æ¬¡")

        return rationale_analysis

    def generate_comprehensive_report(self, validation_results: Dict, consistency_analysis: Dict, rationale_analysis: Dict) -> Dict:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        print(f"\nğŸ“„ ç”Ÿæˆç»¼åˆéªŒè¯æŠ¥å‘Š")
        print("=" * 60)

        # ç”Ÿæˆç»“è®º
        avg_seg2_consistency = consistency_analysis['segment_2_analysis']['average_consistency']
        avg_seg5_consistency = consistency_analysis['segment_5_analysis']['average_consistency']
        avg_cross_consistency = consistency_analysis['cross_segment_comparison']['average_consistency']

        # è¯„ä¼°å“ªä¸ªæ–¹æ¡ˆæ›´å¯ä¿¡
        if avg_seg5_consistency > avg_seg2_consistency:
            more_reliable = "5é¢˜åˆ†æ®µ"
            reliability_advantage = (avg_seg5_consistency - avg_seg2_consistency)
            recommendation = f"âœ… 5é¢˜åˆ†æ®µæ–¹æ¡ˆæ›´å¯ä¿¡ï¼Œä¸€è‡´æ€§é«˜å‡º{reliability_advantage:.1f}%"
        else:
            more_reliable = "2é¢˜åˆ†æ®µ"
            reliability_advantage = (avg_seg2_consistency - avg_seg5_consistency)
            recommendation = f"âœ… 2é¢˜åˆ†æ®µæ–¹æ¡ˆæ›´å¯ä¿¡ï¼Œä¸€è‡´æ€§é«˜å‡º{reliability_advantage:.1f}%"

        print(f"\nğŸ† ç»¼åˆè¯„ä¼°ç»“è®º:")
        print(f"  ğŸ“Š æ›´å¯ä¿¡æ–¹æ¡ˆ: {more_reliable}")
        print(f"  ğŸ“ˆ ä¸€è‡´æ€§ä¼˜åŠ¿: {reliability_advantage:.1f}%")
        print(f"  ğŸ’¡ å»ºè®®: {recommendation}")

        # è¯¦ç»†è¯„ä¼°æ ‡å‡†
        if avg_cross_consistency >= 90:
            overall_rating = "ä¼˜ç§€"
            overall_recommendation = "âœ… ä¸¤ç§æ–¹æ¡ˆé«˜åº¦ä¸€è‡´ï¼Œå¯ä»»æ„é€‰æ‹©"
        elif avg_cross_consistency >= 80:
            overall_rating = "è‰¯å¥½"
            overall_recommendation = "âœ… ä¸¤ç§æ–¹æ¡ˆåŸºæœ¬ä¸€è‡´ï¼Œæ¨èä½¿ç”¨æ›´ä¸€è‡´æ–¹æ¡ˆ"
        elif avg_cross_consistency >= 70:
            overall_rating = "ä¸­ç­‰"
            overall_recommendation = "âš ï¸ å­˜åœ¨ä¸€å®šå·®å¼‚ï¼Œéœ€è¦è°¨æ…é€‰æ‹©"
        else:
            overall_rating = "éœ€è¦æ”¹è¿›"
            overall_recommendation = "âŒ å·®å¼‚è¾ƒå¤§ï¼Œéœ€è¦é‡æ–°è¯„ä¼°"

        print(f"\nğŸ¯ æ€»ä½“è¯„çº§: {overall_rating}")
        print(f"ğŸ“‹ æ€»ä½“å»ºè®®: {overall_recommendation}")

        # ä¿å­˜æŠ¥å‘Š
        comprehensive_report = {
            "validation_info": {
                "validation_date": datetime.now().isoformat(),
                "focus_questions": self.most_different_questions,
                "models_tested": [m['name'] for m in self.models],
                "total_questions_analyzed": len(validation_results)
            },
            "model_consistency_analysis": consistency_analysis,
            "rationale_analysis": rationale_analysis,
            "comprehensive_assessment": {
                "more_reliable_approach": more_reliable,
                "reliability_advantage": reliability_advantage,
                "overall_rating": overall_rating,
                "overall_recommendation": overall_recommendation,
                "segment_2_avg_consistency": avg_seg2_consistency,
                "segment_5_avg_consistency": avg_seg5_consistency,
                "cross_segment_avg_consistency": avg_cross_consistency
            },
            "question_details": validation_results
        }

        with open("multi_model_difference_validation_report.json", 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ ç»¼åˆéªŒè¯æŠ¥å‘Šå·²ä¿å­˜: multi_model_difference_validation_report.json")

        return comprehensive_report

    def run_multi_model_validation(self, data_file: str) -> Dict:
        """è¿è¡Œå¤šæ¨¡å‹å·®å¼‚éªŒè¯"""
        print("ğŸš€ å¼€å§‹å¤šæ¨¡å‹å·®å¼‚éªŒè¯åˆ†æ")
        print("=" * 70)
        print(f"ğŸ“‹ ç›®æ ‡: éªŒè¯å·®å¼‚é¢˜ç›®åœ¨å¤šä¸ªæ¨¡å‹é—´çš„ä¸€è‡´æ€§")
        print(f"ğŸ” é‡ç‚¹: åˆ†æ2é¢˜vs5é¢˜åˆ†æ®µæ–¹æ¡ˆçš„è¯„åˆ†ä¾æ®")
        print(f"ğŸ¤– æ¨¡å‹: {[m['name'] for m in self.models]}")

        # 1. åŠ è½½æ•°æ®
        questions = self.load_test_data(data_file)
        if not questions:
            return {'success': False, 'error': 'æ•°æ®åŠ è½½å¤±è´¥'}

        # 2. å¤šæ¨¡å‹éªŒè¯å·®å¼‚é¢˜ç›®
        validation_results = self.multi_model_validate_differences(questions)

        # 3. åˆ†ææ¨¡å‹ä¸€è‡´æ€§
        consistency_analysis = self.analyze_model_consistency(validation_results)

        # 4. åˆ†æè¯„åˆ†ä¾æ®
        rationale_analysis = self.analyze_scoring_rationale(validation_results)

        # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        comprehensive_report = self.generate_comprehensive_report(
            validation_results, consistency_analysis, rationale_analysis
        )

        return {
            'success': True,
            'comprehensive_report': comprehensive_report
        }

def main():
    """ä¸»å‡½æ•°"""
    validator = MultiModelDifferenceValidator()

    # é€‰æ‹©æµ‹è¯•æ–‡ä»¶
    data_file = "results/results/asses_deepseek_r1_70b_agent_big_five_50_complete2_a10_e0_t0_0_09271.json"

    print(f"ğŸ¯ é€‰æ‹©æµ‹è¯•æ–‡ä»¶:")
    print(f"  æ•°æ®æ–‡ä»¶: {data_file}")

    # æ‰§è¡Œå¤šæ¨¡å‹éªŒè¯
    result = validator.run_multi_model_validation(data_file)

    if result['success']:
        report = result['comprehensive_report']
        print(f"\nğŸ‰ å¤šæ¨¡å‹å·®å¼‚éªŒè¯å®Œæˆ!")
        print(f"  ğŸ† æ›´å¯ä¿¡æ–¹æ¡ˆ: {report['comprehensive_assessment']['more_reliable_approach']}")
        print(f"  ğŸ“Š ä¸€è‡´æ€§ä¼˜åŠ¿: {report['comprehensive_assessment']['reliability_advantage']:.1f}%")
        print(f"  ğŸ¯ æ€»ä½“è¯„çº§: {report['comprehensive_assessment']['overall_rating']}")
        print(f"  ğŸ’¡ å»ºè®®: {report['comprehensive_assessment']['overall_recommendation']}")
    else:
        print(f"\nâŒ å¤šæ¨¡å‹éªŒè¯å¤±è´¥: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()