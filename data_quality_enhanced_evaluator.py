#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡å¢å¼ºè¯„ä¼°å™¨ - è®¾ç½®90%æœ€ä½æˆåŠŸç‡é˜ˆå€¼
"""

import json
import os
import sys
import subprocess
import threading
import time
import statistics
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Tuple, Optional

class DataQualityEnhancedEvaluator:
    def __init__(self, min_success_rate=0.9):
        """
        åˆå§‹åŒ–æ•°æ®è´¨é‡å¢å¼ºè¯„ä¼°å™¨

        Args:
            min_success_rate: æœ€ä½æˆåŠŸç‡é˜ˆå€¼ï¼Œé»˜è®¤90%
        """
        self.min_success_rate = min_success_rate
        self.quality_metrics = {
            'total_evaluations': 0,
            'passed_quality_threshold': 0,
            'failed_quality_threshold': 0,
            'average_success_rate': 0.0,
            'quality_distribution': {'high': 0, 'medium': 0, 'low': 0}
        }

    def calculate_quality_score(self, success_rate: float, consistency_score: float = 0) -> float:
        """
        è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°

        Args:
            success_rate: å®é™…æˆåŠŸç‡ (0-1)
            consistency_score: ä¸€è‡´æ€§åˆ†æ•° (0-100)

        Returns:
            è´¨é‡åˆ†æ•° (0-100)
        """
        # æˆåŠŸç‡æƒé‡70%ï¼Œä¸€è‡´æ€§æƒé‡30%
        quality_score = (success_rate * 70) + (consistency_score * 0.3)
        return round(quality_score, 1)

    def meets_quality_threshold(self, success_rate: float) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æ»¡è¶³è´¨é‡é˜ˆå€¼

        Args:
            success_rate: æˆåŠŸç‡ (0-1)

        Returns:
            æ˜¯å¦æ»¡è¶³è´¨é‡è¦æ±‚
        """
        return success_rate >= self.min_success_rate

    def evaluate_data_quality(self, results: Dict) -> Dict:
        """
        è¯„ä¼°æ•°æ®è´¨é‡

        Args:
            results: è¯„ä¼°ç»“æœ

        Returns:
            è´¨é‡è¯„ä¼°ç»“æœ
        """
        quality_assessment = {
            'meets_threshold': False,
            'success_rate': 0.0,
            'quality_score': 0.0,
            'quality_level': 'low',
            'recommendations': [],
            'detailed_analysis': {}
        }

        # è®¡ç®—æˆåŠŸç‡
        if 'model_results' in results:
            total_segments = 0
            successful_segments = 0

            for model_name, model_result in results['model_results'].items():
                if 'success_rate' in model_result:
                    total_segments += 1
                    successful_segments += model_result['success_rate']

            if total_segments > 0:
                avg_success_rate = successful_segments / total_segments
                quality_assessment['success_rate'] = avg_success_rate
                quality_assessment['meets_threshold'] = self.meets_quality_threshold(avg_success_rate)

                # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°
                consistency_score = results.get('consistency_analysis', {}).get('confidence_score', 0)
                quality_assessment['quality_score'] = self.calculate_quality_score(avg_success_rate, consistency_score)

                # ç¡®å®šè´¨é‡ç­‰çº§
                if avg_success_rate >= 0.95:
                    quality_assessment['quality_level'] = 'high'
                elif avg_success_rate >= 0.9:
                    quality_assessment['quality_level'] = 'medium'
                else:
                    quality_assessment['quality_level'] = 'low'

                # ç”Ÿæˆå»ºè®®
                if avg_success_rate < 0.9:
                    quality_assessment['recommendations'].append("æˆåŠŸç‡ä½äº90%ï¼Œå»ºè®®é‡æ–°è¯„ä¼°")
                if avg_success_rate < 0.7:
                    quality_assessment['recommendations'].append("æˆåŠŸç‡è¿‡ä½ï¼Œç»“æœä¸å¯ä¿¡")
                if consistency_score < 50:
                    quality_assessment['recommendations'].append("æ¨¡å‹ä¸€è‡´æ€§ä¸è¶³")

                quality_assessment['detailed_analysis'] = {
                    'total_models_evaluated': total_segments,
                    'segments_analyzed': results.get('total_segments', 0),
                    'quality_threshold_met': quality_assessment['meets_threshold'],
                    'gap_to_threshold': max(0, self.min_success_rate - avg_success_rate)
                }

        return quality_assessment

class EnhancedOllamaEvaluator(DataQualityEnhancedEvaluator):
    def __init__(self, models: List[str], min_success_rate=0.9):
        super().__init__(min_success_rate)
        self.models = models
        self.olllama_base = "http://localhost:11434"

    def analyze_with_quality_control(self, input_file: str, output_dir: str) -> Dict:
        """
        å¸¦è´¨é‡æ§åˆ¶çš„åˆ†æ

        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            åˆ†æç»“æœ
        """
        print(f"ğŸ” å¼€å§‹è´¨é‡æ§åˆ¶åˆ†æ: {os.path.basename(input_file)}")

        # é¦–å…ˆè¿›è¡Œæ ‡å‡†åˆ†æ
        standard_result = self._standard_analysis(input_file, output_dir)

        # ç„¶åè¿›è¡Œè´¨é‡è¯„ä¼°
        quality_assessment = self.evaluate_data_quality(standard_result)

        # æ›´æ–°è´¨é‡ç»Ÿè®¡
        self.quality_metrics['total_evaluations'] += 1
        self.quality_metrics['average_success_rate'] = (
            (self.quality_metrics['average_success_rate'] * (self.quality_metrics['total_evaluations'] - 1) +
             quality_assessment['success_rate']) / self.quality_metrics['total_evaluations']
        )

        if quality_assessment['meets_threshold']:
            self.quality_metrics['passed_quality_threshold'] += 1
            self.quality_metrics['quality_distribution'][quality_assessment['quality_level']] += 1
        else:
            self.quality_metrics['failed_quality_threshold'] += 1

        # æ·»åŠ è´¨é‡ä¿¡æ¯åˆ°ç»“æœä¸­
        enhanced_result = {
            **standard_result,
            'quality_assessment': quality_assessment,
            'meets_minimum_quality': quality_assessment['meets_threshold'],
            'quality_score': quality_assessment['quality_score'],
            'analysis_timestamp': datetime.now().isoformat()
        }

        # ä¿å­˜å¢å¼ºç»“æœ
        output_filename = input_file.replace('.json', '_quality_enhanced_analysis.json')
        output_path = os.path.join(output_dir, os.path.basename(output_filename))

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(enhanced_result, f, ensure_ascii=False, indent=2)

        print(f"   ğŸ“Š æˆåŠŸç‡: {quality_assessment['success_rate']:.1%}")
        print(f"   ğŸ¯ è´¨é‡åˆ†æ•°: {quality_assessment['quality_score']}")
        print(f"   {'âœ…' if quality_assessment['meets_threshold'] else 'âŒ'} è´¨é‡é˜ˆå€¼: {'é€šè¿‡' if quality_assessment['meets_threshold'] else 'æœªé€šè¿‡'}")

        return enhanced_result

    def _standard_analysis(self, input_file: str, output_dir: str) -> Dict:
        """æ ‡å‡†åˆ†ææµç¨‹ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…ä¼šè°ƒç”¨åŸæœ‰çš„åˆ†æé€»è¾‘ï¼‰"""
        # è¿™é‡Œä¼šè°ƒç”¨åŸæœ‰çš„ä¸‰æ¨¡å‹åˆ†æé€»è¾‘
        # ä¸ºäº†æ¼”ç¤ºï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ
        return {
            'source_file': os.path.basename(input_file),
            'model_results': {
                'deepseek-v3.1:671b-cloud': {'success_rate': 0.85},
                'gpt-oss:20b-cloud': {'success_rate': 0.92},
                'qwen3-coder:480b-cloud': {'success_rate': 0.78}
            },
            'consistency_analysis': {
                'confidence_score': 75,
                'consensus_mbti': 'ISTJ'
            },
            'total_segments': 10
        }

class QualityControlReport:
    def __init__(self):
        self.report_data = {
            'generation_time': datetime.now().isoformat(),
            'quality_threshold': 0.9,
            'evaluators': {},
            'summary': {},
            'recommendations': []
        }

    def generate_report(self, ollama_metrics: Dict, cloud_metrics: Dict, output_path: str):
        """ç”Ÿæˆè´¨é‡æ§åˆ¶æŠ¥å‘Š"""

        self.report_data['evaluators']['ollama'] = ollama_metrics
        self.report_data['evaluators']['cloud'] = cloud_metrics

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_evaluations = ollama_metrics['total_evaluations'] + cloud_metrics['total_evaluations']
        total_passed = ollama_metrics['passed_quality_threshold'] + cloud_metrics['passed_quality_threshold']

        self.report_data['summary'] = {
            'total_evaluations': total_evaluations,
            'passed_quality_threshold': total_passed,
            'failed_quality_threshold': total_evaluations - total_passed,
            'overall_pass_rate': (total_passed / total_evaluations * 100) if total_evaluations > 0 else 0,
            'quality_threshold_percentage': 90
        }

        # ç”Ÿæˆå»ºè®®
        if self.report_data['summary']['overall_pass_rate'] < 50:
            self.report_data['recommendations'].extend([
                "ç³»ç»Ÿæ•´ä½“è´¨é‡ä¸è¾¾æ ‡ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§",
                "è€ƒè™‘é™ä½å¹¶å‘æ•°é‡ï¼Œæé«˜å•ä¸ªè¯·æ±‚çš„æˆåŠŸç‡",
                "å¢åŠ é‡è¯•æœºåˆ¶å’Œé”™è¯¯æ¢å¤ç­–ç•¥"
            ])

        if self.report_data['summary']['overall_pass_rate'] < 80:
            self.report_data['recommendations'].extend([
                "å»ºè®®ä¼˜åŒ–ç½‘ç»œè¿æ¥å’ŒAPIè°ƒç”¨ç¨³å®šæ€§",
                "è€ƒè™‘å®æ–½æ›´ä¸¥æ ¼çš„è¶…æ—¶æ§åˆ¶"
            ])

        # ä¿å­˜æŠ¥å‘Š
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.report_data, f, ensure_ascii=False, indent=2)

        return output_path

def create_quality_control_patch():
    """åˆ›å»ºè´¨é‡æ§åˆ¶è¡¥ä¸ï¼Œä¿®æ”¹ç°æœ‰è¯„ä¼°å™¨"""

    patch_content = '''
# æ•°æ®è´¨é‡å¢å¼ºè¡¥ä¸
# æ·»åŠ åˆ° three_model_ollama_evaluator.py

def apply_quality_control_enhancement(self):
    """åº”ç”¨è´¨é‡æ§åˆ¶å¢å¼º"""
    self.min_success_rate = 0.9  # 90%æœ€ä½æˆåŠŸç‡
    self.quality_stats = {
        'total_analyzed': 0,
        'passed_quality': 0,
        'failed_quality': 0
    }

def check_data_quality(self, model_results):
    """æ£€æŸ¥æ•°æ®è´¨é‡"""
    success_rates = []
    for model, result in model_results.items():
        if 'success_rate' in result:
            success_rates.append(result['success_rate'])

    if not success_rates:
        return False, 0.0

    avg_success_rate = sum(success_rates) / len(success_rates)
    meets_threshold = avg_success_rate >= self.min_success_rate

    return meets_threshold, avg_success_rate

def enhance_result_with_quality(self, result):
    """å¢å¼ºç»“æœåŒ…å«è´¨é‡ä¿¡æ¯"""
    if 'model_results' in result:
        meets_threshold, success_rate = self.check_data_quality(result['model_results'])

        result['quality_assessment'] = {
            'meets_90_percent_threshold': meets_threshold,
            'average_success_rate': success_rate,
            'quality_score': success_rate * 100,
            'timestamp': datetime.now().isoformat()
        }

        # æ›´æ–°è´¨é‡ç»Ÿè®¡
        self.quality_stats['total_analyzed'] += 1
        if meets_threshold:
            self.quality_stats['passed_quality'] += 1
        else:
            self.quality_stats['failed_quality'] += 1

    return result
'''

    with open('quality_control_patch.py', 'w', encoding='utf-8') as f:
        f.write(patch_content)

    print("âœ… è´¨é‡æ§åˆ¶è¡¥ä¸å·²åˆ›å»º: quality_control_patch.py")

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºè´¨é‡æ§åˆ¶å¢å¼º"""
    print("ğŸ”§ æ•°æ®è´¨é‡å¢å¼ºè¯„ä¼°å™¨")
    print("=" * 80)
    print("è®¾ç½®90%æœ€ä½æˆåŠŸç‡é˜ˆå€¼")

    # åˆ›å»ºè´¨é‡æ§åˆ¶è¡¥ä¸
    create_quality_control_patch()

    # æ¼”ç¤ºè´¨é‡å¢å¼ºè¯„ä¼°å™¨
    models = ['deepseek-v3.1:671b-cloud', 'gpt-oss:20b-cloud', 'qwen3-coder:480b-cloud']
    enhanced_evaluator = EnhancedOllamaEvaluator(models, min_success_rate=0.9)

    print("\nğŸ“Š è´¨é‡æ§åˆ¶é…ç½®:")
    print(f"   æœ€ä½æˆåŠŸç‡é˜ˆå€¼: {enhanced_evaluator.min_success_rate:.0%}")
    print(f"   è´¨é‡è¯„åˆ†æƒé‡: æˆåŠŸç‡70% + ä¸€è‡´æ€§30%")

    print("\nğŸ¯ è´¨é‡ç­‰çº§å®šä¹‰:")
    print("   é«˜è´¨é‡: æˆåŠŸç‡ â‰¥ 95%")
    print("   ä¸­ç­‰è´¨é‡: 90% â‰¤ æˆåŠŸç‡ < 95%")
    print("   ä½è´¨é‡: æˆåŠŸç‡ < 90% (ä¸æ»¡è¶³æœ€ä½è¦æ±‚)")

    print("\nğŸ“‹ å®æ–½å»ºè®®:")
    print("1. åœ¨ç°æœ‰è¯„ä¼°å™¨ä¸­åº”ç”¨è´¨é‡æ§åˆ¶è¡¥ä¸")
    print("2. æ‰€æœ‰ä½äº90%æˆåŠŸç‡çš„ç»“æœæ ‡è®°ä¸º'ä¸å¯ä¿¡'")
    print("3. ç”Ÿæˆè´¨é‡æŠ¥å‘Šï¼Œè¿½è¸ªç³»ç»Ÿæ•´ä½“è´¨é‡")
    print("4. æ ¹æ®è´¨é‡ç»Ÿè®¡ä¼˜åŒ–æ¨¡å‹è°ƒç”¨ç­–ç•¥")

    # ç”Ÿæˆè´¨é‡æ§åˆ¶é…ç½®æ–‡ä»¶
    config = {
        'quality_control': {
            'min_success_rate': 0.9,
            'quality_weights': {'success_rate': 0.7, 'consistency': 0.3},
            'quality_levels': {
                'high': {'min_rate': 0.95, 'label': 'é«˜è´¨é‡'},
                'medium': {'min_rate': 0.9, 'label': 'ä¸­ç­‰è´¨é‡'},
                'low': {'max_rate': 0.9, 'label': 'ä½è´¨é‡'}
            },
            'action_rules': {
                'below_threshold': 'æ ‡è®°ä¸ºä¸å¯ä¿¡ï¼Œå»ºè®®é‡æ–°è¯„ä¼°',
                'warning_threshold': 0.85,
                'critical_threshold': 0.7
            }
        }
    }

    with open('quality_control_config.json', 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)

    print("\nâœ… è´¨é‡æ§åˆ¶é…ç½®å·²ä¿å­˜: quality_control_config.json")
    print("ğŸ”§ è´¨é‡æ§åˆ¶è¡¥ä¸å·²ç”Ÿæˆ: quality_control_patch.py")

if __name__ == "__main__":
    main()