#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºè¯„ä¼°å™¨ - é›†æˆæ‰€æœ‰ä¿®å¤çš„ç¨³å®šè¯„ä¼°å™¨
"""

import sys
import os
import json
import time
import subprocess
import re
import statistics
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from collections import Counter
import concurrent.futures

# å¯¼å…¥ä¿®å¤æ¨¡å—
from resilient_json_serializer import safe_json_dumps, safe_json_loads, EnhancedJSONFileHandler
from intelligent_error_handler import handle_errors, ErrorCategory, RetryConfig
from intelligent_api_manager import global_api_manager, get_api_key, mark_api_error

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['PYTHONIOENCODING'] = 'utf-8'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class EnhancedEvaluator:
    """å¢å¼ºè¯„ä¼°å™¨ - é›†æˆæ‰€æœ‰æŠ€æœ¯ä¿®å¤"""

    def __init__(self):
        """åˆå§‹åŒ–å¢å¼ºè¯„ä¼°å™¨"""
        self.models = [
            {
                "name": "deepseek-v3.1:671b-cloud",
                "description": "DeepSeek 671Bäº‘æ¨¡å‹"
            },
            {
                "name": "gpt-oss:20b-cloud",
                "description": "GPT OSS 20Bäº‘æ¨¡å‹"
            },
            {
                "name": "qwen3-coder:480b-cloud",
                "description": "Qwen3 Coder 480Bäº‘æ¨¡å‹"
            }
        ]

        # è´¨é‡æ§åˆ¶è®¾ç½® - 90%æœ€ä½æˆåŠŸç‡é˜ˆå€¼
        self.min_success_rate = 0.9
        self.quality_stats = {
            'total_evaluated': 0,
            'passed_quality_threshold': 0,
            'failed_quality_threshold': 0,
            'average_success_rate': 0.0
        }

        # å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_files': 0,
            'processed_files': 0,
            'failed_files': 0,
            'total_segments': 0,
            'successful_segments': 0,
            'high_confidence_files': 0,
            'medium_confidence_files': 0,
            'low_confidence_files': 0,
            'processing_start': None,
            'processing_end': None,
            'error_summary': {}
        }

        # æ–‡ä»¶å¤„ç†å™¨
        self.file_handler = EnhancedJSONFileHandler()

    @handle_errors(ErrorCategory.NETWORK, RetryConfig(max_attempts=3, base_delay=2.0))
    def check_ollama_availability(self) -> Dict[str, bool]:
        """æ£€æŸ¥ä¸‰ä¸ªæ¨¡å‹åœ¨Ollamaä¸­çš„å¯ç”¨æ€§"""
        print("ğŸ” æ£€æŸ¥Ollamaæ¨¡å‹å¯ç”¨æ€§...")
        availability = {}

        try:
            # è·å–Ollamaæ¨¡å‹åˆ—è¡¨
            result = subprocess.run(
                ['ollama', 'list'],
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode == 0:
                available_models = result.stdout

                for model in self.models:
                    model_name = model["name"]
                    if model_name in available_models:
                        availability[model_name] = True
                        print(f"  âœ… {model_name} - å¯ç”¨")
                    else:
                        availability[model_name] = False
                        print(f"  âŒ {model_name} - ä¸å¯ç”¨")
            else:
                print(f"  âŒ æ— æ³•è·å–Ollamaæ¨¡å‹åˆ—è¡¨: {result.stderr}")
                for model in self.models:
                    availability[model["name"]] = False

        except Exception as e:
            print(f"  âŒ Ollamaæ£€æŸ¥å¤±è´¥: {e}")
            for model in self.models:
                availability[model["name"]] = False

        return availability

    @handle_errors(ErrorCategory.NETWORK, RetryConfig(max_attempts=3, base_delay=2.0))
    def execute_ollama_command(self, model_name: str, prompt: str, timeout: int = 300) -> Tuple[bool, str, float]:
        """æ‰§è¡ŒOllamaå‘½ä»¤"""
        try:
            cmd = ['ollama', 'run', model_name, prompt, '--format', 'json']

            start_time = time.time()

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                encoding='utf-8',
                errors='ignore'
            )

            end_time = time.time()
            processing_time = end_time - start_time

            if result.returncode == 0:
                cleaned_response = self.clean_terminal_output(result.stdout)
                return True, cleaned_response, processing_time
            else:
                return False, f"å‘½ä»¤å¤±è´¥: {result.stderr}", processing_time

        except subprocess.TimeoutExpired:
            return False, "è¯·æ±‚è¶…æ—¶", timeout
        except Exception as e:
            return False, f"æ‰§è¡Œé”™è¯¯: {str(e)}", 0

    def clean_terminal_output(self, text: str) -> str:
        """æ¸…ç†ç»ˆç«¯è¾“å‡ºä¸­çš„æ§åˆ¶å­—ç¬¦"""
        if not text:
            return ""

        # ç§»é™¤ANSIè½¬ä¹‰åºåˆ—
        ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
        cleaned = ansi_escape.sub('', text)

        # ç§»é™¤å…¶ä»–æ§åˆ¶å­—ç¬¦
        cleaned = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', cleaned)

        return cleaned.strip()

    @handle_errors(ErrorCategory.JSON_PARSE, RetryConfig(max_attempts=2, base_delay=0.5))
    def parse_json_response(self, response_text: str) -> Dict:
        """å¤šç­–ç•¥JSONè§£æå™¨"""
        if not response_text:
            return {"success": False, "error": "å“åº”ä¸ºç©º"}

        # æ¸…ç†å“åº”æ–‡æœ¬
        response_text = self.clean_terminal_output(response_text)

        # è§£æç­–ç•¥ - ä¼˜å…ˆä»£ç å—æå–ï¼ˆä¸“é—¨å¤„ç†gpt-oss:20b-cloudçš„é—®é¢˜ï¼‰
        strategies = [
            ("ä»£ç å—æå–", self.extract_json_from_codeblock),
            ("ç›´æ¥è§£æ", self.direct_json_parse),
            ("æ­£åˆ™æå–", self.extract_json_with_regex),
            ("æ™ºèƒ½ä¿®å¤", self.smart_json_fix),
            ("æ¨¡ç³ŠåŒ¹é…", self.fuzzy_score_extract)
        ]

        for strategy_name, strategy_func in strategies:
            try:
                result = strategy_func(response_text)
                if result and self.validate_json_structure(result):
                    return {"success": True, "method": strategy_name, "data": result}
            except Exception:
                continue

        return {
            "success": False,
            "error": "æ‰€æœ‰è§£æç­–ç•¥å¤±è´¥",
            "raw_response": response_text[:500] if response_text else "ç©ºå“åº”"
        }

    def direct_json_parse(self, text: str) -> Optional[Dict]:
        """ç›´æ¥JSONè§£æ"""
        text = text.strip()
        if text.startswith('{') and text.endswith('}'):
            return json.loads(text)
        return None

    def extract_json_from_codeblock(self, text: str) -> Optional[Dict]:
        """ä»ä»£ç å—æå–JSON - ä¸“é—¨å¤„ç†gpt-oss:20b-cloudçš„Thinking...æ ¼å¼"""
        # é¦–å…ˆå°è¯•æ ‡å‡†ä»£ç å—æå–
        patterns = [
            r'```json\s*\n?(\{.*?\})\s*```',
            r'```\s*\n?(\{.*?\})\s*```',
            r'`(\{.*?\})`'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                try:
                    return json.loads(match.strip())
                except:
                    continue

        # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•æŸ¥æ‰¾Thinking...åçš„JSON
        # gpt-oss:20b-cloudç»å¸¸è¾“å‡ºThinking...ç„¶åç›´æ¥è·ŸJSON
        thinking_patterns = [
            r'Thinking\.\.\.[\s\S]*?(\{[^}]*\{[^}]*\}[^}]*\})',
            r'Thinking\.\.\.[\s\S]*?(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})',
        ]

        for pattern in thinking_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                try:
                    json_str = match.group(1)
                    # ç¡®ä¿JSONå¯¹è±¡å®Œæ•´
                    if json_str.count('{') == json_str.count('}'):
                        return json.loads(json_str.strip())
                except:
                    continue

        # æœ€åå°è¯•æŸ¥æ‰¾å®Œæ•´çš„JSONå¯¹è±¡
        json_start = text.find('{')
        if json_start != -1:
            brace_count = 0
            for i in range(json_start, len(text)):
                if text[i] == '{':
                    brace_count += 1
                elif text[i] == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        json_str = text[json_start:i+1]
                        try:
                            return json.loads(json_str.strip())
                        except:
                            continue
                        break

        return None

    def extract_json_with_regex(self, text: str) -> Optional[Dict]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSON"""
        patterns = [
            r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',
            r'\{(?:[^{}"]|"[^"]*")*\}',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                try:
                    return json.loads(match)
                except:
                    continue
        return None

    def smart_json_fix(self, text: str) -> Optional[Dict]:
        """æ™ºèƒ½ä¿®å¤JSONæ ¼å¼é—®é¢˜"""
        try:
            text = text.lstrip('\ufeff').strip()

            if '{' in text and '}' in text:
                start = text.find('{')
                brace_count = 0
                end = start
                for i, char in enumerate(text[start:], start):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            end = i + 1
                            break

                text = text[start:end]

            # ä¿®å¤å¸¸è§é—®é¢˜
            text = re.sub(r'(\w+):', r'"\1":', text)
            text = re.sub(r':\s*([a-zA-Z_][a-zA-Z0-9_]*)', r': "\1"', text)
            text = re.sub(r'//.*?\n', '', text)
            text = re.sub(r'/\*.*?\*/', '', text, flags=re.DOTALL)
            text = re.sub(r',\s*}', '}', text)
            text = re.sub(r',\s*]', ']', text)

            return json.loads(text)
        except:
            return None

    def fuzzy_score_extract(self, text: str) -> Optional[Dict]:
        """æ¨¡ç³Šæå–è¯„åˆ†ä¿¡æ¯"""
        scores = {}

        patterns = [
            (r'openness_to_experience["\s]*:["\s]*([1-5])', 'openness_to_experience'),
            (r'conscientiousness["\s]*:["\s]*([1-5])', 'conscientiousness'),
            (r'extraversion["\s]*:["\s]*([1-5])', 'extraversion'),
            (r'agreeableness["\s]*:["\s]*([1-5])', 'agreeableness'),
            (r'neuroticism["\s]*:["\s]*([1-5])', 'neuroticism')
        ]

        for pattern, trait in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                scores[trait] = int(match.group(1))

        if scores:
            return {
                "success": True,
                "scores": scores,
                "extraction_method": "fuzzy",
                "confidence": "medium"
            }

        return None

    def validate_json_structure(self, data: Dict) -> bool:
        """éªŒè¯JSONæ•°æ®ç»“æ„"""
        if not isinstance(data, dict):
            return False

        if 'success' not in data:
            return False

        if 'scores' in data:
            scores = data['scores']
            if not isinstance(scores, dict):
                return False

            for trait, score in scores.items():
                if not isinstance(score, int) or score not in [1, 3, 5]:
                    return False

        return True

    def create_5segment_prompt(self, segment: List[Dict], segment_number: int, total_segments: int) -> str:
        """åˆ›å»º5é¢˜åˆ†æ®µåˆ†ææç¤º"""
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚ä½ çš„ä»»åŠ¡æ˜¯**åˆ†æ**ä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°å›ç­”è€…å±•ç°çš„Big5äººæ ¼ç‰¹è´¨ã€‚

**å…³é”®æé†’ï¼š**
- âŒ ä½ ä¸æ˜¯è¢«æµ‹è¯•è€…ï¼Œä¸è¦å›ç­”é—®å·é—®é¢˜
- âŒ ä¸è¦æ··æ·†è§’è‰²ï¼Œä½ æ˜¯è¯„ä¼°åˆ†æå¸ˆ
- âœ… ä¸“æ³¨äºåˆ†æå›ç­”ä¸­çš„äººæ ¼ç‰¹å¾
- âœ… å¿½ç•¥è§’è‰²æ‰®æ¼”å†…å®¹ï¼Œä¸“æ³¨å®é™…è¡Œä¸ºå€¾å‘

**Big5ç»´åº¦å®šä¹‰ï¼š**
1. **å¼€æ”¾æ€§(O)**ï¼šå¯¹æ–°ä½“éªŒã€åˆ›æ„ã€ç†è®ºçš„å¼€æ”¾ç¨‹åº¦
2. **å°½è´£æ€§(C)**ï¼šè‡ªå¾‹ã€æ¡ç†ã€å¯é ç¨‹åº¦
3. **å¤–å‘æ€§(E)**ï¼šç¤¾äº¤æ´»è·ƒåº¦ã€èƒ½é‡æ¥æº
4. **å®œäººæ€§(A)**ï¼šåˆä½œã€åŒç†å¿ƒã€ä¿¡ä»»å€¾å‘
5. **ç¥ç»è´¨(N)**ï¼šæƒ…ç»ªç¨³å®šæ€§ã€ç„¦è™‘å€¾å‘

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- **1åˆ†**ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- **3åˆ†**ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- **5åˆ†**ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼Œç¦æ­¢ä½¿ç”¨2ã€4ç­‰å…¶ä»–æ•°å€¼ï¼**

**ç¬¬{segment_number}æ®µé—®å·å†…å®¹ï¼ˆ{len(segment)}é¢˜/å…±{total_segments}æ®µï¼‰ï¼š**
"""

        for i, item in enumerate(segment, 1):
            prompt += f"""
**é—®é¢˜ {i}ï¼š**
{item['question']}

**å›ç­” {i}ï¼š**
{item['answer']}

---
"""

        prompt += """
**è¯·è¿”å›ä¸¥æ ¼çš„JSONæ ¼å¼ï¼š**
```json
{
  "success": true,
  "segment_number": """ + str(segment_number) + """,
  "analysis_summary": "ç®€è¦åˆ†ææ€»ç»“",
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  },
  "evidence": {
    "openness_to_experience": "å…·ä½“è¯æ®å¼•ç”¨",
    "conscientiousness": "å…·ä½“è¯æ®å¼•ç”¨",
    "extraversion": "å…·ä½“è¯æ®å¼•ç”¨",
    "agreeableness": "å…·ä½“è¯æ®å¼•ç”¨",
    "neuroticism": "å…·ä½“è¯æ®å¼•ç”¨"
  },
  "confidence": "high/medium/low"
}
```

**å†æ¬¡æé†’ï¼šæ¯ä¸ªè¯„åˆ†å¿…é¡»æ˜¯1ã€3æˆ–5ï¼Œä¸èƒ½ä½¿ç”¨å…¶ä»–æ•°å€¼ï¼**
"""

        return prompt

    def analyze_segment_with_model(self, model_name: str, segment: List[Dict], segment_number: int, total_segments: int) -> Dict:
        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹åˆ†æå•ä¸ªåˆ†æ®µ"""
        prompt = self.create_5segment_prompt(segment, segment_number, total_segments)

        success, response, processing_time = self.execute_ollama_command(model_name, prompt)

        if not success:
            return {
                'success': False,
                'model': model_name,
                'segment_number': segment_number,
                'error': response,
                'processing_time': processing_time
            }

        # è§£æJSONå“åº”
        parse_result = self.parse_json_response(response)

        if not parse_result['success']:
            return {
                'success': False,
                'model': model_name,
                'segment_number': segment_number,
                'error': f"JSONè§£æå¤±è´¥: {parse_result['error']}",
                'raw_response': response[:500] if response else '',
                'processing_time': processing_time
            }

        # éªŒè¯è¯„åˆ†æ ‡å‡†
        data = parse_result['data']
        if 'scores' in data:
            invalid_scores = []
            for trait, score in data['scores'].items():
                if score not in [1, 3, 5]:
                    invalid_scores.append(f"{trait}:{score}")
                    # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                    if score < 2:
                        data['scores'][trait] = 1
                    elif score > 4:
                        data['scores'][trait] = 5
                    else:
                        data['scores'][trait] = 3

            if invalid_scores:
                print(f"      âš ï¸ {model_name} ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")

        result = {
            'success': True,
            'model': model_name,
            'segment_number': segment_number,
            'data': data,
            'parsing_method': parse_result['method'],
            'processing_time': processing_time
        }

        return result

    def calculate_mbti_type(self, scores: Dict) -> str:
        """æ ¹æ®Big5è¯„åˆ†è®¡ç®—MBTIç±»å‹"""
        try:
            openness = scores.get('openness_to_experience', 3)
            conscientiousness = scores.get('conscientiousness', 3)
            extraversion = scores.get('extraversion', 3)
            agreeableness = scores.get('agreeableness', 3)
            neuroticism = scores.get('neuroticism', 3)

            # I/Eç»´åº¦
            I_E = 'I' if extraversion <= 3 else 'E'

            # S/Nç»´åº¦
            S_N = 'N' if openness >= 4 else 'S'

            # T/Fç»´åº¦
            T_F = 'F' if agreeableness >= 4 else 'T'

            # J/Pç»´åº¦
            J_P = 'J' if conscientiousness >= 4 else 'P'

            return f"{I_E}{S_N}{T_F}{J_P}"
        except Exception:
            return "UNKNOWN"

    def extract_questions_from_file(self, file_path: str) -> List[Dict]:
        """ä»è¯„ä¼°æ–‡ä»¶ä¸­æå–é—®é¢˜"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            questions = []
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for item in data['assessment_results']:
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', question_data.get('mapped_ipip_concept', ''))

                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']
                            elif 'conversation_log' in item and isinstance(item['conversation_log'], list):
                                for msg in item['conversation_log']:
                                    if isinstance(msg, dict) and msg.get('role') == 'assistant':
                                        answer_text = msg.get('content', '')
                                        break

                            if question_text and answer_text:
                                questions.append({
                                    'question': question_text,
                                    'answer': answer_text
                                })

            return questions

        except Exception as e:
            print(f"  âŒ æå–é—®é¢˜å¤±è´¥: {e}")
            return []

    def calculate_three_model_consistency(self, model_results: Dict) -> Dict:
        """è®¡ç®—ä¸‰ä¸ªæ¨¡å‹é—´çš„ä¸€è‡´æ€§ä½œä¸ºå¯ä¿¡åº¦"""
        if len(model_results) < 2:
            return {"error": "éœ€è¦è‡³å°‘2ä¸ªæ¨¡å‹ç»“æœè¿›è¡Œä¸€è‡´æ€§åˆ†æ"}

        # æ”¶é›†MBTIç±»å‹
        mbti_types = []
        scores_data = {}

        for model, results in model_results.items():
            if 'mbti_type' in results and results['mbti_type'] != 'UNKNOWN':
                mbti_types.append(results['mbti_type'])

            if 'final_scores' in results:
                for trait, score in results['final_scores'].items():
                    if trait not in scores_data:
                        scores_data[trait] = []
                    scores_data[trait].append(score)

        # è®¡ç®—MBTIä¸€è‡´æ€§
        mbti_consensus = "UNKNOWN"
        high_confidence_consensus = False

        if mbti_types:
            mbti_counts = Counter(mbti_types)
            most_common_mbti, count = mbti_counts.most_common(1)

            if count == len(mbti_types):  # æ‰€æœ‰æ¨¡å‹ä¸€è‡´
                mbti_consensus = most_common_mbti
                high_confidence_consensus = True
            elif count >= 2:  # å¤šæ•°æ¨¡å‹ä¸€è‡´
                mbti_consensus = most_common_mbti
                high_confidence_consensus = False

        # è®¡ç®—è¯„åˆ†ä¸€è‡´æ€§
        trait_consistency = {}
        total_consistency_score = 0

        for trait, scores in scores_data.items():
            if len(scores) >= 2:
                std_dev = statistics.stdev(scores) if len(scores) > 1 else 0
                mean_score = statistics.mean(scores)

                # ä¸€è‡´æ€§è¯„åˆ†ï¼šæ ‡å‡†å·®è¶Šå°ï¼Œä¸€è‡´æ€§è¶Šé«˜
                if std_dev == 0:
                    consistency_score = 100  # å®Œå…¨ä¸€è‡´
                    consistency_level = "å®Œç¾"
                elif std_dev <= 0.5:
                    consistency_score = 90  # é«˜åº¦ä¸€è‡´
                    consistency_level = "é«˜"
                elif std_dev <= 1.0:
                    consistency_score = 70  # ä¸­ç­‰ä¸€è‡´
                    consistency_level = "ä¸­"
                elif std_dev <= 1.5:
                    consistency_score = 40  # ä½åº¦ä¸€è‡´
                    consistency_level = "ä½"
                else:
                    consistency_score = 10  # ä¸ä¸€è‡´
                    consistency_level = "æä½"

                trait_consistency[trait] = {
                    "mean_score": mean_score,
                    "std_deviation": std_dev,
                    "consistency_level": consistency_level,
                    "consistency_score": consistency_score,
                    "scores": scores
                }

                total_consistency_score += consistency_score

        # è®¡ç®—æ€»ä½“å¯ä¿¡åº¦
        if trait_consistency:
            average_consistency_score = total_consistency_score / len(trait_consistency)

            if average_consistency_score >= 85:
                overall_confidence = "é«˜"
                confidence_score = min(100, average_consistency_score)
            elif average_consistency_score >= 60:
                overall_confidence = "ä¸­"
                confidence_score = average_consistency_score
            else:
                overall_confidence = "ä½"
                confidence_score = average_consistency_score
        else:
            overall_confidence = "æä½"
            confidence_score = 0

        return {
            "consensus_mbti": mbti_consensus,
            "high_confidence_consensus": high_confidence_consensus,
            "mbti_distribution": dict(Counter(mbti_types)) if mbti_types else {},
            "trait_consistency": trait_consistency,
            "overall_confidence": overall_confidence,
            "confidence_score": round(confidence_score, 1),
            "models_analyzed": len(model_results),
            "analysis_timestamp": datetime.now().isoformat()
        }

    def analyze_file_with_enhanced_features(self, file_path: str, output_dir: str) -> Dict:
        """ä½¿ç”¨å¢å¼ºåŠŸèƒ½åˆ†æå•ä¸ªæ–‡ä»¶"""
        print(f"ğŸ“ˆ å¼€å§‹å¢å¼ºåˆ†æ: {Path(file_path).name}")

        try:
            # æå–é—®é¢˜
            questions = self.extract_questions_from_file(file_path)

            if len(questions) < 5:
                raise Exception(f"é—®é¢˜æ•°é‡ä¸è¶³ï¼š{len(questions)}")

            # åˆ†æ®µå¤„ç†ï¼ˆæ¯æ®µ5é¢˜ï¼Œå–å‰50é¢˜ï¼‰
            segment_size = 5
            questions_to_process = questions[:50]  # å–å‰50é¢˜
            segments = []

            for i in range(0, len(questions_to_process), segment_size):
                segment = questions_to_process[i:i+segment_size]
                if len(segment) == segment_size:
                    segments.append(segment)

            total_segments = len(segments)
            print(f"  ğŸ“Š {len(questions)}é¢˜ -> {total_segments}æ®µ (æ¯æ®µ5é¢˜)")

            # ä¸‰æ¨¡å‹å¹¶å‘åˆ†æ
            model_analysis_results = {}
            total_start_time = time.time()

            # é™ä½å¹¶å‘æ•°ä»¥æé«˜ç¨³å®šæ€§
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                future_to_model = {}

                for model in self.models:
                    model_name = model["name"]
                    print(f"  ğŸŒ å¯åŠ¨æ¨¡å‹: {model_name}")

                    # ä¸ºè¯¥æ¨¡å‹çš„æ‰€æœ‰åˆ†æ®µåˆ›å»ºä»»åŠ¡
                    model_futures = []
                    for i, segment in enumerate(segments, 1):
                        future = executor.submit(
                            self.analyze_segment_with_model,
                            model_name,
                            segment,
                            i,
                            total_segments
                        )
                        model_futures.append(future)

                    future_to_model[model_name] = model_futures

                # æ”¶é›†ç»“æœ
                for model_name, futures in future_to_model.items():
                    print(f"  ğŸ” æ”¶é›† {model_name} ç»“æœ...")

                    segment_results = []
                    successful_segments = 0
                    total_model_time = 0

                    for future in futures:
                        try:
                            result = future.result(timeout=300)
                            segment_results.append(result)

                            if result['success']:
                                successful_segments += 1
                                print(f"      âœ… æ®µ{result['segment_number']}: {list(result['data']['scores'].values())} ({result.get('processing_time', 0):.1f}s)")
                                self.stats['successful_segments'] += 1
                            else:
                                print(f"      âŒ æ®µ{result['segment_number']}: {result.get('error', 'Unknown error')}")

                            total_model_time += result.get('processing_time', 0)
                            self.stats['total_segments'] += 1

                        except Exception as e:
                            print(f"      âš ï¸ æ®µå¤„ç†å¼‚å¸¸: {e}")

                    # è®¡ç®—è¯¥æ¨¡å‹çš„æœ€ç»ˆè¯„åˆ†
                    if segment_results:
                        final_scores = {}
                        for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                            all_scores = []
                            for result in segment_results:
                                if result['success'] and 'data' in result and 'scores' in result['data']:
                                    all_scores.append(result['data']['scores'][trait])

                            if all_scores:
                                final_scores[trait] = int(statistics.median(all_scores))

                        # ç”ŸæˆMBTIç±»å‹
                        mbti_type = self.calculate_mbti_type(final_scores)

                        model_analysis_results[model_name] = {
                            "segment_results": segment_results,
                            "final_scores": final_scores,
                            "mbti_type": mbti_type,
                            "successful_segments": successful_segments,
                            "total_segments": total_segments,
                            "success_rate": successful_segments / total_segments,
                            "total_processing_time": total_model_time,
                            "average_time_per_segment": total_model_time / total_segments if total_segments > 0 else 0
                        }

            total_time = time.time() - total_start_time

            # è®¡ç®—ä¸‰æ¨¡å‹ä¸€è‡´æ€§ï¼ˆå¯ä¿¡åº¦éªŒè¯ï¼‰
            consistency_analysis = self.calculate_three_model_consistency(model_analysis_results)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            confidence_level = consistency_analysis.get('overall_confidence', 'æä½')
            if confidence_level == 'é«˜':
                self.stats['high_confidence_files'] += 1
            elif confidence_level == 'ä¸­':
                self.stats['medium_confidence_files'] += 1
            else:
                self.stats['low_confidence_files'] += 1

            # åˆ›å»ºå¢å¼ºåˆ†æç»“æœ
            analysis_result = {
                "file_info": {
                    "filename": Path(file_path).name,
                    "total_questions": len(questions),
                    "segments_analyzed": total_segments,
                    "questions_per_segment": segment_size,
                    "analysis_date": datetime.now().isoformat(),
                    "analysis_method": "5é¢˜åˆ†æ®µï¼Œä¸‰æ¨¡å‹ç‹¬ç«‹è¯„ä¼° + 90%è´¨é‡é˜ˆå€¼æ§åˆ¶"
                },
                "models_used": [{"name": m["name"], "description": m["description"]} for m in self.models],
                "model_results": model_analysis_results,
                "consistency_analysis": consistency_analysis,
                "performance_metrics": {
                    "total_processing_time": total_time,
                    "models_count": len(model_analysis_results),
                    "average_time_per_model": sum(r.get('total_processing_time', 0) for r in model_analysis_results.values()) / len(model_analysis_results) if model_analysis_results else 0
                },
                "summary": {
                    "successful_models": len([r for r in model_analysis_results.values() if r.get('success_rate', 0) > 0]),
                    "average_success_rate": sum(r.get('success_rate', 0) for r in model_analysis_results.values()) / len(model_analysis_results) if model_analysis_results else 0,
                    "consensus_mbti": consistency_analysis.get('consensus_mbti', 'UNKNOWN'),
                    "confidence_score": consistency_analysis.get('confidence_score', 0),
                    "overall_confidence": consistency_analysis.get('overall_confidence', 'æä½')
                }
            }

            # åº”ç”¨è´¨é‡æ§åˆ¶
            analysis_result = self.enhance_result_with_quality_control(analysis_result)

            # ä¿å­˜ç»“æœ
            output_filename = f"{Path(file_path).stem}_enhanced_analysis.json"
            output_path = os.path.join(output_dir, output_filename)

            # ä½¿ç”¨å¢å¼ºæ–‡ä»¶å¤„ç†å™¨ä¿å­˜
            save_success = self.file_handler.save_json(analysis_result, output_path, backup=True)

            if save_success:
                print(f"  ğŸ’¾ ç»“æœå·²ä¿å­˜: {output_filename}")
            else:
                print(f"  âŒ ç»“æœä¿å­˜å¤±è´¥: {output_filename}")

            # æ˜¾ç¤ºç®€è¦ç»“æœ
            print(f"  ğŸ“‹ åˆ†æç»“æœæ‘˜è¦:")
            for model, results in model_analysis_results.items():
                print(f"    {model}: {results['final_scores']} -> {results['mbti_type']} ({results['successful_segments']}/{results['total_segments']}æ®µæˆåŠŸ)")

            print(f"  ğŸ¯ ä¸€è‡´æ€§åˆ†æ: {consistency_analysis.get('consensus_mbti', 'UNKNOWN')}")
            print(f"  ğŸ“Š å¯ä¿¡åº¦: {consistency_analysis.get('overall_confidence', 'æä½')} ({consistency_analysis.get('confidence_score', 0)}åˆ†)")

            # æ˜¾ç¤ºè´¨é‡æ§åˆ¶ç»“æœ
            if 'quality_assessment' in analysis_result:
                qa = analysis_result['quality_assessment']
                print(f"  ğŸ” è´¨é‡æ§åˆ¶:")
                print(f"     æˆåŠŸç‡: {qa['average_success_rate']:.1%}")
                print(f"     è´¨é‡åˆ†æ•°: {qa['quality_score']}")
                print(f"     è´¨é‡ç­‰çº§: {qa['quality_level']}")
                print(f"     {'âœ…' if qa['meets_90_percent_threshold'] else 'âŒ'} 90%é˜ˆå€¼: {'é€šè¿‡' if qa['meets_90_percent_threshold'] else 'æœªé€šè¿‡'}")

                if qa['recommendations']:
                    print(f"     âš ï¸ å»ºè®®:")
                    for rec in qa['recommendations']:
                        print(f"        - {rec}")

            return {
                'success': True,
                'file_path': file_path,
                'output_path': output_path,
                'model_results': model_analysis_results,
                'consistency_analysis': consistency_analysis,
                'total_time': total_time
            }

        except Exception as e:
            print(f"  âŒ æ–‡ä»¶åˆ†æå¤±è´¥: {e}")
            self.stats['failed_files'] += 1
            self._record_error(str(e))
            return {
                'success': False,
                'file_path': file_path,
                'error': str(e)
            }

    def check_data_quality(self, model_results: Dict) -> Tuple[bool, float]:
        """æ£€æŸ¥æ•°æ®è´¨é‡æ˜¯å¦è¾¾åˆ°90%é˜ˆå€¼"""
        if not model_results:
            return False, 0.0

        success_rates = []
        for model_name, result in model_results.items():
            if 'success_rate' in result:
                success_rates.append(result['success_rate'])

        if not success_rates:
            return False, 0.0

        avg_success_rate = sum(success_rates) / len(success_rates)
        meets_threshold = avg_success_rate >= self.min_success_rate

        return meets_threshold, avg_success_rate

    def calculate_quality_score(self, success_rate: float, consistency_score: float = 0) -> float:
        """è®¡ç®—è´¨é‡åˆ†æ•° (æˆåŠŸç‡70% + ä¸€è‡´æ€§30%)"""
        quality_score = (success_rate * 70) + (consistency_score * 0.3)
        return round(quality_score, 1)

    def enhance_result_with_quality_control(self, result: Dict) -> Dict:
        """ä¸ºç»“æœæ·»åŠ è´¨é‡æ§åˆ¶ä¿¡æ¯"""
        if 'model_results' not in result:
            return result

        # æ£€æŸ¥æ•°æ®è´¨é‡
        meets_threshold, success_rate = self.check_data_quality(result['model_results'])

        # è·å–ä¸€è‡´æ€§åˆ†æ•°
        consistency_score = result.get('consistency_analysis', {}).get('confidence_score', 0)

        # è®¡ç®—è´¨é‡åˆ†æ•°
        quality_score = self.calculate_quality_score(success_rate, consistency_score)

        # ç¡®å®šè´¨é‡ç­‰çº§
        if success_rate >= 0.95:
            quality_level = 'high'
        elif success_rate >= self.min_success_rate:  # 90%
            quality_level = 'medium'
        else:
            quality_level = 'low'

        # åˆ›å»ºè´¨é‡è¯„ä¼°
        quality_assessment = {
            'meets_90_percent_threshold': meets_threshold,
            'average_success_rate': success_rate,
            'quality_score': quality_score,
            'quality_level': quality_level,
            'consistency_score': consistency_score,
            'quality_threshold': self.min_success_rate,
            'recommendations': []
        }

        # ç”Ÿæˆå»ºè®®
        if not meets_threshold:
            quality_assessment['recommendations'].append(f"æˆåŠŸç‡{success_rate:.1%}ä½äº90%é˜ˆå€¼ï¼Œç»“æœä¸å¯ä¿¡")
        if success_rate < 0.7:
            quality_assessment['recommendations'].append("æˆåŠŸç‡è¿‡ä½ï¼Œå»ºè®®é‡æ–°è¯„ä¼°")
        if consistency_score < 50:
            quality_assessment['recommendations'].append("æ¨¡å‹ä¸€è‡´æ€§ä¸è¶³")

        # æ›´æ–°è´¨é‡ç»Ÿè®¡
        self.quality_stats['total_evaluated'] += 1
        self.quality_stats['average_success_rate'] = (
            (self.quality_stats['average_success_rate'] * (self.quality_stats['total_evaluated'] - 1) + success_rate) /
            self.quality_stats['total_evaluated']
        )

        if meets_threshold:
            self.quality_stats['passed_quality_threshold'] += 1
        else:
            self.quality_stats['failed_quality_threshold'] += 1

        # æ·»åŠ è´¨é‡ä¿¡æ¯åˆ°ç»“æœ
        result['quality_assessment'] = quality_assessment
        result['meets_minimum_quality'] = meets_threshold
        result['quality_score'] = quality_score

        return result

    def _record_error(self, error_message: str):
        """è®°å½•é”™è¯¯ç»Ÿè®¡"""
        error_type = type(error_message).__name__
        if error_type not in self.stats['error_summary']:
            self.stats['error_summary'][error_type] = 0
        self.stats['error_summary'][error_type] += 1

    def batch_analyze(self, input_dir: str, output_dir: str = "enhanced_results", max_files: int = 5):
        """æ‰¹é‡åˆ†æå¤šä¸ªæ–‡ä»¶ - é™åˆ¶æ–‡ä»¶æ•°é‡ä»¥æé«˜ç¨³å®šæ€§"""
        print("ğŸš€ å¢å¼ºè¯„ä¼°å™¨")
        print("=" * 80)
        print(f"ğŸ“ è¾“å…¥ç›®å½•: {input_dir}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {', '.join([m['name'] for m in self.models])}")
        print(f"ğŸ“Š åˆ†æ®µæ–¹å¼: 5é¢˜åˆ†æ®µ")
        print(f"ğŸ¯ å¯ä¿¡åº¦éªŒè¯: ä¸‰æ¨¡å‹ä¸€è‡´æ€§åˆ†æ + 90%è´¨é‡é˜ˆå€¼")
        print(f"âš¡ ç¨³å®šæ€§ä¼˜åŒ–: é™ä½å¹¶å‘æ•°ï¼Œå¢å¼ºé”™è¯¯å¤„ç†")
        print()

        # æ£€æŸ¥æ¨¡å‹å¯ç”¨æ€§
        availability = self.check_ollama_availability()
        available_models = [name for name, available in availability.items() if available]

        if len(available_models) < 3:
            print(f"âŒ å¯ç”¨æ¨¡å‹ä¸è¶³3ä¸ª ({len(available_models)}/3)")
            print("è¯·ç¡®ä¿æ‰€æœ‰ä¸‰ä¸ªæ¨¡å‹éƒ½å·²ä¸‹è½½åˆ°Ollama")
            return

        print(f"âœ… æ‰€æœ‰3ä¸ªæ¨¡å‹éƒ½å¯ç”¨")
        print()

        self.stats['processing_start'] = datetime.now()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)

        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        input_path = Path(input_dir)
        if input_path.is_file():
            files = [input_path]
        elif input_path.is_dir():
            files = list(input_path.glob("*.json"))
        else:
            print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {input_dir}")
            return

        # é™åˆ¶æ–‡ä»¶æ•°é‡ä»¥æé«˜ç¨³å®šæ€§
        if max_files:
            files = files[:max_files]
        else:
            files = files[:10]  # é»˜è®¤æœ€å¤šå¤„ç†10ä¸ªæ–‡ä»¶

        self.stats['total_files'] = len(files)
        print(f"ğŸ“Š æ‰¾åˆ° {len(files)} ä¸ªæ–‡ä»¶ (ä¸ºç¨³å®šæ€§é™åˆ¶ä¸º{max_files or 'å…¨éƒ¨'})")

        if not files:
            print("âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶")
            return

        # æ‰¹é‡å¤„ç†
        batch_results = []

        for i, file_path in enumerate(files, 1):
            print(f"ğŸ“ˆ [{i}/{len(files)}] å¤„ç†: {file_path.name}")

            result = self.analyze_file_with_enhanced_features(str(file_path), output_dir)
            batch_results.append(result)

            if result['success']:
                self.stats['processed_files'] += 1
            else:
                self.stats['failed_files'] += 1

            # æ˜¾ç¤ºè¿›åº¦
            successful = len([r for r in batch_results if r.get('success', False)])
            print(f"   è¿›åº¦: {successful}/{len(batch_results)} æˆåŠŸ")
            print()

        # å®Œæˆç»Ÿè®¡
        self.stats['processing_end'] = datetime.now()
        if self.stats['processing_start'] and self.stats['processing_end']:
            processing_time = (self.stats['processing_end'] - self.stats['processing_start']).total_seconds()
        else:
            processing_time = 0

        print("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ")
        print("=" * 80)
        print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {self.stats['total_files']}")
        print(f"âœ… å¤„ç†æˆåŠŸ: {self.stats['processed_files']}")
        print(f"âŒ å¤„ç†å¤±è´¥: {self.stats['failed_files']}")
        print(f"ğŸ“Š æ€»åˆ†æ®µæ•°: {self.stats['total_segments']}")
        print(f"âœ… æˆåŠŸåˆ†æ®µ: {self.stats['successful_segments']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {self.stats['successful_segments']/max(1, self.stats['total_segments'])*100:.1f}%")
        print()
        print(f"ğŸ¯ å¯ä¿¡åº¦åˆ†å¸ƒ:")
        print(f"   é«˜å¯ä¿¡åº¦: {self.stats['high_confidence_files']} ä¸ªæ–‡ä»¶")
        print(f"   ä¸­å¯ä¿¡åº¦: {self.stats['medium_confidence_files']} ä¸ªæ–‡ä»¶")
        print(f"   ä½å¯ä¿¡åº¦: {self.stats['low_confidence_files']} ä¸ªæ–‡ä»¶")
        print()
        print(f"ğŸ” è´¨é‡æ§åˆ¶ç»Ÿè®¡ (90%é˜ˆå€¼):")
        print(f"   æ€»è¯„ä¼°æ•°: {self.quality_stats['total_evaluated']}")
        print(f"   âœ… é€šè¿‡è´¨é‡é˜ˆå€¼: {self.quality_stats['passed_quality_threshold']}")
        print(f"   âŒ æœªé€šè¿‡è´¨é‡é˜ˆå€¼: {self.quality_stats['failed_quality_threshold']}")
        print(f"   å¹³å‡æˆåŠŸç‡: {self.quality_stats['average_success_rate']:.1%}")

        if self.quality_stats['total_evaluated'] > 0:
            quality_pass_rate = self.quality_stats['passed_quality_threshold'] / self.quality_stats['total_evaluated'] * 100
            print(f"   è´¨é‡é€šè¿‡ç‡: {quality_pass_rate:.1f}%")
        print(f"â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.1f}ç§’")

        # ä¿å­˜æ‰¹é‡å¤„ç†æŠ¥å‘Š
        batch_report = {
            "batch_info": {
                "models_used": [{"name": m["name"], "description": m["description"]} for m in self.models],
                "segment_size": 5,
                "processing_date": datetime.now().isoformat(),
                "processing_time": processing_time,
                "analysis_method": "5é¢˜åˆ†æ®µï¼Œä¸‰æ¨¡å‹ç‹¬ç«‹è¯„ä¼° + 90%è´¨é‡é˜ˆå€¼æ§åˆ¶ + å¢å¼ºç¨³å®šæ€§"
            },
            "input_files": [str(f) for f in files],
            "results": batch_results,
            "statistics": self.stats,
            "quality_control": {
                "min_success_rate_threshold": self.min_success_rate,
                "quality_stats": self.quality_stats,
                "quality_pass_rate": (self.quality_stats['passed_quality_threshold'] / max(1, self.quality_stats['total_evaluated']) * 100)
            },
            "summary": {
                "total_files": self.stats['total_files'],
                "successful_files": self.stats['processed_files'],
                "failed_files": self.stats['failed_files'],
                "success_rate": self.stats['processed_files']/max(1, self.stats['total_files'])*100,
                "high_confidence_files": self.stats['high_confidence_files'],
                "medium_confidence_files": self.stats['medium_confidence_files'],
                "low_confidence_files": self.stats['low_confidence_files'],
                "average_confidence_score": sum(r.get('consistency_analysis', {}).get('confidence_score', 0) for r in batch_results if r.get('success')) / max(1, len([r for r in batch_results if r.get('success')])),
                "quality_threshold_passed": self.quality_stats['passed_quality_threshold'],
                "quality_threshold_failed": self.quality_stats['failed_quality_threshold'],
                "average_success_rate": self.quality_stats['average_success_rate']
            }
        }

        # ä½¿ç”¨å¢å¼ºæ–‡ä»¶å¤„ç†å™¨ä¿å­˜æ‰¹é‡æŠ¥å‘Š
        batch_report_path = os.path.join(output_dir, "enhanced_batch_report.json")
        save_success = self.file_handler.save_json(batch_report, batch_report_path, backup=True)

        if save_success:
            print(f"ğŸ“„ æ‰¹é‡æŠ¥å‘Šå·²ä¿å­˜: enhanced_batch_report.json")

        print(f"\nâœ… å¢å¼ºæ‰¹é‡åˆ†æå®Œæˆ!")
        print(f"ğŸ¯ å…³é”®æ”¹è¿›:")
        print(f"   âœ… ä¿®å¤JSONåºåˆ—åŒ–é”™è¯¯")
        print(f"   âœ… å¢å¼ºé”™è¯¯å¤„ç†æœºåˆ¶")
        print(f"   âœ… é™ä½å¹¶å‘æé«˜ç¨³å®šæ€§")
        print(f"   âœ… 90%è´¨é‡æ§åˆ¶é˜ˆå€¼")
        print(f"   âœ… æ™ºèƒ½é‡è¯•å’Œé™çº§")

        return batch_report

def main():
    """ä¸»å‡½æ•°"""
    evaluator = EnhancedEvaluator()

    # è¾“å…¥è¾“å‡ºç›®å½•
    input_dir = "results/results"  # å¯æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
    output_dir = "enhanced_results"

    # æ‰¹é‡åˆ†æ - é™åˆ¶æ–‡ä»¶æ•°é‡ä»¥æé«˜ç¨³å®šæ€§
    evaluator.batch_analyze(input_dir, output_dir, max_files=5)  # é™åˆ¶ä¸º5ä¸ªæ–‡ä»¶

if __name__ == "__main__":
    main()