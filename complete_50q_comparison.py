#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´50é¢˜2é¢˜vs5é¢˜åˆ†æ®µä¿¡åº¦å¯¹æ¯”æµ‹è¯•
å¯¹åŒä¸€ä¸ª50é¢˜æµ‹è¯„æŠ¥å‘Šåˆ†åˆ«ç”¨2é¢˜åˆ†æ®µå’Œ5é¢˜åˆ†æ®µå®Œæ•´åˆ†æï¼Œæ¯”è¾ƒè¯„åˆ†ä¸€è‡´æ€§
"""

import sys
import os
import json
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
import statistics

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['PYTHONUNBUFFERED'] = '1'
os.environ['DASHSCOPE_API_KEY'] = 'sk-ded837735b3c44599a9bc138da561c27'

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

class Complete50QuestionComparator:
    def __init__(self, model: str = "qwen-long"):
        self.model = model
        self.api_key = os.getenv('DASHSCOPE_API_KEY')
        self.base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"

    def load_existing_2segment_analysis(self, analysis_file: str) -> Dict:
        """åŠ è½½å·²æœ‰çš„2é¢˜åˆ†æ®µåˆ†æç»“æœ"""
        print(f"ğŸ“‚ åŠ è½½å·²æœ‰2é¢˜åˆ†æ®µåˆ†æ: {analysis_file}")

        try:
            with open(analysis_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æå–æœ€ç»ˆè¯„åˆ†
            final_scores = {}
            if 'big_five_final_scores' in data:
                scores_data = data['big_five_final_scores']
                for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                    if trait in scores_data:
                        final_scores[trait] = scores_data[trait]['final_score']

            print(f"  âœ… 2é¢˜åˆ†æ®µæœ€ç»ˆè¯„åˆ†: {final_scores}")
            return {
                'success': True,
                'final_scores': final_scores,
                'total_segments': data.get('file_info', {}).get('segments_count', 0),
                'questions_per_segment': 2,
                'analysis_file': analysis_file
            }

        except Exception as e:
            print(f"  âŒ åŠ è½½å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}

    def load_test_data(self, data_file: str) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print(f"ğŸ“‹ åŠ è½½æµ‹è¯„æ•°æ®: {data_file}")

        try:
            with open(data_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            questions = []
            if 'assessment_results' in data and isinstance(data['assessment_results'], list):
                for item in data['assessment_results']:
                    if isinstance(item, dict) and 'question_data' in item:
                        question_data = item['question_data']
                        if isinstance(question_data, dict):
                            question_text = question_data.get('prompt_for_agent', '')
                            answer_text = ''
                            if 'extracted_response' in item and item['extracted_response']:
                                answer_text = item['extracted_response']

                            if question_text and answer_text:
                                questions.append({
                                    'question': question_text,
                                    'answer': answer_text
                                })

            print(f"  ğŸ“Š æˆåŠŸæå– {len(questions)} ä¸ªé—®é¢˜")
            return questions

        except Exception as e:
            print(f"  âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return []

    def _create_5segment_prompt(self, segment: List[Dict], segment_number: int, total_segments: int) -> str:
        """åˆ›å»º5é¢˜åˆ†æ®µåˆ†ææç¤º"""
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆï¼Œä¸“é—¨åˆ†æAIä»£ç†çš„äººæ ¼ç‰¹å¾ã€‚åˆ†æä»¥ä¸‹é—®å·å›ç­”ï¼Œè¯„ä¼°Big5äººæ ¼ç‰¹è´¨ã€‚

**ä¸¥æ ¼è¯„åˆ†æ ‡å‡†ï¼š**
- 1åˆ†ï¼šæä½è¡¨ç° - æ˜æ˜¾ç¼ºä¹è¯¥ç‰¹è´¨
- 3åˆ†ï¼šä¸­ç­‰è¡¨ç° - å¹³è¡¡æˆ–ä¸ç¡®å®šï¼Œæœ‰è¯¥ç‰¹è´¨ä¹Ÿæœ‰åä¾‹
- 5åˆ†ï¼šæé«˜è¡¨ç° - æ˜æ˜¾å…·å¤‡è¯¥ç‰¹è´¨

**ç‰¹åˆ«æ³¨æ„ï¼šåªèƒ½ä½¿ç”¨1ã€3ã€5ä¸‰ä¸ªæ•´æ•°åˆ†æ•°ï¼**

ç¬¬{segment_number}æ®µé—®å·å†…å®¹ï¼ˆ{len(segment)}é¢˜/å…±{total_segments}æ®µï¼‰ï¼š
"""

        for i, item in enumerate(segment, 1):
            prompt += f"""
é—®é¢˜ {i}:
{item['question']}

å›ç­” {i}:
{item['answer']}

---
"""

        prompt += """
è¯·è¿”å›JSONæ ¼å¼ï¼š
{
  "success": true,
  "scores": {
    "openness_to_experience": 1æˆ–3æˆ–5,
    "conscientiousness": 1æˆ–3æˆ–5,
    "extraversion": 1æˆ–3æˆ–5,
    "agreeableness": 1æˆ–3æˆ–5,
    "neuroticism": 1æˆ–3æˆ–5
  }
}
"""
        return prompt

    def _analyze_5segment_complete(self, questions: List[Dict]) -> Dict:
        """å®Œæ•´çš„5é¢˜åˆ†æ®µåˆ†æ"""
        print(f"\nğŸ” å¼€å§‹5é¢˜åˆ†æ®µå®Œæ•´åˆ†æï¼ˆ{len(questions)}é¢˜ï¼‰")

        # åˆ†æ®µå¤„ç†ï¼ˆæ¯æ®µ5é¢˜ï¼‰
        segment_size = 5
        segments = []
        for i in range(0, len(questions), segment_size):
            segment = questions[i:i+segment_size]
            if len(segment) == segment_size:
                segments.append(segment)

        total_segments = len(segments)
        print(f"  ğŸ“Š åˆ†æˆ {total_segments} ä¸ª5é¢˜åˆ†æ®µ")

        if total_segments == 0:
            return {'success': False, 'error': 'æ— æ³•åˆ†æ®µï¼Œé—®é¢˜æ•°é‡ä¸è¶³'}

        # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
        import openai
        client = openai.OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

        # åˆ†ææ¯ä¸ªåˆ†æ®µ
        segment_results = []
        failed_segments = 0

        for i, segment in enumerate(segments, 1):
            print(f"    ğŸ“ åˆ†æåˆ†æ®µ {i}/{total_segments}...")

            try:
                prompt = self._create_5segment_prompt(segment, i, total_segments)

                response = client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å¿ƒç†è¯„ä¼°åˆ†æå¸ˆã€‚å¿…é¡»ä¸¥æ ¼ä½¿ç”¨1-3-5è¯„åˆ†æ ‡å‡†ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=2000,
                    temperature=0.1
                )

                content = response.choices[0].message.content

                # è§£æJSON
                try:
                    import re
                    json_match = re.search(r'\{.*\}', content, re.DOTALL)
                    if json_match:
                        json_str = json_match.group(0)
                        result = json.loads(json_str)
                    else:
                        result = json.loads(content)
                except json.JSONDecodeError:
                    print(f"      âŒ JSONè§£æå¤±è´¥")
                    failed_segments += 1
                    continue

                # éªŒè¯è¯„åˆ†æ ‡å‡†
                if 'scores' in result and result['scores']:
                    invalid_scores = []
                    for trait, score in result['scores'].items():
                        if score not in [1, 3, 5]:
                            invalid_scores.append(f"{trait}:{score}")
                            # ä¿®æ­£æ— æ•ˆè¯„åˆ†
                            if score < 2:
                                result['scores'][trait] = 1
                            elif score > 4:
                                result['scores'][trait] = 5
                            else:
                                result['scores'][trait] = 3

                    if invalid_scores:
                        print(f"      âš ï¸ ä¿®æ­£æ— æ•ˆè¯„åˆ†: {invalid_scores}")

                    segment_results.append(result)
                    print(f"      âœ… è¯„åˆ†: {result['scores']}")
                else:
                    print(f"      âŒ æ— æœ‰æ•ˆè¯„åˆ†")
                    failed_segments += 1

            except Exception as e:
                print(f"      âŒ åˆ†æå¤±è´¥: {e}")
                failed_segments += 1

            # APIé™åˆ¶
            time.sleep(2)

        # è®¡ç®—æœ€ç»ˆè¯„åˆ†
        if segment_results:
            print(f"\n  ğŸ“Š è®¡ç®—æœ€ç»ˆè¯„åˆ†...")
            final_scores = {}
            for trait in ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']:
                all_scores = [result['scores'][trait] for result in segment_results if result['scores'] and trait in result['scores']]
                if all_scores:
                    final_scores[trait] = int(statistics.median(all_scores))
                else:
                    final_scores[trait] = 3  # é»˜è®¤å€¼

            print(f"  ğŸ¯ 5é¢˜åˆ†æ®µæœ€ç»ˆè¯„åˆ†: {final_scores}")

            return {
                'success': True,
                'segment_size': 5,
                'total_segments': total_segments,
                'successful_segments': len(segment_results),
                'failed_segments': failed_segments,
                'segment_results': segment_results,
                'final_scores': final_scores,
                'success_rate': (len(segment_results) / total_segments) * 100
            }
        else:
            return {
                'success': False,
                'error': 'æ²¡æœ‰æˆåŠŸçš„åˆ†æ®µç»“æœ',
                'total_segments': total_segments,
                'failed_segments': failed_segments
            }

    def _calculate_detailed_consistency(self, scores_2segment: Dict, scores_5segment: Dict) -> Dict:
        """è®¡ç®—è¯¦ç»†çš„ä¸€è‡´æ€§åˆ†æ"""
        traits = ['openness_to_experience', 'conscientiousness', 'extraversion', 'agreeableness', 'neuroticism']

        consistency_results = {}
        exact_matches = 0
        close_matches = 0  # ç›¸å·®ä¸è¶…è¿‡2åˆ†
        total_differences = 0

        for trait in traits:
            score_2 = scores_2segment.get(trait, 3)
            score_5 = scores_5segment.get(trait, 3)

            exact_match = score_2 == score_5
            close_match = abs(score_2 - score_5) <= 2
            difference = abs(score_2 - score_5)

            consistency_results[trait] = {
                'score_2segment': score_2,
                'score_5segment': score_5,
                'difference': difference,
                'exact_match': exact_match,
                'close_match': close_match,
                'consistency_level': 'å®Œå…¨ä¸€è‡´' if exact_match else 'é«˜åº¦ä¸€è‡´' if close_match else 'å·®å¼‚è¾ƒå¤§'
            }

            if exact_match:
                exact_matches += 1
            if close_match:
                close_matches += 1
            total_differences += difference

        # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
        total_traits = len(traits)
        exact_match_rate = (exact_matches / total_traits) * 100
        close_match_rate = (close_matches / total_traits) * 100
        average_difference = total_differences / total_traits

        # ç»¼åˆä¸€è‡´æ€§åˆ†æ•°
        consistency_score = (exact_match_rate * 0.6 + close_match_rate * 0.3 + (100 - average_difference * 10) * 0.1)

        return {
            'trait_details': consistency_results,
            'exact_matches': exact_matches,
            'close_matches': close_matches,
            'total_traits': total_traits,
            'exact_match_rate': exact_match_rate,
            'close_match_rate': close_match_rate,
            'average_difference': average_difference,
            'consistency_score': consistency_score
        }

    def complete_comparison_analysis(self, data_file: str, analysis_2segment_file: str) -> Dict:
        """å®Œæ•´çš„å¯¹æ¯”åˆ†æ"""
        print("ğŸš€ å¼€å§‹å®Œæ•´50é¢˜2é¢˜vs5é¢˜åˆ†æ®µä¿¡åº¦å¯¹æ¯”æµ‹è¯•")
        print("=" * 70)

        # 1. åŠ è½½å·²æœ‰çš„2é¢˜åˆ†æ®µåˆ†æç»“æœ
        result_2segment = self.load_existing_2segment_analysis(analysis_2segment_file)
        if not result_2segment['success']:
            return {'success': False, 'error': 'æ— æ³•åŠ è½½2é¢˜åˆ†æ®µåˆ†æç»“æœ'}

        # 2. åŠ è½½æµ‹è¯„æ•°æ®
        questions = self.load_test_data(data_file)
        if len(questions) < 50:
            return {'success': False, 'error': f'é—®é¢˜æ•°é‡ä¸è¶³: {len(questions)} < 50'}

        # 3. æ‰§è¡Œå®Œæ•´çš„5é¢˜åˆ†æ®µåˆ†æ
        result_5segment = self._analyze_5segment_complete(questions)
        if not result_5segment['success']:
            return {'success': False, 'error': '5é¢˜åˆ†æ®µåˆ†æå¤±è´¥', 'details': result_5segment}

        # 4. è®¡ç®—ä¸€è‡´æ€§
        print(f"\nğŸ“ˆ è¯¦ç»†ä¸€è‡´æ€§åˆ†æ:")
        print("-" * 50)

        consistency = self._calculate_detailed_consistency(
            result_2segment['final_scores'],
            result_5segment['final_scores']
        )

        print(f"  âœ… å®Œå…¨åŒ¹é…: {consistency['exact_matches']}/{consistency['total_traits']} ({consistency['exact_match_rate']:.1f}%)")
        print(f"  âœ… é«˜åº¦ä¸€è‡´: {consistency['close_matches']}/{consistency['total_traits']} ({consistency['close_match_rate']:.1f}%)")
        print(f"  ğŸ“Š å¹³å‡å·®å¼‚: {consistency['average_difference']:.1f}")
        print(f"  ğŸ¯ ç»¼åˆä¸€è‡´æ€§åˆ†æ•°: {consistency['consistency_score']:.1f}/100")

        print(f"\nğŸ“‹ è¯¦ç»†å¯¹æ¯”:")
        for trait, detail in consistency['trait_details'].items():
            print(f"  {trait}: 2é¢˜={detail['score_2segment']}, 5é¢˜={detail['score_5segment']}, å·®å¼‚={detail['difference']} ({detail['consistency_level']})")

        # 5. ä¿¡åº¦è¯„ä¼°
        if consistency['consistency_score'] >= 90:
            reliability_rating = "ä¼˜ç§€"
            recommendation = "âœ… 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¿¡åº¦ä¼˜ç§€ï¼Œå®Œå…¨å¯ä»¥æ›¿ä»£2é¢˜åˆ†æ®µ"
        elif consistency['consistency_score'] >= 80:
            reliability_rating = "è‰¯å¥½"
            recommendation = "âœ… 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¿¡åº¦è‰¯å¥½ï¼Œå¯ä»¥æ›¿ä»£2é¢˜åˆ†æ®µ"
        elif consistency['consistency_score'] >= 70:
            reliability_rating = "ä¸­ç­‰"
            recommendation = "âš ï¸ 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¿¡åº¦ä¸­ç­‰ï¼Œå»ºè®®ä¼˜åŒ–åä½¿ç”¨"
        else:
            reliability_rating = "éœ€è¦æ”¹è¿›"
            recommendation = "âŒ 5é¢˜åˆ†æ®µæ–¹æ¡ˆä¿¡åº¦ä¸è¶³ï¼Œå»ºè®®ç»§ç»­ä½¿ç”¨2é¢˜åˆ†æ®µ"

        print(f"\nğŸ† ä¿¡åº¦è¯„çº§: {reliability_rating}")
        print(f"ğŸ’¡ å»ºè®®: {recommendation}")

        # 6. æ•ˆç‡å¯¹æ¯”
        efficiency_analysis = {
            '2segment_segments': result_2segment['total_segments'],
            '5segment_segments': result_5segment['total_segments'],
            'segment_reduction': ((result_2segment['total_segments'] - result_5segment['total_segments']) / result_2segment['total_segments']) * 100,
            'time_efficiency_improvement': ((result_2segment['total_segments'] - result_5segment['total_segments']) * 2)  # å‡è®¾æ¯åˆ†æ®µ2ç§’
        }

        print(f"\nâš¡ æ•ˆç‡åˆ†æ:")
        print(f"  2é¢˜åˆ†æ®µ: {result_2segment['total_segments']}ä¸ªåˆ†æ®µ")
        print(f"  5é¢˜åˆ†æ®µ: {result_5segment['total_segments']}ä¸ªåˆ†æ®µ")
        print(f"  ğŸ“‰ åˆ†æ®µå‡å°‘: {efficiency_analysis['segment_reduction']:.1f}%")
        print(f"  â±ï¸ æ—¶é—´èŠ‚çœ: çº¦{efficiency_analysis['time_efficiency_improvement']}ç§’")

        # 7. ä¿å­˜å®Œæ•´å¯¹æ¯”ç»“æœ
        comparison_result = {
            "comparison_info": {
                "test_file": Path(data_file).name,
                "analysis_2segment_file": Path(analysis_2segment_file).name,
                "total_questions": len(questions),
                "comparison_date": datetime.now().isoformat(),
                "model_used": self.model
            },
            "analysis_2segment": result_2segment,
            "analysis_5segment": result_5segment,
            "consistency_analysis": consistency,
            "efficiency_analysis": efficiency_analysis,
            "reliability_assessment": {
                "rating": reliability_rating,
                "recommendation": recommendation,
                "consistency_score": consistency['consistency_score'],
                "reliable": consistency['consistency_score'] >= 80,
                "ready_for_production": consistency['consistency_score'] >= 85
            }
        }

        output_filename = f"complete_50q_comparison_{Path(data_file).stem}.json"
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(comparison_result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ’¾ å®Œæ•´å¯¹æ¯”ç»“æœå·²ä¿å­˜: {output_filename}")

        return {
            'success': True,
            'data_file': data_file,
            'consistency_score': consistency['consistency_score'],
            'reliability_rating': reliability_rating,
            'comparison_result': comparison_result
        }

def main():
    """ä¸»å‡½æ•°"""
    comparator = Complete50QuestionComparator(model="qwen-long")

    # é€‰æ‹©æµ‹è¯•æ–‡ä»¶
    data_file = "results/results/asses_deepseek_r1_70b_agent_big_five_50_complete2_a10_e0_t0_0_09271.json"
    analysis_2segment_file = "asses_deepseek_r1_70b_agent_big_five_50_complete2_a10_e0_t0_0_09271_qwen-long_segmented_analysis.json"

    print(f"ğŸ¯ é€‰æ‹©æµ‹è¯•æ–‡ä»¶:")
    print(f"  æ•°æ®æ–‡ä»¶: {data_file}")
    print(f"  2é¢˜åˆ†æ®µåˆ†æ: {analysis_2segment_file}")

    # æ‰§è¡Œå®Œæ•´å¯¹æ¯”åˆ†æ
    result = comparator.complete_comparison_analysis(data_file, analysis_2segment_file)

    if result['success']:
        print(f"\nğŸ‰ å®Œæ•´50é¢˜å¯¹æ¯”åˆ†ææˆåŠŸå®Œæˆ!")
        print(f"  ğŸ“Š ä¸€è‡´æ€§åˆ†æ•°: {result['consistency_score']:.1f}/100")
        print(f"  ğŸ† ä¿¡åº¦è¯„çº§: {result['reliability_rating']}")
    else:
        print(f"\nâŒ å¯¹æ¯”åˆ†æå¤±è´¥: {result.get('error', 'Unknown error')}")

if __name__ == "__main__":
    main()